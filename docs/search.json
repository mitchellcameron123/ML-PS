[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and the Propensity Score",
    "section": "",
    "text": "Preface\nThis project is submitted in partial fulfillment of the requirements for the Master of Applied Science in Statistics at the University of Otago. My academic background in economics and politics sparked an enduring interest in causal inference, particularly its role in shaping evidence-based policymaking. However, my focus has evolved to include machine learning — a field that, while not traditionally central to economics, offers powerful tools for refining causal analysis.\nThe motivation for this project stems from my desire to bridge the gap between propensity score methods and machine learning. While the literature is rich with simulation studies, there is a noticeable lack of comprehensive, tutorial-style resources that guide readers through the application of machine learning to propensity score estimation. This project aims to fill that void, providing a practical and accessible exploration of these techniques with approachable theoretical discussion and coded examples in R. As a dedicated user of R, all the packages and methodologies discussed in this project exist in the R ecosystem. Although comparable tools exist in other languages and software.\nThe intended audience for this work includes individuals with a foundational understanding of causal inference and machine learning, particularly those interested in enhancing propensity score models. However, my discussion extends beyond propensity scores; much of what is covered is relevant to anyone using machine learning for probability prediction.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Machine Learning and the Propensity Score",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to express my deepest gratitude to several individuals whose support, guidance, and encouragement have been invaluable throughout the course of this project.\nFirst and foremost, I extend my sincere thanks to Associate Professor Matthew Parry, my supervisor. His Socratic method of teaching has fostered not only my academic growth but also my critical thinking abilities. I am particularly grateful for his flexibility and understanding as I navigated through various iterations and changes in the direction of this project. His patience and positivity, even as the project’s focus shifted, were invaluable, and I deeply appreciate his guidance throughout this journey.\nI am also grateful to Dr Conor Kresin, whose knowledge of causal inference provided helpful background for this project. Additionally, my thanks go to Dr Falco J. Bargagli Stoffi, whose correspondence was incredibly helpful during earlier iterations of this project.\nTo my fellow students — Bethany, Jess, Shalini, Sarah, Maryam, Anna, Alex, Jackson, Teddy, Steven, and Chris — thank you for the camaraderie, the shared challenges, collective encouragement, and practical advice.\nLastly, but by no means least, I want to thank my girlfriend, Dani for her patience, support, and understanding throughout this process. I also want to express my gratitude for her meticulous help with proofreading, which has greatly improved the clarity and quality of this project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#presentation-tools",
    "href": "index.html#presentation-tools",
    "title": "Machine Learning and the Propensity Score",
    "section": "Presentation Tools",
    "text": "Presentation Tools\nQuarto is an open-source publishing system that enables the creation of dynamic documents, reports, presentations, books, and websites using R, Python, or Julia. It integrates code, markdown, and graphics seamlessly, making it ideal for reproducible research and communication. I the Quarto Book template to create which is hosted on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro_background.html",
    "href": "chapters/intro_background.html",
    "title": "1  Introduction and Background",
    "section": "",
    "text": "1.1 What is Causal Inference?\nCausal inference is a field of study that focuses on identifying and estimating causal relationships between things. It goes beyond correlation by establishing a cause-and-effect relationship. Causal inference methods often utilise counterfactual reasoning to estimate the causal effect of an exposure or treatment on an outcome. Such counterfactual reasoning is used unbeknownst every day. For example, if someone misses their bus and thinks, “If I had left home five minutes earlier, I wouldn’t have missed it,” they are engaging in counterfactual reasoning. In everyday life, policy-making, medicine, and business, understanding the size and nature of a cause is essential for decision making and avoiding misleading conclusions. In this background chapter I discuss key ideas in causal inference such as the potential outcomes framework, common estimands, and assumptions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Background</span>"
    ]
  },
  {
    "objectID": "chapters/intro_background.html#layout",
    "href": "chapters/intro_background.html#layout",
    "title": "1  Introduction and Background",
    "section": "1.2 Layout",
    "text": "1.2 Layout\nChapter 1 provides a foundational introduction to causal inference, which is essential for understanding the context of this project. This section is designed to provide a concise background for readers who may not be familiar with causal inference, ensuring they have the necessary foundation to follow the rest of the project.\nChapter 2 introduces the central focus of this project: the use of machine learning to estimate propensity scores. The section begins with a traditional introduction to propensity scores, explaining their role in balancing covariates between treatment and control groups to reduce estimator bias. This leads into a discussion on the limitations of conventional propensity score estimation methods, which often rely on logistic regression. These limitations motivate the use of machine learning algorithms for propensity score estimation. The section then provides a theoretical comparison of common machine learning algorithms such as random forests, bootstrap aggregation (bagging), and gradient boosting machines. The goal of this section is to provide readers with an intuitive understanding of how machine learning can enhance propensity score estimation, setting the stage for practical applications later in the project.\nChapter 3 presents a comprehensive tutorial for implementing machine learning techniques in the estimation of propensity scores. This section is highly practical, walking the reader through the software implementations available for estimating and assessing propensity scores. The National Supported Work (NSW) program dataset is used as a running example throughout the tutorial. This dataset is commonly used in causal inference studies due to its simplicity and well-documented treatment effect, making it an ideal example candidate. By the end of this section, readers should be able to apply these methods to their own datasets, replicating the steps and analyses presented.\nChapter 4 provides a detailed replication study of Jena et al. (2012), a paper that examines the impact of fair trade certification on farmers’ livelihood in Ethiopia. This section builds on the original findings of the aforementioned authors by applying machine learning-based propensity score methods. The replication study is designed to demonstrate the practical advantages of using machine learning for propensity score estimation in a real-world setting. Specifically, my replication re-examines the causal effect on per capita income. This comparison highlights how machine learning can potentially lead to more accurate and reliable estimates of treatment effects.\nChapter 5 provides a comprehensive overview of the findings from this project. It synthesizes the key insights gained from the theoretical discussions, the practical tutorial, and the replication study. This section also outlines potential avenues for future research, emphasizing the importance of continued exploration of machine learning methods in causal inference.\nAppendix A offers supplementary material that supports the main content of the project. This includes explanations of the datasets used, along with coded examples of loading and manipulating the data in R. Additionally, Appendix B presents custom functions developed for this project, which are used to present results and facilitate analyses throughout the project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Background</span>"
    ]
  },
  {
    "objectID": "chapters/intro_background.html#sec-outcomes_framework",
    "href": "chapters/intro_background.html#sec-outcomes_framework",
    "title": "1  Introduction and Background",
    "section": "1.3 Potential Outcomes Framework",
    "text": "1.3 Potential Outcomes Framework\nThe Potential Outcomes Framework, also known as the Rubin Causal Model, was introduced by Rubin (1974) and builds upon the work of Splawa-Neyman (1923). The framework dominates how researchers think about causal inference by formalising counterfactual reasoning. Rubin defines a causal effect as a defined comparison between two states of the world. For each individual, there are two potential outcomes: one if they receive the treatment and one if they do not. The causal effect is the difference between these two potential outcomes. Hence we have two potential outcomes, one with the treatment and one without.\nThe framework is highly flexible and adaptive, extending beyond traditional notions of “treatment” in medical or experimental contexts. It can apply to any kind of intervention, exposure, or condition that could influence an outcome, whether it’s a medical treatment, policy change, environmental exposure, or even abstract events like decisions or natural occurrences. Philosophically, the framework aligns with a view of the world that considers reality through alternative scenarios or what-ifs.\nConsider a binary treatment variable, let the treatment for an observation be a random variable, \\(T\\), with a realisation \\(t_i \\in \\{0,1\\}\\) under control and treatment. The absence of treatment is refer to as the control state. Let \\(Y_i(1)\\) and \\(Y_i(0)\\) be the two potential outcomes for observation \\(i\\) under treatment and control. Let the individual treatment effect (ITE) be defined as the difference between the two potential outcomes:\n\\[\n\\tau_i=Y_i(1) - Y_i(0).\n\\tag{1.1}\\]\n\n\n\n\n\n\nNote 1.1: Fundamental Problem of Causal Inference\n\n\n\nThere is a clear problem that only the outcome under either treatment or control is observable. If our observations are on people, then it is logically impossible for an individual to simultaneously both receive and not receive the treatment. For example, if someone takes medication to relieve a headache and their headache improves, it could never know what would have happened if they did not take the medication. This leads to the commonly discussed fundamental problem of causal inference - it is impossible to observe both potential outcomes. A counterfactual, the counter to the observed outcome, is infeasible and can never be practicably known.\n\n\nLet the observed outcome for \\(i\\) be denoted \\(y_i(1)\\) and \\(y_i(0)\\) under treatment or control. Many causal inference methods involve finding or estimating a counterfactual to compare outcomes to sove some variation of Equation 1.1. Let an estimated potential outcomes for \\(i\\) be denoted \\(\\hat{y}_i(1)\\) and \\(\\hat{y}_i(0)\\) under treatment or control.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Background</span>"
    ]
  },
  {
    "objectID": "chapters/intro_background.html#sec-estimands",
    "href": "chapters/intro_background.html#sec-estimands",
    "title": "1  Introduction and Background",
    "section": "1.4 Estimands",
    "text": "1.4 Estimands\nIn causal inference, there are multiple parameters of interest called and estimand. The preferred estimand depends on the motivating example, discipline, or intended interpretation of a result.\nThe most basic estimand is the average treatment effect or the ATE denoted \\(\\tau_{ATE}\\) which is the average amount of effect on all individuals in the population regardless of whether they receive the treatment or not. This can be written as:\n\\[\n\\begin{aligned}\n  \\text{ATE}& = E[\\tau_{ATE}] \\\\\n  &= E[Y(1) - Y(0)] \\\\\n  &= E[Y(1)] - E[Y(0)].\n\\end{aligned}\n\\tag{1.2}\\]\nUnder certain conditions, such as a randomised control trial, Equation 1.2 can be an estimated using the explicit equation:\n\\[\n\\widehat{\\text{ATE}} = \\hat{\\tau}_{ATE}= \\frac{1}{N_t} \\sum_{i=1}^{n} (y_i \\mid t_i = 1) - \\frac{1}{N_c} \\sum_{i=1}^{n} (y_i \\mid t_i = 0),\n\\tag{1.3}\\]\nwhere \\(N_t\\) and \\(N_c\\) are is the number of treated and control observations. Essentially, Equation 1.3 is just a difference in the mean outcome between the two groups.\nThe second parameter of interest is the average treatment effect on the treated or ATT and is the difference (contrast) between the potential outcomes of those who actually receive the treatment. In other words, considering observations where \\(t_i=1\\), what is the effect of the treatment? This can be written as:\n\\[\n\\begin{aligned}\n  \\text{ATT}  &= \\tau_{ATT}= E[\\tau \\mid T = 1] \\\\\n  & = E[Y(1) - Y(0) \\mid T = 1] \\\\\n  & = E[[Y(1) \\mid T = 1] - E[Y(0) \\mid T = 1].\n\\end{aligned}\n\\tag{1.4}\\]\nThe final parameter is the average treatment effect on the control or ATC which is similar to the ATT but on those who are actually under control. The ATC is the contrast between the two potential outcomes for individuals which are actually in the control. This is also known as the average treatment effect on the untreated or the ATU. It can be written as:\n\\[\n\\begin{aligned}\n  \\text{ATC} &= \\tau_{ATC} = E[\\tau\\mid T = 0] \\\\\n  & = E[Y(1) - Y(0) \\mid T = 0] \\\\\n  & = E[[Y(1) \\mid T = 0] - E[Y(0) \\mid T = 0]\n\\end{aligned}\n\\tag{1.5}\\]\nFor the estimated ATT and ATC, no explicit expression exist. Estimation is completed using G-methods to obtain contrasts of potential outcomes (see Naimi, Cole, and Kennedy 2017).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Background</span>"
    ]
  },
  {
    "objectID": "chapters/intro_background.html#sec-assumptions",
    "href": "chapters/intro_background.html#sec-assumptions",
    "title": "1  Introduction and Background",
    "section": "1.5 Assumptions in Causal Inference",
    "text": "1.5 Assumptions in Causal Inference\nAssumptions are made for the potential outcomes framework to be logically coherent and for estimands to be identifiable. Firstly, independence must be assumed, implying the potential outcomes are independent of \\(T\\). This assumption is also known as unconfoundedness, ignorability, or selection on observables, and means there is no confounding relationship between the treatment and potential outcomes. This matters as confounding variables can create a spurious relationship between the treatment and the outcome, leading to biased estimates of the treatment’s effect. Hence, the treatment assignment should be random, allowing an unbiased estimate. Mathematically independence can be stated as:\n\\[\nY(1), Y(0) \\perp \\!\\!\\! \\perp T.\n\\tag{1.6}\\]\nIndependence implies exchangeability meaning the individuals in the treatment and control groups could be swapped and the potential outcomes are still the same. A weaker assumption is conditional independence that states that assignment into treatment is random conditioned on some \\(X\\):\n\\[\nY(1), Y(0) \\perp \\!\\!\\! \\perp T\\mid X.\n\\tag{1.7}\\]\nThe assumption requires that covariates must be known and measurable which may not always hold. Independence motivates the use of randomisation in experimental contexts as this should guarantee independence. Chapter 2 discusses conditional independence and uses propensity scores to condition on covariates.\nA second assumption is positivity. This means that for each \\(i\\), the condition probability when \\(X=x\\) of being in either the treatment or control group is strictly between \\(0\\) and \\(1\\). In other words, \\(\\Pr(T = 1 \\mid X = x) &gt; 0\\) and \\(\\Pr(T = 0 \\mid X = x) &gt; 0\\). This ensures that all observations have at least some chance of receiving either the treatment or control. If not, it is theoretically impossible to obtain both potential outcomes and so the treatment effect cannot be estimated.\nBuilding on positivity, the third assumption is common support. This implies the treatment and control groups overlap in terms of their characteristics. Overlap is crucial because it ensures that for every person in the treatment group, there are similar individuals in the control group—similar in terms of age, gender, income, and other factors. Mathematically, for all of \\(i\\), if the conditional probability of being treated, \\(\\Pr(T = 1 \\mid X = x)\\), is near to \\(1\\), and \\(\\Pr(T = 0 \\mid X = x)\\) is near to \\(0\\), then there are no compatible cases and there is no common support. Without compatible cases, it is not possible to satisfy exchangability and so treatment effect estimates are likely to be biased.\nThe fourth assumption is consistency between the potential outcome and observed outcome. For every \\(i\\), the observed outcome under treatment equals the potential outcome under treatment. Additionally, the observed outcome under control equals the potential outcome under control. Mathematically, \\(y_i(1)=Y_i(1)\\) and \\(y_i(0)=Y_i(0)\\) that leads to a switching equation which defines \\(y_i\\) as a function of the potential outcomes:\n\\[\ny_i = T_i Y_i(1)+(1 - T_{i})Y_i(0).\n\\tag{1.8}\\]\nNotice the logic of this equation, when \\(T=1\\) then \\(y_i =Y_i(1)\\) as the second term becomes zero. Similarly, when \\(T=0\\) then \\(y_i = Y_i(0)\\) as the first term becomes zero.\nThe final key assumption is called the stable unit treatment value assumption or SUTVA. This is a complex way of stating that there is no interference between observations. More specifically, neither potential outcome is affected by the treatment status of any other individual. To borrow terminology from economics, there are no externalities or spillover effects from one observations’ treatment status to another observations’ potential outcomes.\n\n\n\n\n\n\nNote 1.2: Why Assumptions Matter\n\n\n\nIn causal inference, especially when working with observational data, it is critical that these assumptions are considered. If these assumptions do not hold, any model, regardless of the modelling assumptions, will not have a causal interpretation. Unfortunately, there are no tests that can confirm if these causal assumptions hold and thus researchers must understand the context and data generating process in which they operate.\n\n\n\n\n\n\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.\n\n\nNaimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. “An introduction to g methods.” International Journal of Epidemiology 46 (2): 756–62. https://doi.org/10.1093/ije/dyw323.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Experimental and Observational Studies.” Journal of Educational Psychology 66 (5): 688–701. https://doi.org/10.1002/j.2333-8504.1972.tb00631.x.\n\n\nSplawa-Neyman, Jerzy. 1923. “On the application of probability theory to agricultural experiments. Essay on principles. Section 9.” PhD thesis. https://doi.org/10.1214/ss/1177012031.\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.\n\n\nNaimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. “An introduction to g methods.” International Journal of Epidemiology 46 (2): 756–62. https://doi.org/10.1093/ije/dyw323.\n\n\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Experimental and Observational Studies.” Journal of Educational Psychology 66 (5): 688–701. https://doi.org/10.1002/j.2333-8504.1972.tb00631.x.\n\n\nSplawa-Neyman, Jerzy. 1923. “On the application of probability theory to agricultural experiments. Essay on principles. Section 9.” PhD thesis. https://doi.org/10.1214/ss/1177012031.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Background</span>"
    ]
  },
  {
    "objectID": "chapters/pscore_theory.html",
    "href": "chapters/pscore_theory.html",
    "title": "2  Propensity Scores and Machine Learning",
    "section": "",
    "text": "2.1 A Conventional Approach: Propensity Scores and Balance\nIn a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, Equation 1.3 leads to a biased estimate of the treatment effect.\nFor example, consider the causal question: “How much does obtaining a bachelors degree increase lifetime earnings?”. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the cause of higher incomes when the effect is confounded by the differences in covariates between groups. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of covariate balance which is a key concept behind underlying propensity score methods.\nIn the bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parental income, geographic residence etc. The covariates are said to be conditioned on by matching individuals on these covariates. Comparing the balanced matched pairs should result in a robust estimate of a bachelors degree’s impact on earnings because the individuals are exchangeable and satisfy Equation 1.7. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a dimensionality problem as the dimension of the number of covariates increases.\nRosenbaum and Rubin (1983) offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as \\(e(X)\\) and be expressed as:\n\\[\ne(X)=P(T=1|X=x).\n\\tag{2.1}\\]\nA prediction of the conditional probability of treatment on covariates is a good summary of the covariate’s effect on receiving the treatment. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects.\nIt is hoped that conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in Equation 1.7. There are many sources that offer a comprehensive guide to propensity score methods such as (Cunningham 2021, chap. 4) who provides applications and coded examples in R, Python, and Stata.\nTwo common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW).1 PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.\nAt a high level, the conditioned property of the propensity score is translated into a model by using PSM or IPW. King and Nielsen (2019) provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity Scores and Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/pscore_theory.html#a-conventional-approach-propensity-scores-and-balance",
    "href": "chapters/pscore_theory.html#a-conventional-approach-propensity-scores-and-balance",
    "title": "2  Propensity Scores and Machine Learning",
    "section": "",
    "text": "Note 2.1: What is Covariate Balance\n\n\n\nCovariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate is the same across each group, then the covariates are balanced and a meaningful comparison between groups can be made. Equally, similar covariates across groups implies exchangability as the two groups should be similar (thus can be exchanged). There is a conceptual link between covariate balance, unconfoundedness, and exchangeability meaning that Equation 1.6 is satisfied when covariates are balanced.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 2.2: Balance and Propensity Scores\n\n\n\nNote that an RCT will satisfy Equation 1.6 as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy Equation 1.7 as the potential outcomes are independent of the treatment status conditioned on some covariates. Thus, \\(Y(1), Y(0) \\perp \\!\\!\\! \\perp T\\mid X\\) is satisfied by \\(Y(1), Y(0) \\perp \\!\\!\\! \\perp T\\mid \\widehat{e(X)}\\).\nConditioning on the propensity score aims to replicate an RCT in observational data by balancing covariates between groups. When observations are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates.\n\n\n\n\n\n2.1.1 Assessing balance\nBalance assessment is an important step to ensure that conditioning on the propensity score has been successful. A commonly recommended measure of covariate balance is the standardised mean difference or SMD. This is the difference in the mean of each covariate between treatment groups standardised by a standardization factor so that it is on the same scale for all covariates.\nSMDs close to zero indicate good balance. P. Austin (2011) notes that \\(0.1\\) is a common threshold for determining if a variable is balanced. This threshold is a guideline to the approximate region that indicates a covariate is balanced and should not be interpreted as a binary rule. Additionally, a variance ratio below 2 is generally acceptable. For brevity, my analysis only considers the SMD.\n\n\n2.1.2 Propensity Score Modelling with Logistic Regression\nA conventional propensity score model uses logistic regression to predict a probability between \\(0\\) and \\(1\\). Models may be specified to include interaction terms and polynomial terms to capture complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is a heuristically driven art rather than science. (see Brookhart et al. 2006; Heinrich 2010). One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are not statistically significant. Notably, many researchers do not discuss the specification of propensity models in their papers. P. C. Austin (2008) reviews \\(47\\) papers that use propensity scores and find few perform adequate model selection, assess balance, or apply correct statistical tests.\nIt is important to note that the true propensity score is never observable. A propensity score that is close to the theoretically true probability is well calibrated. Poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. Propensity scores may be poorly calibrated as covariates may be omitted by error, poorly measured, or be unobservable. Logistic regression may not predict calibrated scores if the true relationship is non-linear or involves complex interactions between covariates. Another important note is that the propensity model itself does not have an informative causal interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for each variable which is not informative of the desired estimand.\nThe first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity Scores and Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/pscore_theory.html#probability-machines-probability-theory-and-machine-learning",
    "href": "chapters/pscore_theory.html#probability-machines-probability-theory-and-machine-learning",
    "title": "2  Propensity Scores and Machine Learning",
    "section": "2.2 Probability Machines: Probability Theory and Machine Learning",
    "text": "2.2 Probability Machines: Probability Theory and Machine Learning\nSupervised machine learning usually focuses on classifying observations into groups, or predicting continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability an observation belongs to a certain class. Machine learning methods that predict probabilities are sometimes called probability machines.\nProbability machines are valuable in applications requiring calibrated probability predictions. For example, probability machines can predict loan defaults or other adverse events in finance. In marketing, they estimate the likelihood of customer response to a campaign. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.\nProbability machines offer many advantages over parametric methods like logistic regression:\n\nImproved Calibration: Probability machines often provide better-calibrated predictions by capturing complex data relationships.\nFlexible Modelling: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.\nEfficient Feature Selection: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.\nHandling Missing Data: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.\nSimplified Data Exploration: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.\n\nIn causal inference, probability machines can predict better calibrated propensity scores and better estimate treatment effects. This discussion aims to clarify the use of probability machines in causal inference given the unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.\n\n\n\n\n\n\nNote 2.3: A Particularly Important Method: Classification and Regression Trees\n\n\n\nMoving forward, a particularly important model is the Classification and Regression Trees. L. Breiman et al. (1984) introduces method, commonly known as CART, that partitions data according to a splitting criterion, resulting in an “if this, then that” interpretation. CART models are also widely known as a decision trees. The splits are recursive, meaning splits are applied upon previous splits, such as trees breaking into branches and then leaves. The splits are also greedy as each potential split only considers information available at that split instead of past or future splits. Each parent node is split to create two child nodes and the final nodes of a CART model are called terminal nodes.\nFor example, when classifying pets into cats versus dogs, the first split might be“if barks” and the second is “heavier than 5 kg”. The tree says If it barks and is heavier than 5 kg, then it is a dog.\nA single classification tree typically uses the Gini index to determine its splits. Each split aims to maximise node purity, meaning the nodes contain the highest possible proportion of one class. The Gini index measures impurity, with lower Gini values indicating higher purity. Intuitively, the aim of a classification tree’s loss function is to minimise the misclassification rate of observations. By selecting splits that reduce the Gini index, the tree minimises classification error and increases accuracy.\n\n\n\n2.2.1 Choice of Loss Function and Probability Prediction\nThe loss function measures the difference between a model’s predictions and the actual target values, serving as a measure of a model’s performance. Different loss functions influence a model’s behaviour so the choice of loss function is important. Classification models predict the category that each observation belongs to not the probability of each class. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Many classification loss functions minimise classification errors and improve accuracy as this results in the best classification. A loss function like the Gini index is effective for classification problems but it is unclear if this applies to probability problems. In other words, minimizing misclassification error may not lead to accurate probability predictions.\nAt a high level, to classify an observation, \\(x_i\\) as an \\(A\\) or \\(B\\), a model needs to determine if \\(\\Pr(x_i=A)\\) is less than or greater than \\(0.5\\). If \\(\\widehat{\\Pr}(x_i=A) &gt; 0.5\\), then it is more likely to be an \\(A\\) and if \\(\\widehat{\\Pr}(x_i=A) &lt;0.5\\) then it is more likely to be a \\(B\\). Thus, if \\(x_i\\) is an \\(A\\), it is trivial if \\(\\widehat{\\Pr}(x_i=A)\\) is \\(0.51\\) or \\(0.99\\) as this makes no difference to the classification as an \\(A\\). But the difference between \\(\\widehat{\\Pr}(x_i=A) = 0.51\\) and \\(0.99\\) is extreme for a probability machine. It is important to understand that classification models are optimised for classification accuracy rather than probability prediction. This distinction affects the performance of ensemble methods like random forests or bagging ensembles that use classification trees for probability prediction.\n\n\n2.2.2 Bagging and Random Forest as Probability Machines\n\n\n\n\n\n\nNote 2.4: A Quick Note on Ensemble Learning\n\n\n\nEnsemble learning refers to a general framework of machine learning that combines multiple simple models to create a better overall model. The philosophy behind ensemble learning is rooted in the wisdom of crowds principle, where the collective decisions from multiple models often outperform that of individual models. Often, ensemble learning methods use multiple CART models.\nBagging ensembles, random forests, and gradient boosting machines are all machine learning methods that combine CART models and are all examples of ensemble learning algorithms. These methods are introduced in the following discussion.\n\n\nConsider an ensemble method called the lazy ensemble that combines multiple CART trees. Each tree is fitted on the same data without any cross validation. In this ensemble, class probabilities are determined through a vote count method. Within the lazy ensemble, each tree makes a class prediction based on the majority class in a terminal node. For instance, if \\(x_i\\) lies in a terminal node where \\(80\\%\\) of the observations are classified as an \\(A\\), that individual tree will classify \\(x_i\\) as \\(A\\). The ensemble’s overall prediction for \\(x_i\\) is derived from the proportion of trees that classify \\(x_i\\) as \\(A\\) or \\(B\\). Thus, the ensemble counts votes for each class across the ensemble. Let \\(T\\) be the total number of trees and \\(b_t\\) be the \\(t\\)-th tree in the ensemble. Let \\(\\mathbb{I}(b_t(x_i) = A)\\) be the indicator function that returns \\(1\\) when \\(b_t\\) predicts that observation \\(x_i\\) belongs to class \\(A\\). The probability of class \\(A\\) for observation \\(x_i\\) is calculated as:\n\\[\n\\widehat{\\Pr}(x_i = A) = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{I}(b_t(x_i) = A).\n\\tag{2.2}\\]\nOlson and Wyner (2018) note bias towards predictions of \\(0\\) or \\(1\\) when trees in an ensemble framework are highly correlated and a vote count method is used. In the lazy ensemble, each tree is identical and perfectly correlated implying that each tree will make the the same class prediction for each \\(x_i\\). For such an ensemble, the predicted probabilities will will be exactly \\(0\\) or \\(1\\) using a vote count. Of course a lazy ensemble of identical trees would never be used but the intuition still applies in the real world where ensembles may have some degree of correlation. The larger the correlation, the more the probability predictions will exhibit a divergence bias towards \\(0\\) and \\(1\\). Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.\nTo reduce tree correlation and improve upon the lazy ensemble, a bagging ensemble (see Leo Breiman 1996) trains each tree on a randomly selected bootstrapped sample of the data. Random forests (see Leo Breiman 2001) further reduce tree correlation by considering a random number of variables at each split, commonly referred to as \\(mtry\\) in software implementations. Note that these ensemble methods typically use a vote count method in the same way as the lazy ensemble. When \\(mtry\\) is equal to to number of predictors, the model considers all variables at each split and the random forest is equal to a bagging ensemble. A lower \\(mtry\\) should reduce the correlation between trees and decrease divergence bias as the structure of the tree is modified by the selected variables at each split. However, a lower \\(mtry\\) also introduces other theoretical problems.\nConsider the scenario where the binary outcome (treatment and control) of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If \\(mtry\\) is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. Each predictor has a chance of \\(\\frac{mtry}{\\text{number of predictors}}\\) of selection at each split implying a lower \\(mtry\\) decreases the chance of a splitting on the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of \\(x_i\\) in each tree. Such an ensemble may have highly unstable probability predictions.\nAdditionally, consider there is a class imbalance and the majority of observations are classified as \\(A\\) not \\(B.\\) The terminal nodes of each tree within an ensemble are more likely to contain the majority class. Consequently, there is a majority class bias as each tree in the ensemble is more likely to misclassifying an observation as an \\(A\\) because the terminal nodes have a higher proportion of \\(A\\) due to the higher proportion of \\(A\\)’s in the data overall.\n\n\n\n\n\n\nNote 2.5: Class Imbalance and Machine Learning\n\n\n\nWhen there is a difference in the number of observations in each class, this is called class imbalance. It is important to note that majority class bias exists in conventional machine learning classification tasks. Bagging ensembles and random forest are well known to be sensitive to class imbalance meaning that class predictions are biased towards the majority (see Bader-El-Den, Teitei, and Perry 2019).\nHowever, the class imbalance problem is particularly notable when predicting probabilities. The probability prediction from a vote count method is very sensitive to a change in the votes from each tree. Suppose that balanced data results in \\(80/100\\) trees classifying \\(B\\) as \\(B\\) and imbalanced data (more \\(A\\) than \\(B\\)) reduces correct classifications of \\(B\\) to \\(60/100\\). This results in a \\(20\\%\\) margin of error in probability estimates but the classification remains as \\(B\\).\n\n\nIndividually, a low \\(mtry\\) can lead to unstable probability predictions and class imbalance can create bias towards the majority class. But probability machines are particularly effected when there is both a low \\(mtry\\) and class imbalance. Because successive noisy splits (relating to a low \\(mtry\\)) result in impure child nodes, the effects of majority class bias are exaggerated. Without the ability to separate the classes, the majority class will dominate terminal nodes. If the ensemble was able to split on informative covariates each time (\\(mtry\\) is higher), then it should still be able to create pure splits even when there is some class imbalance. In other words, if there is a small class imbalance, reducing \\(mtry\\) may reveal majority class bias not visible at higher \\(mtry\\)’s. Equally, if there is low \\(mtry\\), then even a small class imbalance can lead to majority class bias.\nAs a general philosophy, ensemble learning methods based on classification trees are poor at predicting probabilities. If \\(x_i\\) has a known membership of \\(A\\), and an unknown \\(\\Pr_{\\text{true}}(x_i=A) = 0.6\\), the ensemble might classify \\(x_i\\) correctly \\(90\\%\\) of the time leading to \\(\\widehat{\\Pr}(x_i=A) = 0.9\\). As a probability machine, the ensemble has overestimated the probability by \\(30\\%\\) even though a \\(90\\%\\) classification accuracy is excellent. To predict \\(\\Pr_{\\text{true}}(x_i=A) = 0.6\\), an ensemble needs to incorrectly classify \\(x_i\\) in \\(40\\%\\) of its trees. However, bagging ensembles and random forests are designed to maximise classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.\nTo exemplify these theoretical points, the National Supported Work (NSW) programme is a commonly discussed dataset in causal inference. The data results from a randomised controlled trial with \\(445\\) total participants, \\(185\\) in the program group, and \\(260\\) in the control group, so the true probability of treatment for each individual can be calculated as \\(185/445=0.42\\) or \\(42\\%\\). Further information about this data is found in Appendix A. Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although Figure 2.2 and Table 3.1 suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by Smith and Todd (2005) who suggest randomisation is imperfect in the NSW dataset. Thus, the “best” calibrated probability model will have a distribution centred near \\(0.42\\) with some noise due to imperfect randomisation.\nFigure 2.1 shows both divergence bias and majority class bias using randomForest() to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when \\(mtry\\) is equal to the number of predictors and so specifying mtry = 7 in the randomForest() function fits a bagging ensemble. Additionally, logistic regression using the glm() function provides a meaningful comparison.\n\n\nCode: Create Figure 2.1\n# Create Figure 2.1\nlibrary(randomForest)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(ragg)\n\ntheme_set(custom_ggplot_theme)\n\nset.seed(88)\nnsw_formula &lt;- as.formula(as.factor(treat) ~ age + educ + re75 + \n                          black + hisp + degree + marr)\n\nlogit_preds &lt;- glm(nsw_formula, data = nsw_data, \n                   family = binomial())$fitted.values \n\nrf_mtry1_preds &lt;- predict(randomForest(nsw_formula, \n                          mtry = 1, data = nsw_data), \n                          newdata = nsw_data, type = \"prob\")[, 2]\n\nbagging_model &lt;- randomForest(nsw_formula, mtry = 7, importance = TRUE, \n                              data = nsw_data)\n\nbagged_preds &lt;- predict(bagging_model, newdata = nsw_data, type = \"prob\")[, 2]\n\nplot_pmachines &lt;- function(pscores, plot_subtitle) {\n  ggplot(nsw_data, aes(x = pscores, fill = factor(treat))) +\n    geom_density(alpha = 0.6, linewidth = 0.6) +\n    scale_fill_manual(values = c(\"#e5e5e5\", \"#2780e3\"), \n                      labels = c(\"Control\", \"Participant\")) +\n    labs(subtitle = plot_subtitle, x = \"Propensity Scores\", y = \"Density\", \n         fill = \"Group:\") +\n    scale_x_continuous(expand = expansion(0), limits = c(0,1)) + \n    scale_y_continuous(expand = expansion(0), limits = c(0,10)) +\n    custom_ggplot_theme\n}\n\np1 &lt;- plot_pmachines(logit_preds, \"Logistic Regression\") + xlab(NULL) + \n  theme(legend.position = \"none\") + \n  annotate(geom = \"curve\", x = 0.6, y = 5, xend = 0.42, yend = 0, \n           curvature = .3, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 0.6, y = 5, label = \"True Probability\", \n           hjust = \"left\", color = \"#333333\", size = 3)\n\np2 &lt;- plot_pmachines(rf_mtry1_preds, \"Random Forest (mtry = 1)\") + xlab(NULL) + \n  theme(legend.position = \"none\")\np3 &lt;- plot_pmachines(bagged_preds, \"Bagging (Bootstrap Aggregation)\")\n\np1 / p2 / p3 + plot_annotation(\n  title = \"Density Plots of Propensity Scores for NSW Data\", theme = custom_ggplot_theme)\n\n\n\n\n\n\n\n\nFigure 2.1: Compares the density estimates of the propensity scores for control and participant groups in the National Supported Work programme. randomForest() fits a random forest with mtry = 1 and bagging ensemble with mtry = 7. The default values of ntree = 500 and nodesize = 1 are used. A logistic regression model is included for a comparison.\n\n\n\n\n\n\n\nCode: Create Figure 2.2\n# Create Figure 2.2\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nimp &lt;- as.data.frame(importance(bagging_model))\nimp &lt;- cbind(vars = rownames(imp), imp)\nimp &lt;- imp[order(imp$MeanDecreaseGini),]\nimp$vars &lt;- factor(imp$vars, levels = unique(imp$vars))\n\n\nimp %&gt;% \n  pivot_longer(cols = matches(\"Mean\")) %&gt;% \n  ggplot(aes(y = vars, x = value, fill = name)) +\n  geom_bar(stat = \"identity\", width = 0.8, show.legend = TRUE, \n           position = position_dodge(width = 0.8), color = \"black\", linewidth = 0.6) +\n  facet_grid(~ factor(name, \n                      levels = c(\"MeanDecreaseGini\", \"MeanDecreaseAccuracy\")), \n             scales = \"free_x\") +\n  scale_fill_manual(values = c(\"#e5e5e5\", \"#2780e3\")) +\n  scale_x_continuous(expand = expansion(c(0, 0.04))) +\n  labs(title = \"Variable Importance\",\n    x = \"% Decrease if Variable is Omitted from Model\", y = \"Variable Name\"\n  ) + custom_ggplot_theme + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 2.2: Compares the variable importance assigned to each variable from a bagging ensemble fitted on data from the National Supported Work programme. randomForest() fits a baggin ensemble with mtry = 7 with default ntree = 500 and nodesize = 1.\n\n\n\n\n\nFigure 2.1 shows the logistic regression model has identified a central tendency and most propensities are between \\(0.25\\) and \\(0.75\\) which roughly aligns with the true probability. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside \\(0.25\\) and \\(0.75\\) which is likely related to tree correlation. For the random forest with \\(mtry=1\\), a significant number of the treatment and control observations are centred near the control area (\\(T=0\\)) with a wide range of other predictions. Recall that the control group is the majority class. Reducing \\(mtry\\) from \\(7\\) to \\(1\\) reveals the majority class bias reinforcing the theoretical discussion that a combination of low \\(mtry\\) and class imbalance is especially troubling. The model over predicts the majority class and has unstable predictions otherwise. Both random forest and bagging ensembles have performed poorly compared to the true probability of \\(0.42\\%\\).\nThe tuning of \\(mtry\\) faces double jeopardy and is another important area of discussion in probability machines. The selection of \\(mtry\\) is typically carried out in with a classification loss function such as accuracy or out-of-bag error. Olson and Wyner (2018) compares tuning \\(mtry\\) measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of \\(mtry\\) for classification differs from probability prediction.2 In other words, if a grid search finds that \\(mtry=3\\) is optimal for a classification task, this does not imply that \\(mtry=3\\) is optimal for predicting probabilities. Putting this together, \\(mtry\\) is a double-edged sword, typically controlled with an imperfect method.\nRandom forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in Section 2.3. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.\nHeuristically, divergence bias and majority class bias will most affect a probability machine when there is considerable overlap of true probabilities between groups. Recall the meaning of common support and overlap from Section 1.5. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common support may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low \\(mtry\\), the terminal nodes of each tree may be relatively impure leading to a majority class bias. Alternatively, if true probabilities exist near \\(0\\) or \\(1\\) and there is a clear separation of class, divergence bias may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low \\(mtry\\) may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.\n\n\n2.2.3 Gradient Boosting Machines as Probability Machines\nMoving beyond classification trees in random forests or bagging ensembles, Friedman (2001) introduced the Gradient Boosting Machine (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss function to reach a minimum. A learning rate controls the contribution of each weak learner to the final model. By using a small learning rate, the machine learns slowly so that it can slowly descend the loss function. This allows for finer adjustments during the iterative process to better capture patterns in the data.\nGBMs can be generalised to many different applications by minimizing a different loss function which can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple regression trees and a logistic function to transform regression predictions into probabilities. Specifically, the logistic function used is:\n\\[\n\\widehat{\\Pr}(x_i=A) = \\frac{1}{1 + \\exp(-\\text{model}(x_i))}.\n\\tag{2.3}\\]\nThis logistic function is the same as in logistic regression, so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimise the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood with logistic regression. Maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The full GBM model, \\(f_T(x)\\) after \\(T\\) iterations can be written as:\n\\[\nf_T(x_i) = b_1(x_i) + \\lambda \\sum_{t=1}^{T} b_t(x_i).\n\\tag{2.4}\\]\nInside a base tree, each split considers all variables and makes the most informative split to descend the loss function using gradient descent. GBMs utilise many weak learners, such as a regression tree with a single split called a regression stump. However, additional splits enable the model to capture interactions between terms, which may increase probability calibration in complex or high-dimensional datasets.\nBy outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques, as well as allowing a probability distribution-based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. Since GBMs predict probabilities from a logistic function, they avoid problems associated with a vote count method. The implementation and workflow to fit a GBM for propensity scores is discussed in Section 3.1.\nFigure 2.3 shows the propensity scores resulting from a GBM model using the gbm package on the NSW data provides. A GBM is a notable performance improvement to random forest and bagging shown in Figure 2.1. Recall that a “better” model would predict probabilities near to \\(42\\%\\) as this is the treatment/control share in the randomised NSW data.\n\n\nCode: Create Figure 2.3\n# Create Figure 2.3\nset.seed(88)\nlibrary(gbm)\nnsw_gbm &lt;- gbm(treat ~ age + educ + re75 + black + hisp + degree + \n    marr, distribution = \"bernoulli\", data = nsw_data, n.trees = 10000, \n    shrinkage = 0.0001)\n\nboosted_preds &lt;- predict(nsw_gbm, type = \"response\")\n\nplot_pmachines(boosted_preds, \"Gradient Boosting Machine\") + \n    scale_y_continuous(expand = expansion(0), limits = c(0, 25)) + \n  annotate(geom = \"curve\", x = 0.6, y = 5, xend = 0.42, yend = 0, \n           curvature = .3, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 0.6, y = 5, label = \"True Probability\", \n           hjust = \"left\", color = \"#333333\", size = 3) + labs(\n  title = \"Density Plots of Propensity Scores for NSW Data\") + \n  custom_ggplot_theme\n\n\n\n\n\n\n\n\nFigure 2.3: Density estimates of the propensity scores for control and participant groups in the National Supported Work programme using the gbm package with distribution = \"bernoulli\", data = nsw_data, n.trees = 10000, and shrinkage = 0.0001.\n\n\n\n\n\n\n\n2.2.4 Overfitting\nOverfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model typically shows excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting can prevent overfitting to ensure a model can generalise to unseen data.\nHowever, the model is not required to generalise a propensity score model as different datasets will have a different model. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.\n\n\n\n\n\n\nNote 2.6: Overfitting in Logistic Regression\n\n\n\nThere is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is Schuster, Lowe, and Platt (2016), who suggest a general rule that the number of observations per parameter should be between \\(10\\) and \\(20\\). When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards \\(0\\) or \\(1\\) because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to \\(0\\) or \\(1\\) receive artificially inflated weighting.\n\n\nLee, Lessler, and Stuart (2010) simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.\nIf overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores with overfitting being a minor concern.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity Scores and Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/pscore_theory.html#sec-mlps-sims",
    "href": "chapters/pscore_theory.html#sec-mlps-sims",
    "title": "2  Propensity Scores and Machine Learning",
    "section": "2.3 Comparison of Machine Learning Algorithms: Simulation Results",
    "text": "2.3 Comparison of Machine Learning Algorithms: Simulation Results\nA small body of simulation studies benchmarks probability machines for predicting propensity scores (see McCaffrey, Ridgeway, and Morral 2004; Setoguchi et al. 2008; Lee, Lessler, and Stuart 2010; Cannas and Arpino 2019; Tu 2019; Goller et al. 2020; Ferri-García and Del Mar Rueda 2020). Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.\nTu (2019) compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.\nDespite poor theoretical properties as a probability machine, Lee, Lessler, and Stuart (2010) find that bagging results in the lowest standard error across many datasets.3 This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.\nAdditionally, Lee, Lessler, and Stuart (2010) finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best \\(95\\%\\) coverage in all simulation scenarios, with \\(98.6\\%\\) coverage.4\nCannas and Arpino (2019) also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation.5 This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.\nGoller et al. (2020) adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.6 7 Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.\nBased on a review of the literature, the findings can be distilled into five important points:\n\nProbability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.\nIn cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures (Rosenbaum and Rubin 1983).\nBagging ensembles perform poorly, a finding replicated across multiple studies.\nRandom forests can perform excellently when hyperparameters are satisfactorily tuned.\nFurther research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.\nA tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.\nA good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.\n\n\n\n\n\n\n\nAustin, Peter. 2011. “An introduction to propensity score methods for reducing the effects of confounding in observational studies.” Multivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C. 2008. “A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003.” Statistics in Medicine 27 (April): 2037–49. https://doi.org/10.1002/sim.3150.\n\n\nBader-El-Den, Mohammed, Eleman Teitei, and Todd Perry. 2019. “Biased Random Forest for Dealing with the Class Imbalance Problem.” IEEE Transactions on Neural Networks and Learning Systems 30 (7): 2163–72. https://doi.org/10.1109/TNNLS.2018.2878400.\n\n\nBreiman, Leo. 1996. “Bagging predictors.” Machine Learning 24: 123–40. https://doi.org/10.3390/risks8030083.\n\n\n———. 2001. “Random Forests.” Machine Learning 45: 5–32. https://doi.org/https://doi.org/10.1023/A:1010933404324.\n\n\nBreiman, L, Jerome H Friedman, Richard A Olshen, and C J Stone. 1984. “Classification and Regression Trees.” Biometrics 40: 874. https://api.semanticscholar.org/CorpusID:29458883.\n\n\nBrookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. “Variable selection for propensity score models.” American Journal of Epidemiology 163 (12): 1149–56. https://doi.org/10.1093/aje/kwj149.\n\n\nCannas, Massimo, and Bruno Arpino. 2019. “A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting.” Biometrical Journal 61 (4): 1049–72. https://doi.org/10.1002/bimj.201800132.\n\n\nCunningham, Scott. 2021. “Matching and Subclassification.” In Causal Inference: The Mixtape, 175–240. Yale University Press. https://doi.org/10.2307/j.ctv1c29t27.8.\n\n\nFerri-García, Ramón, and María Del Mar Rueda. 2020. “Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys.” PLoS ONE 15 (4): 1–19. https://doi.org/10.1371/journal.pone.0231500.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986.\n\n\nGoller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. “Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed.” Labour Economics 65 (March). https://doi.org/10.1016/j.labeco.2020.101855.\n\n\nHeinrich, Carolyn. 2010. “A Primer for Applying Propensity-Score Matching.” Development, no. August: 59. http://www.iadb.org/document.cfm?id=35320229.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nLee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. “Improving propensity score weighting using machine learning.” Statistics in Medicine 29: 337–46. https://doi.org/10.1002/sim.3782.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. “Propensity score estimation with boosted regression for evaluating causal effects in observational studies.” Psychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nOlson, Matthew A., and Abraham J. Wyner. 2018. “Making Sense of Random Forest Probabilities: a Kernel Perspective,” 1–35. http://arxiv.org/abs/1812.05792.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The central role of the propensity score in observational studies for causal effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1017/CBO9780511810725.016.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. “Propensity score model overfitting led to inflated variance of estimated odds ratios.” Journal of Clinical Epidemiology 80: 97–106. https://doi.org/10.1016/j.jclinepi.2016.05.017.\n\n\nSetoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. “Evaluating uses of data mining techniques in propensity score estimation: a simulation study.” Pharmacoepidemiology and Drug Safety 17 (March): 546–55. https://doi.org/10.1002/pds.\n\n\nSmith, Jeffrey A., and Petra E. Todd. 2005. Does matching overcome LaLonde’s critique of nonexperimental estimators? Vol. 125. 1-2 SPEC. ISS. https://doi.org/10.1016/j.jeconom.2004.04.011.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society 58 (1): 267–88. https://www.jstor.org/stable/2346178.\n\n\nTu, Chunhao. 2019. “Comparison of various machine learning algorithms for estimating generalized propensity score.” Journal of Statistical Computation and Simulation 89 (4): 708–19. https://doi.org/10.1080/00949655.2019.1571059.\n\n\nAustin, Peter. 2011. “An introduction to propensity score methods for reducing the effects of confounding in observational studies.” Multivariate Behavioral Research 46 (3): 399–424. https://doi.org/10.1080/00273171.2011.568786.\n\n\nAustin, Peter C. 2008. “A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003.” Statistics in Medicine 27 (April): 2037–49. https://doi.org/10.1002/sim.3150.\n\n\nBader-El-Den, Mohammed, Eleman Teitei, and Todd Perry. 2019. “Biased Random Forest for Dealing with the Class Imbalance Problem.” IEEE Transactions on Neural Networks and Learning Systems 30 (7): 2163–72. https://doi.org/10.1109/TNNLS.2018.2878400.\n\n\nBreiman, Leo. 1996. “Bagging predictors.” Machine Learning 24: 123–40. https://doi.org/10.3390/risks8030083.\n\n\n———. 2001. “Random Forests.” Machine Learning 45: 5–32. https://doi.org/https://doi.org/10.1023/A:1010933404324.\n\n\nBreiman, L, Jerome H Friedman, Richard A Olshen, and C J Stone. 1984. “Classification and Regression Trees.” Biometrics 40: 874. https://api.semanticscholar.org/CorpusID:29458883.\n\n\nBrookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. “Variable selection for propensity score models.” American Journal of Epidemiology 163 (12): 1149–56. https://doi.org/10.1093/aje/kwj149.\n\n\nCannas, Massimo, and Bruno Arpino. 2019. “A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting.” Biometrical Journal 61 (4): 1049–72. https://doi.org/10.1002/bimj.201800132.\n\n\nCunningham, Scott. 2021. “Matching and Subclassification.” In Causal Inference: The Mixtape, 175–240. Yale University Press. https://doi.org/10.2307/j.ctv1c29t27.8.\n\n\nFerri-García, Ramón, and María Del Mar Rueda. 2020. “Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys.” PLoS ONE 15 (4): 1–19. https://doi.org/10.1371/journal.pone.0231500.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986.\n\n\nGoller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. “Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed.” Labour Economics 65 (March). https://doi.org/10.1016/j.labeco.2020.101855.\n\n\nHeinrich, Carolyn. 2010. “A Primer for Applying Propensity-Score Matching.” Development, no. August: 59. http://www.iadb.org/document.cfm?id=35320229.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nLee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. “Improving propensity score weighting using machine learning.” Statistics in Medicine 29: 337–46. https://doi.org/10.1002/sim.3782.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. “Propensity score estimation with boosted regression for evaluating causal effects in observational studies.” Psychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nOlson, Matthew A., and Abraham J. Wyner. 2018. “Making Sense of Random Forest Probabilities: a Kernel Perspective,” 1–35. http://arxiv.org/abs/1812.05792.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The central role of the propensity score in observational studies for causal effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1017/CBO9780511810725.016.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. “Propensity score model overfitting led to inflated variance of estimated odds ratios.” Journal of Clinical Epidemiology 80: 97–106. https://doi.org/10.1016/j.jclinepi.2016.05.017.\n\n\nSetoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. “Evaluating uses of data mining techniques in propensity score estimation: a simulation study.” Pharmacoepidemiology and Drug Safety 17 (March): 546–55. https://doi.org/10.1002/pds.\n\n\nSmith, Jeffrey A., and Petra E. Todd. 2005. Does matching overcome LaLonde’s critique of nonexperimental estimators? Vol. 125. 1-2 SPEC. ISS. https://doi.org/10.1016/j.jeconom.2004.04.011.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society 58 (1): 267–88. https://www.jstor.org/stable/2346178.\n\n\nTu, Chunhao. 2019. “Comparison of various machine learning algorithms for estimating generalized propensity score.” Journal of Statistical Computation and Simulation 89 (4): 708–19. https://doi.org/10.1080/00949655.2019.1571059.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity Scores and Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/pscore_theory.html#footnotes",
    "href": "chapters/pscore_theory.html#footnotes",
    "title": "2  Propensity Scores and Machine Learning",
    "section": "",
    "text": "IPW is also commonly known as inverse probability of treatment weighting (IPTW). IPTW uses propensity scores and can equally be called IPW.↩︎\nNote that tuning \\(mtry\\) for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.↩︎\nIn this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.↩︎\nIn this context, the coverage is the proportion of times that the true treatment effect is within the \\(95\\%\\) confidence interval across the number of simulations. This author implements \\(1000\\) simulations of each scenario.↩︎\nCannas and Arpino (2019) provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the twang and gbm package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benefits discussed in Section 2.2.3.↩︎\nGoller et al. (2020) calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the true treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.↩︎\nTibshirani (1996) introduces LASSO regularization, short for Least Absolute Shrinkage and Selection Operator, is a technique used in linear regression to prevent overfitting by penalising the absolute size of the coefficients. It adds a penalty term to the ordinary least squares objective function, meaning that some coefficients may “shrink” to zero.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Propensity Scores and Machine Learning</span>"
    ]
  },
  {
    "objectID": "chapters/implimentation_workflow.html",
    "href": "chapters/implimentation_workflow.html",
    "title": "3  Tutorial: Implimentation, Workflow, and Example with WeightIt andgbm in R",
    "section": "",
    "text": "3.1 Hyperparameter Tuning and Workflow\nThe WeigthtIt package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called cobalt. The best information for this package can be found on this website or accessed with vignette(\"WeightIt\") inside R after installation using install.packages(\"WeightIt\").\nA workflow for hyperparameter tuning in WeightIt may be completed as follows:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R</span>"
    ]
  },
  {
    "objectID": "chapters/implimentation_workflow.html#sec-gbm-tune-workflow",
    "href": "chapters/implimentation_workflow.html#sec-gbm-tune-workflow",
    "title": "3  Tutorial: Implimentation, Workflow, and Example with WeightIt andgbm in R",
    "section": "",
    "text": "Specify the criterion option or measure of balance, which specifies the measure of the best model. The available measures are any balance measure that cobalt can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called sdm.mean or the smallest maximum SDM across covariates called sdm.max. It may be worthwhile to complete the tuning process with different tuning criteria.\nSet the number of trees high. The package default is n.trees = 10000 for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to ensure slow learners have the opportunity to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller n.tree if optimal.\nSpecify the grid search for the depth of the tree called interaction.depth and the learning rate called shrinkage. These values can be specified using c() such as shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3) or as integers such as interaction.depth = 1:5. These particular values are heuristically selected suggestions of good starting values. Additionally, an offset can be considered by performing a grid search across offset = c(TRUE, FALSE).1\n\n\nThe model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with my_weightit_object$info$best.tune.\nThe best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with interaction.depth = 1:5 and the best fit is \\(5\\), then a new search can consider interaction.depth = 3:7 so that the local area around \\(5\\) can be searched.\nExperiment with bag.fraction, which means each tree will consider a drawn proportion of observations equal to bag.fraction. Iteratively changing bag.fraction and assessing balance at each value should be practical. Consider \\(0.5\\), \\(0.67\\), and \\(1\\).\nAssess balance of covariates and model fit. Covariate balance can be assessed with a balance table such as bal.tab() or visualised using love.plot() from cobalt.\nThe tuned model is stated and reported to allow replicable results. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.\nEstimation and reporting of treatment effect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R</span>"
    ]
  },
  {
    "objectID": "chapters/implimentation_workflow.html#example-nsw-jobs-dataset-using-r",
    "href": "chapters/implimentation_workflow.html#example-nsw-jobs-dataset-using-r",
    "title": "3  Tutorial: Implimentation, Workflow, and Example with WeightIt andgbm in R",
    "section": "3.2 Example: NSW Jobs Dataset Using R",
    "text": "3.2 Example: NSW Jobs Dataset Using R\nFor demonstration, propensity scores are estimated following the workflow discussed in Section 3.1 to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in Appendix A. Randomisation should eliminate structural differences between groups, but Rosenbaum and Rubin (1983) notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using WeightIt and a logistic regression model using glm(). Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect.\n\n\n\n\n\n\nNote 3.1: Inverse Probability of Treatment Weighting\n\n\n\nInverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a pseudo-population where treatment assignment is independent of observed covariates, similar to a randomised controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.\n\n\n\n3.2.1 Step 1-6: Model Fitting and Tuning\nThe glm() function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the binomial(). Recall the nsw_formula is specified in Section 2.2.2.\n\nnsw_logit_pmodel &lt;- glm(nsw_formula, data = nsw_data,\n                        family=binomial())\n\nnsw_logit_pscores &lt;- nsw_logit_pmodel$fitted.values\n\n\nFits a logistic regression model using the glm() function specified to be a logistic model with family=binomial() using the previously created nsw_formula.\nExtracts the fitted values (propensity scores) from the model.\n\nThe weightit() function from the WeightIt package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called nsw_logit_weight.\nlibrary(WeightIt)\nnsw_logit_weight &lt;- weightit(nsw_formula, data = nsw_data,\n                             ps = nsw_logit_pscores,\n                             estimand = \"ATE\")\n\n\nSpecifies the formula and data.\nProvides weightit() with the propensity scores from the logistic regression function. Note that in practice this can be completed within the weightit() function with method = \"glm\". The separate estimation of the propensity scores is for illustrative purposes.\nSpecifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.\n\n\nA GBM model for propensity scores can be specified using method = \"gbm\" inside the weightit() function. To ensure consistent results, running set.seed(88) will ensure each tree uses the same seed if bag.fraction less than \\(1\\). The model is fit using the heuristically suggested starting values. Note that this model may take approximately \\(30\\) second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.\nset.seed(88)\nnsw_boosted_weight &lt;- weightit(nsw_formula, data = nsw_data,\n                               method = \"gbm\",\n                               estimand = \"ATE\",\n                               shrinkage = c(0.0005, 0.001, 0.05, \n                                             0.1, 0.2, 0.3),\n                               interaction.depth = 1:5,\n                               bag.fraction = 1,\n                               offset = c(TRUE, FALSE),\n                               criterion = \"smd.mean\",\n                               n.trees = 10000)\nprint(nsw_boosted_weight$info$best.tune)\n\n\nSpecifies the formula and data.\nSpecifies the propensity score prediction method to be a GBM and the estimand to the ATE.\nPerforms a grid search over these values of the learning rate and depth of tree.\nRequires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.\nDefines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be \\(10000\\) which is the package default.\nPrints the tune details of the model with the best covariate balance.\n\n\n\n\n  shrinkage interaction.depth distribution use.offset best.smd.mean best.tree\n6       0.3                 1    bernoulli      FALSE    0.02253485      2392\n\n\nThe best balance across all tuning combinations yields an average SMD of \\(0.023\\) showing strong balance compared to the \\(0.1\\) threshold. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in Section 3.2.2 accompanying a discussion of balance.\nThe best machine has a learning rate of \\(0.3\\) and contains \\(2392\\) decision stumps (trees with a depth of \\(1\\)). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to \\(0.3\\). A reduction in the depth of tree and number of trees will reduce computation time.\nThe new tune grid will consider shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5) as this allows the GBM to consider values between \\(0.2\\) and \\(0.3\\) and above \\(0.3\\) which were missing in the previous grid.\n\n\nCode: Fit weightit() with Updated Tune Grid\n# Additional weightit() GBM grid\nset.seed(88)\nnsw_boosted_weight2 &lt;- weightit(nsw_formula, data = nsw_data, \n                               method=\"gbm\",\n                               estimand = \"ATE\", \n                               shrinkage= c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5),\n                               interaction.depth = 1:3,\n                               bag.fraction = 1,\n                               offset = c(TRUE, FALSE),\n                               criterion = \"smd.mean\", \n                               n.trees = 5000)\n\nprint(nsw_boosted_weight2$info$best.tune)\n\n\n   shrinkage interaction.depth distribution use.offset best.smd.mean best.tree\n11      0.45                 2    bernoulli      FALSE    0.01965492        95\n\n\nComparing the two iterations, there is a reduction from \\(0.022\\) to \\(0.02\\) in the SMD. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of \\(0.45\\), a tree depth of \\(2\\), and \\(95\\) trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.\nPlotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, Figure 3.1 shows the optimal number of trees is highly variable. If the learning rate is set to shrinkage = 0.05, then the best balance is not achieved until near to \\(20,000\\) trees.\n\n\nCode: Create Figure 3.1\n# Create Figure 3.1\nlibrary(ggplot2)\nlibrary(patchwork)\nlow_shrinkage &lt;- weightit(nsw_formula, data = nsw_data, \n                               method = \"gbm\",\n                               estimand = \"ATE\", \n                               shrinkage = 0.05,\n                               interaction.depth = 1,\n                               offset = c(TRUE, FALSE),\n                               criterion = \"smd.mean\", \n                               n.trees = 40000)\n\noptimal_boost_plot &lt;- ggplot(nsw_boosted_weight2$info$tree.val, \n                             aes(x = tree, y = smd.mean)) +\n  geom_line(linewidth = 1, color = \"#2780e3\") + \n  labs(subtitle = \"Optimal Tune\",\n       x = \"Number of Iterations\",\n       y = \"Average Standardised Mean Difference\") +\n  xlim(0,500) + custom_ggplot_theme\n\nlowshrinkage_boost_plot &lt;- ggplot(low_shrinkage$info$tree.val, \n                                  aes(x = tree, y = smd.mean)) +\n  geom_line(linewidth = 1, color = \"#2780e3\") + \n  labs(subtitle = \"Low Learning Rate (shrinkage = 0.05)\",\n       x = \"Number of Iterations\", \n       y = NULL) +\n  annotate(geom = \"curve\", x = 30000, y = 0.05, \n           xend = low_shrinkage$info$best.tree, yend = 0.0231,\n           curvature = 0.3, arrow = arrow(length = unit(2, \"mm\"))) +\n  annotate(geom = \"text\", x = 31000, y = 0.05, label = \"Minimum\", \n           hjust = \"left\", color = \"#333333\", size = 3) + custom_ggplot_theme \n\noptimal_boost_plot + lowshrinkage_boost_plot + plot_annotation(\n  title = 'Number of Tree Iterations and Balance', theme = custom_ggplot_theme)\n\n\n\n\n\n\n\n\nFigure 3.1: Relatoinship between standardised mean difference, number of interations, and learning rate in a GBM model. Please note the difference in horozontal scale between the two learning rates. The model is fit using weightit from the WeightIt package.\n\n\n\n\n\nFor the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.\n\n\n3.2.2 Step 7 and 8: Assessing Balance\n\n\n\n\n\n\nThe Importance of Discussing Balance\n\n\n\nAssessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomised scenario. This step is vital for the reliability and validity of the study’s conclusions. King and Nielsen (2019) notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.\nA good resource of information for assessing balance is documentation from the cobalt package, which can be viewed by running vignette(“cobalt”, package = “cobalt”) in R.\n\n\nAs stated, cobalt provides very good integration with other related packages such as WeightIt for IPW and MatchIt for propensity score matching. Balance tables are created using bal.tab().\nlibrary(cobalt)\nnsw_logit_btab &lt;- bal.tab(nsw_logit_weight,\n                          data = nsw_data,\n                          stats = c(\"mean.diffs\",\"variance.ratios\"),\n                          binary = \"std\", continuous = \"std\",\n                          thresholds = c(mean.diffs = 0.1))\n\nnsw_logit_btab &lt;- nsw_logit_btab$Balance[-1,-c(2,3)]\n\n\nLoads the cobalt package. This assumes the package is already installed with install.packages(\"cobalt\")\nUses the bal.tab() fucntion to create balance statistics for the previously created nsw_logit_weight model.\nSpecifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.\nSets a threshold of balance to be \\(0.1\\) to determine if a covariate is balanced.\nExtracts the balance table of the nsw_logit_btab object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.\n\n\nAdditionally, bal.tab() will create balance tables for the GBM method’s IPWs and the raw data. For presentation, dplyr combines each of the individual balance tables for presentation using kable and kableExtra.\n\n\nCode: Create Balance Tables\n# Create balance tables\nnsw_boosted_btab &lt;- bal.tab(nsw_boosted_weight, \n                            data = nsw_data,\n                            stats = c(\"mean.diffs\",\"variance.ratios\"),\n                            binary = \"std\", continuous = \"std\",\n                            thresholds = c(mean.diffs = 0.1))\n\nnsw_raw_btab &lt;- bal.tab(nsw_formula, \n                        data = nsw_data, \n                        stats = c(\"mean.diffs\",\"variance.ratios\"),\n                        binary = \"std\", continuous = \"std\",\n                        thresholds = c(mean.diffs = 0.1),\n                        s.d.denom = \"treated\")\n\nnsw_boosted_btab &lt;- nsw_boosted_btab$Balance[-1, -c(2, 3)]\nnsw_raw_btab &lt;- nsw_raw_btab$Balance[-c(5, 6)]\n\n\n\n\nCode: Create Table 3.1\n# Create Table 3.1\nlibrary(tidyverse)\nlibrary(kableExtra)\n\ncollabels &lt;- c(\"Type\", \"SMD\", \"Balanced\", \"Variance Ratio\",\"Method\")\n\nrowlabels &lt;- c(\"Age\", \"Education\", \"Income 1975\",\"Black\", \n               \"Hispanic\", \"Degree\", \"Married\")\n\nnsw_raw_btab$method &lt;- \"Raw Data\"\nnsw_logit_btab$method &lt;- \"IPTW: Logistic Regression\"\nnsw_boosted_btab$method &lt;- \"IPTW: Boosting\"\n\ncombined_btab &lt;- bind_rows(setNames(nsw_raw_btab,collabels),\n                           setNames(nsw_logit_btab,collabels),\n                           setNames(nsw_boosted_btab,collabels))\n\ncombined_btab$Variable &lt;- rep(rowlabels,3)\n\ncombined_btab &lt;- combined_btab[c(6,1,2,3,4,5)]\n\nrownames(combined_btab) &lt;- NULL\n\ncombined_btab$Balanced &lt;- ifelse(\n          combined_btab$Balanced == \"Not Balanced, &gt;0.1\", \"No\", \"Yes\")\n\nkbl(combined_btab[-6], digits = 4, booktabs = TRUE, align = \"c\", \n      font_size = 10) %&gt;%\n  kable_styling(full_width = T) %&gt;%\n  row_spec(0, bold = TRUE) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  column_spec(2:5, bold = F, width = \"3cm\") %&gt;%\n  pack_rows(\"Raw Data\", 1, 7, label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Logistic Regression and IPTW\", 8, 14, \n             label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Boosting Machine and IPTW\", 15, 21,\n             label_row_css = \"text-align: center;\")\n\n\n\n\nTable 3.1: Standardised mean difference (a measure of balance) across different covariates in the National Supported Work data. The values are categorised for different propenensity score methods allowing a comparison. Balance tables are computed using bal.tab() from the cobalt package.\n\n\n\n\n\n\nVariable\nType\nSMD\nBalanced\nVariance Ratio\n\n\n\n\nRaw Data\n\n\nAge\nContin.\n0.1066\nNo\n1.0278\n\n\nEducation\nContin.\n0.1281\nNo\n1.5513\n\n\nIncome 1975\nContin.\n0.0824\nYes\n1.0763\n\n\nBlack\nBinary\n0.0449\nYes\nNA\n\n\nHispanic\nBinary\n-0.2040\nNo\nNA\n\n\nDegree\nBinary\n0.2783\nNo\nNA\n\n\nMarried\nBinary\n0.0902\nYes\nNA\n\n\nLogistic Regression and IPTW\n\n\nAge\nContin.\n-0.0001\nYes\n0.9809\n\n\nEducation\nContin.\n0.0012\nYes\n1.2725\n\n\nIncome 1975\nContin.\n0.0081\nYes\n0.7971\n\n\nBlack\nBinary\n0.0006\nYes\nNA\n\n\nHispanic\nBinary\n-0.0031\nYes\nNA\n\n\nDegree\nBinary\n0.0009\nYes\nNA\n\n\nMarried\nBinary\n0.0045\nYes\nNA\n\n\nBoosting Machine and IPTW\n\n\nAge\nContin.\n-0.0065\nYes\n0.9086\n\n\nEducation\nContin.\n0.0220\nYes\n1.1391\n\n\nIncome 1975\nContin.\n-0.0152\nYes\n1.0134\n\n\nBlack\nBinary\n0.0028\nYes\nNA\n\n\nHispanic\nBinary\n-0.0547\nYes\nNA\n\n\nDegree\nBinary\n0.0481\nYes\nNA\n\n\nMarried\nBinary\n0.0085\nYes\nNA\n\n\n\n\n\n\n\n\n\n\nTable 3.1 shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.\n\n\n3.2.3 Step 9: Results\nFinally, the treatment effect can be estimated using lm_weightit() from the WeightIt package and avg_comparisons() from the marginaleffects package. Note that the outcome variable is re78 which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.\nnsw_boosted_lm &lt;- lm_weightit(re78 ~ treat * (age + educ + re75 + black +\n                              hisp + degree + marr), data = nsw_data,\n                              weights = nsw_boosted_weight$weights)\n\nlibrary(marginaleffects)\nnsw_boosted_result &lt;- avg_comparisons(nsw_boosted_lm, variables = \"treat\")\n\n\nUses lm_weightit() to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that * indicates multiplication in R.\nSpecifies the weights from the nsw_boosted_weight object created earlier by the weightit() function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by nsw_boosted_weight$weights.\nEstimates a comparison between the potential outcomes as well as standard errors for inference.\n\n\nAdditionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.\n\n\nCode: Create Table 3.2\n# Create Table 3.2\nnsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + \n                             re75 + black + hisp + \n                             degree + marr), data = nsw_data, \n                             weights = nsw_logit_weight$weights)\n\nnsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = \"treat\")\n\nnsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),\n                             extract_comparison_results(nsw_boosted_result))\nrownames(nsw_comparisons_tab) &lt;- c(\"Logistic Regression\", \"GBM\")\n\nknitr::kable(nsw_comparisons_tab, digits = 4)\n\n\n\n\nTable 3.2: Comparison of estimates of the average treatment for the National Supported Work data.\n\n\n\n\n\n\n\nEstimate\nSE\nP.Value\nLower.CI\nUpper.CI\n\n\n\n\nLogistic Regression\n1610.786\n668.4870\n0.0160\n300.5756\n2920.997\n\n\nGBM\n1609.947\n669.4201\n0.0162\n297.9081\n2921.987\n\n\n\n\n\n\n\n\nTable 3.2 shows that both estimates of the treatment effect are nearly identical at \\(\\$1610\\) with logistic regression inferring a \\(\\$0.86\\) larger treatment effect. Additionally, these results are statistically significant at the \\(5\\%\\) level with nearly identical standard errors.\n\n\n\n\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. “Propensity score estimation with boosted regression for evaluating causal effects in observational studies.” Psychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nRidgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. “Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package.” https://doi.org/10.7249/tl136.1.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The central role of the propensity score in observational studies for causal effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1017/CBO9780511810725.016.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://www.jstor.org/stable/2699986.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nMcCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. “Propensity score estimation with boosted regression for evaluating causal effects in observational studies.” Psychological Methods 9 (4): 403–25. https://doi.org/10.1037/1082-989X.9.4.403.\n\n\nRidgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. “Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package.” https://doi.org/10.7249/tl136.1.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The central role of the propensity score in observational studies for causal effects.” Biometrika 70 (1): 41–55. https://doi.org/10.1017/CBO9780511810725.016.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R</span>"
    ]
  },
  {
    "objectID": "chapters/implimentation_workflow.html#footnotes",
    "href": "chapters/implimentation_workflow.html#footnotes",
    "title": "3  Tutorial: Implimentation, Workflow, and Example with WeightIt andgbm in R",
    "section": "",
    "text": "In the context of gradient boosting machines (GBMs) or boosted logistic regression models, an offset refers to an additional term that is included in the model to account for a known effect or baseline value that should be factored into the prediction, but is not estimated by the model itself. In gbm the offset is estimated using conventional logistic regression.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R</span>"
    ]
  },
  {
    "objectID": "chapters/coffee_replication.html",
    "href": "chapters/coffee_replication.html",
    "title": "4  Replication Case Study",
    "section": "",
    "text": "4.1 Replication of Original Results\nJena et al. (2012) provides a replication package including Stata code that uses Stata’s psmatch2 package to perform nearest neighbour matching with replacement and common support trimming. Common support trimming means that any observations outside the commonly overlapping are are discarded. The results of the paper are be fully replicated using the MatchIt package inside R.\nCode: Create Table 4.2\n# Create Table 4.2\nlibrary(MatchIt)\nlibrary(MatchItSE)\nlibrary(marginaleffects)\n\ncoffee_formula &lt;- as.formula(certified ~ age_hh + agesq + nonfarmincome_access +\n                             depratio + logtotal_land + badweat + edu + gender + \n                             years_cofeproduction + access_credit)\n\ncoffee_rep_pmodel &lt;- matchit(coffee_formula, data = coffee_data, \n                             distance = \"glm\", method = \"nearest\",\n                             replace = TRUE, estimand = \"ATT\", discard = \"both\") \n\ncoffee_logit_md &lt;- match.data(coffee_rep_pmodel)\n\ncoffee_rep_fit&lt;- lm(percapitaincome_day_maleeq ~ certified,\n                    data = coffee_logit_md, weights = weights)\n\nreplicated_result &lt;- avg_comparisons(coffee_rep_fit, variables = \"certified\",\n                                     vcov = NULL, \n                                     newdata = subset(coffee_logit_md, \n                                                      certified == 1),\n                                     wts = \"weights\")\n\nai_se &lt;- abadie_imbens_se(obj = coffee_rep_pmodel, \n                          Y = coffee_data$percapitaincome_day_maleeq)\n\nreplicated_result_tbl &lt;- extract_comparison_results(replicated_result)\n\nreplicated_result_tbl$SE &lt;- ai_se\n\nrownames(replicated_result_tbl) &lt;- \"Replicated Result\"\n\nknitr::kable(replicated_result_tbl, digits = 4)\n\n\n\n\nTable 4.2: Replication of results in Jena et al. (2012). Note a slight difference in standard error which should be \\(1.1\\) as the MatchItSE package uses a trivially different method than psmatch2 in Stata (see Abadie and Imbens 2006). Matching is performed by matchit() from the MatchIt package.\n\n\n\n\n\n\n\nEstimate\nSE\nP.Value\nLower.CI\nUpper.CI\n\n\n\n\nReplicated Result\n-0.1538\n0.9898\n0.835\n-1.6009\n1.2934\nTable 4.2 shows the replicated result obtained by Jena et al. (2012). The intriguing finding of the paper is that the average treatment effect on the treated (ATT) is negative. That is, of the farmers that become certified, their per capita income is expected to decrease by \\(\\$0.15\\) per day. Intuition and proponents of certification schemes suggest that certification leads to an increase of income. If certification negatively impacts income, it would call into question a significant effort to engage in certification and fair trade practices.\nJena et al. (2012) does not perform any discussion or consideration of balance in their paper and so it is unclear if propensity score matching results in covariate balance. The cobalt package creates balance tables using bal.tab() and a visualisation using love.plot().\nCode: Create Table 4.3\n# Create Table 4.3\nlibrary(cobalt)\ncoffee_rep_btab &lt;- bal.tab(coffee_rep_pmodel, \n                        data = coffee_data, \n                        stats = c(\"mean.diffs\",\"variance.ratios\"),\n                        binary = \"std\", continuous = \"std\",\n                        thresholds = c(mean.diffs = 0.1),\n                        s.d.denom = \"treated\")\n\ncoffee_rep_btab_ss &lt;- coffee_rep_btab$Observations\n\ncoffee_rep_btab &lt;- coffee_rep_btab$Balance[-1,-c(2,3)]\n\nrowlabels &lt;- c(\n  \"Household Age\", \"Squared Household Age\", \"Non-farm Income Access\", \n  \"Log Total Land\", \"Dependency Ratio\", \"Bad Weather\",\n  \"Education Level\", \"Gender\", \"Years of Coffee Production\", \n  \"Access to Credit\")\n\ncolnames &lt;- c(\"Variable\",\"Type\", \"SMD\", \"Balance Threshold\", \"Variance Ratio\")\n\nrownames(coffee_rep_btab) &lt;- rowlabels\n\ncoffee_rep_btab[,3] &lt;- ifelse(\n          coffee_rep_btab[,3] &gt;= \"Not Balanced, &gt;0.1\", \"No\", \"Yes\")\n\nknitr::kable(coffee_rep_btab, digits = 4, align = \"c\")\n\n\n\n\nTable 4.3: The standardised mean difference (SMD) for each covariate in Jena et al. (2012) using a logistic regression propensity model and propensity score matching. Across each of the covariates, a balance threshold is set at \\(0.1\\) to indicate if a covariate is balanced. Binary and continuous variables are both standardised over the treatment group. SMDs are computed using bal.tab() from the cobalt package.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nDiff.Adj\nM.Threshold\nV.Ratio.Adj\n\n\n\n\nHousehold Age\nContin.\n-0.2723\nNo\n1.0728\n\n\nSquared Household Age\nContin.\n-0.2552\nNo\n1.1430\n\n\nNon-farm Income Access\nBinary\n0.3005\nNo\nNA\n\n\nLog Total Land\nContin.\n0.2597\nNo\n1.2969\n\n\nDependency Ratio\nContin.\n-0.3996\nNo\n0.9789\n\n\nBad Weather\nBinary\n0.2016\nNo\nNA\n\n\nEducation Level\nContin.\n0.2443\nNo\n1.0338\n\n\nGender\nBinary\n-0.1324\nNo\nNA\n\n\nYears of Coffee Production\nContin.\n-0.3400\nNo\n0.9111\n\n\nAccess to Credit\nBinary\n0.1949\nNo\nNA\nCode: Create Figure 4.1\n# Create Figure 4.1\nlibrary(ggplot2)\nlove.plot(coffee_formula, data = coffee_data, \n          weights = list(Replication = coffee_rep_pmodel),\n          sample.names = c(\"Unadjusted\", \"Replication\"),\n          var.order = \"unadjusted\", binary = \"std\",\n          abs = TRUE, colors = c(\"#333333\", \"#2780e3\"), \n          shapes = c(\"circle filled\", \"circle filled\"),\n          line = TRUE, thresholds = 0.1, s.d.denom = \"treated\") +\n  labs(title = \"Variable Balance of Replication\",\n       x = \"Absolute Standardised Mean Differences\", fill = \"Method\") +\n  scale_x_continuous(breaks = seq(0,0.6,length.out=7),\n                     expand = expansion(c(0, 0.05))) + \n  custom_ggplot_theme \n\n\n\n\n\n\n\n\nFigure 4.1: A visual representation of Table 4.3 called a love plot. Additionally, the unadjusted (raw data) SMDs are displayed for comparison. Variables are ordered by the SMD in the unadjusted data. Plot is created using love.plot() from the cobalt package.\nTable 4.3 and Figure 4.1 show that propensity score matching has obtained very poor balance. Based on the \\(0.10\\) rule discussed in Section 2.1.1, not a single variable is balanced and so the estimate of the treatment effect is likely to be biased by structural differences between control and certified.\nFour key variables: age, gender, education, and access to credit all exhibit poor balance. These variables are strong confounders in theory and so emphasising balance in these variables is critical to making a robust causal inference. Perhaps there is gender or age discrimination in the certification process. Perhaps, those with lesser education may struggle to obtain certification. Perhaps those who have less access to credit are unable to afford to become certified. Moving forward, these variables must exhibit better covariate balance to make a robust conclusion.\nFigure 4.2 shows the effect of common support trimming. Table 4.4 shows 34 total observations are dropped of which 33 are treated and 1 are control. By dropping these observations, PSM avoids making poor matches which should lead to better covariate balance. When observations are discarded, the estimand is no longer the ATT. Instead, it is refereed to as the average treatment effect on the matched or ATM. There is a significant reduction in the effective sample size in the control group from \\(82\\) to \\(21\\) individuals.\nCode: Create Figure 4.2\n# Create Figure 4.2\ndiscarded_scores &lt;- coffee_rep_pmodel$distance[coffee_rep_pmodel$discarded]\n\ndiscard_min &lt;- min(discarded_scores, na.rm = TRUE)\ndiscard_max &lt;- max(discarded_scores, na.rm = TRUE)\n\nggplot(coffee_data, aes(x = coffee_rep_pmodel$distance, \n                        fill = factor(certified))) +\n  geom_density(alpha = 0.6, linewidth = 0.6) +\n  scale_fill_manual(values = c(\"#e5e5e5\", \"#2780e3\"), \n                    labels = c(\"Control\", \"Certified\")) +\n  labs(title = \"Distribution of Propensity Scores in Replication\", \n       x = \"Propensity Scores\", y = \"Density\", fill = \"Group:\") +\n  scale_x_continuous(expand = expansion(0), limits = c(0, 1)) + \n  scale_y_continuous(expand = expansion(0), limits = c(0, 3)) +\n  geom_vline(xintercept = discard_max, color = \"#333333\", linewidth = 0.8) +\n  annotate(\"rect\", xmin = 0, xmax = discard_min, ymin = -Inf, ymax = Inf, \n           fill = \"#333333\", alpha = 0.2) +\n  annotate(\"rect\", xmin = discard_max, xmax = 1, ymin = -Inf, ymax = Inf, \n           fill = \"#333333\", alpha = 0.2) +\n  annotate(\"text\", x = 0.02, y = 1.5, \n           label = \"Discarded Range\", angle = 90, vjust = 1.5, size = 4,\n           fontface = \"bold\", color = \"#333333\") +\n  custom_ggplot_theme\n\n\n\n\n\n\n\n\nFigure 4.2: Density estimates of the propensity scores from Jena et al. (2012) using logistic regression. Propensity score matching discards some observations as displayed by the discarded range on the left. A single observation is discarded on the right.\nCode: Create Table 4.4\n# Create Table 4.4\ncolnames(coffee_rep_btab_ss) &lt;- c(\"Control\", \"Certified\")\n\nknitr::kable(coffee_rep_btab_ss, digits=0, align = \"c\")\n\n\n\n\nTable 4.4: The effective sample size resulting from the use of propensity score matching in Jena et al. (2012). The effective sample size (ESS) is displayed in the unweighted (raw) and matched data as well as the number of disarded observations. Computed using bal.tab() from the cobalt package.\n\n\n\n\n\n\n\nControl\nCertified\n\n\n\n\nAll (ESS)\n82\n164\n\n\nAll (Unweighted)\n82\n164\n\n\nMatched (ESS)\n21\n131\n\n\nMatched (Unweighted)\n42\n131\n\n\nUnmatched\n39\n0\n\n\nDiscarded\n1\n33\nOverall, the propensity score matching in Jena et al. (2012) is poor and results in unbalanced covariates and a loss of estimand.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Replication Case Study</span>"
    ]
  },
  {
    "objectID": "chapters/coffee_replication.html#further-modelling",
    "href": "chapters/coffee_replication.html#further-modelling",
    "title": "4  Replication Case Study",
    "section": "4.2 Further Modelling",
    "text": "4.2 Further Modelling\nTo improve the poor balance achieved by the Jena et al. (2012), there are two strategies to obtain better balance. First, the propensity scores can be re-estimated using machine learning to obtain better calibrated propensity scores. Second, inverse propensity weighting (IPW) can be used instead of propensity score matching (PSM). IPW should ensure that the sample size remains the same as no observations are lost through a matching process. IPW should retain all observations and preserve the estimand as the ATT. Additionally, IPW is generally more efficient as a pseudo-population is based on precise weights compared to matched observations based on approximate similarity.\nThe machine learning propensity scores will be estimated using the WeightIt package in the same process as Chapter 3. The model will be used using criterion = \"smd.mean\" for simplicity.\n\n\nCode: Perform IPW with a GBM\n# Perform IPW with a GBM\nlibrary(WeightIt)\nlibrary(cobalt)\n\nset.seed(88)\ncoffee_boosted_weight &lt;- weightit(coffee_formula, data = coffee_data, \n                                  method = \"gbm\", distribution = \"bernoulli\",\n                                  use.offset = T,\n                                  shrinkage = seq(0.15, 0.4, length.out = 5),\n                                  bag.fraction = 0.67, \n                                  interaction.depth = 3:6,\n                                  n.trees = 500,\n                                  criterion = \"smd.mean\", \n                                  estimand = \"ATT\")\n\ncoffee_boosted_btab &lt;- bal.tab(coffee_boosted_weight, data = coffee_data, \n                               stats = c(\"mean.diffs\", \"variance.ratios\"),\n                               binary = \"std\", continuous = \"std\",\n                               thresholds = c(mean.diffs = 0.1),\n                               s.d.denom = \"treated\")\n\ncoffee_boosted_btab &lt;- coffee_boosted_btab$Balance[-1, -c(2,3)]\n\n\n\n\n\n\n\n\nNote 4.1: Discussion of Tuning\n\n\n\nInitially, a tuning grid considering shrinkage values of \\(0.001,0.005, 0.01, 0.05, 0.1, 0.2, text{ and }0.3\\) were considered using \\(10000\\) trees with a depth between \\(1\\) and \\(5\\). The best tuning performance was found with shrinkage of \\(0.2\\) and \\(13\\) trees which were three splits \\(2\\) deep.\nAs such my later tuning grids considered higher learning rates and a smaller number of trees. The third and final iteration of the tuning grid searches between \\(0.15, 0.2125, 0.275, 0.3375, \\text{ and }0.4\\), between \\(3\\) and \\(6\\) splits deep, uses an offset, and randomly selects \\(67\\%\\) of the data at each tree.\n\n\n\n\nCode: Perform IPW with Logistic Regression\n# Perform IPW with Logistic Regression\ncoffee_logit_weight &lt;- weightit(coffee_formula, data = coffee_data, \n                                method= \"glm\", estimand = \"ATT\")\n\ncoffee_logit_btab &lt;- bal.tab(coffee_logit_weight, data = coffee_data,\n                             formula = coffee_formula,\n                             stats = c(\"mean.diffs\", \"variance.ratios\"),\n                             binary = \"std\", continuous = \"std\",\n                             thresholds = c(mean.diffs = 0.1),\n                             s.d.denom = \"treated\")\n\ncoffee_logit_btab &lt;- coffee_logit_btab$Balance[-1, -c(2,3)]\n\n\nOf course there is no guarantee that the GBM model will perform the best and so a logistic model is also fitted. An interesting comparison is between the SMDs in the matched data and in the weighted sample. Any differences between the two samples relates to the difference between PSM and IPW as the propensity scores are identical.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Replication Case Study</span>"
    ]
  },
  {
    "objectID": "chapters/coffee_replication.html#comparison-of-methods",
    "href": "chapters/coffee_replication.html#comparison-of-methods",
    "title": "4  Replication Case Study",
    "section": "4.3 Comparison of Methods",
    "text": "4.3 Comparison of Methods\nAs before, cobalt creates a balance table and love plot.\n\n\nCode: Create Table 4.5.\n# Create Table 4.5\nlibrary(\"data.table\")\nlibrary(cobalt)\nlibrary(kableExtra)\nlibrary(tidyverse)\ncoffee_raw_btab &lt;- bal.tab(coffee_formula, data = coffee_data, \n                        stats = c(\"mean.diffs\",\"variance.ratios\"),\n                        binary = \"std\", continuous = \"std\",\n                        thresholds = c(mean.diffs = 0.1),\n                        s.d.denom = \"treated\")\n\ncoffee_raw_btab &lt;- coffee_raw_btab$Balance[,-c(5,6)]\n\ncoffee_combined_btab &lt;- rbindlist(list(coffee_raw_btab,\n                                       coffee_logit_btab,\n                                       coffee_boosted_btab), \n                                  use.names = FALSE)\n\ncoffee_combined_btab$Variable &lt;- rep(rowlabels, 3)\n\ncoffee_combined_btab &lt;- coffee_combined_btab[, c(5, 1, 2, 3, 4)]\n\ncoffee_combined_btab[, 4] &lt;- ifelse(\n          coffee_combined_btab[, 4] &gt;= \"Not Balanced, &gt;0.1\", \"No\", \"Yes\")\n\nkbl(coffee_combined_btab, digits = 4, booktabs = TRUE, align = \"c\", \n    font_size = 10, col.names = colnames) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  column_spec(2:5, bold = FALSE, width = \"2cm\") %&gt;%\n  pack_rows(\"Raw Data\", 1, 10, label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Logistic Regression and IPTW\", 11, 20, \n            label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Boosting Machine with IPTW\", 21, 30, \n            label_row_css = \"text-align: center;\")\nkbl(combined_btab[-6], digits = 4, booktabs = TRUE, align = \"c\", \n      font_size = 10) %&gt;%\n  kable_styling(full_width = T) %&gt;%\n  row_spec(0, bold = TRUE) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  column_spec(2:5, bold = F, width = \"3cm\") %&gt;%\n  pack_rows(\"Raw Data\", 1, 7, label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Logistic Regression and IPTW\", 8, 14, \n             label_row_css = \"text-align: center;\") %&gt;%\n  pack_rows(\"Boosting Machine and IPTW\", 15, 21,\n             label_row_css = \"text-align: center;\")\n\n\n\n\nTable 4.5: Comparison of standardised mean difference (SMD) using different propensity score models. Across each of the covariates, a balance threshold is set at \\(0.1\\) to indicate if a covariate is balanced. Binary and continuous variables are both standardised over the treatment group. SMDs are computed using bal.tab() from the cobalt package.\n\n\n\n\n\n\nVariable\nType\nSMD\nBalance Threshold\nVariance Ratio\n\n\n\n\nRaw Data\n\n\nHousehold Age\nContin.\n0.5634\nNo\n0.8650\n\n\nSquared Household Age\nContin.\n0.4912\nNo\n1.0070\n\n\nNon-farm Income Access\nBinary\n-0.3928\nNo\nNA\n\n\nLog Total Land\nContin.\n-0.4048\nNo\n0.5507\n\n\nDependency Ratio\nContin.\n0.0487\nYes\n1.2371\n\n\nBad Weather\nBinary\n-0.2505\nNo\nNA\n\n\nEducation Level\nContin.\n-0.0020\nYes\n0.7272\n\n\nGender\nBinary\n-0.2750\nNo\nNA\n\n\nYears of Coffee Production\nContin.\n0.4557\nNo\n1.3621\n\n\nAccess to Credit\nBinary\n0.5968\nNo\nNA\n\n\nLogistic Regression and IPTW\n\n\nHousehold Age\nContin.\n0.2449\nNo\n0.9275\n\n\nSquared Household Age\nContin.\n0.2277\nNo\n1.0724\n\n\nNon-farm Income Access\nBinary\n0.1701\nNo\nNA\n\n\nLog Total Land\nContin.\n-0.0923\nYes\n0.8564\n\n\nDependency Ratio\nContin.\n0.1138\nNo\n1.3877\n\n\nBad Weather\nBinary\n0.1941\nNo\nNA\n\n\nEducation Level\nContin.\n0.0473\nYes\n0.9217\n\n\nGender\nBinary\n-0.0465\nYes\nNA\n\n\nYears of Coffee Production\nContin.\n-0.0613\nYes\n1.1125\n\n\nAccess to Credit\nBinary\n-0.0292\nYes\nNA\n\n\nBoosting Machine with IPTW\n\n\nHousehold Age\nContin.\n0.0669\nYes\n1.2690\n\n\nSquared Household Age\nContin.\n0.0989\nYes\n1.4911\n\n\nNon-farm Income Access\nBinary\n0.0579\nYes\nNA\n\n\nLog Total Land\nContin.\n-0.0284\nYes\n0.8760\n\n\nDependency Ratio\nContin.\n-0.0623\nYes\n0.7660\n\n\nBad Weather\nBinary\n0.1915\nNo\nNA\n\n\nEducation Level\nContin.\n0.1377\nNo\n1.0735\n\n\nGender\nBinary\n-0.0816\nYes\nNA\n\n\nYears of Coffee Production\nContin.\n-0.0055\nYes\n0.9704\n\n\nAccess to Credit\nBinary\n0.1231\nNo\nNA\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nSMD\nBalanced\nVariance Ratio\n\n\n\n\nRaw Data\n\n\nAge\nContin.\n0.1066\nNo\n1.0278\n\n\nEducation\nContin.\n0.1281\nNo\n1.5513\n\n\nIncome 1975\nContin.\n0.0824\nYes\n1.0763\n\n\nBlack\nBinary\n0.0449\nYes\nNA\n\n\nHispanic\nBinary\n-0.2040\nNo\nNA\n\n\nDegree\nBinary\n0.2783\nNo\nNA\n\n\nMarried\nBinary\n0.0902\nYes\nNA\n\n\nLogistic Regression and IPTW\n\n\nAge\nContin.\n-0.0001\nYes\n0.9809\n\n\nEducation\nContin.\n0.0012\nYes\n1.2725\n\n\nIncome 1975\nContin.\n0.0081\nYes\n0.7971\n\n\nBlack\nBinary\n0.0006\nYes\nNA\n\n\nHispanic\nBinary\n-0.0031\nYes\nNA\n\n\nDegree\nBinary\n0.0009\nYes\nNA\n\n\nMarried\nBinary\n0.0045\nYes\nNA\n\n\nBoosting Machine and IPTW\n\n\nAge\nContin.\n-0.0065\nYes\n0.9086\n\n\nEducation\nContin.\n0.0220\nYes\n1.1391\n\n\nIncome 1975\nContin.\n-0.0152\nYes\n1.0134\n\n\nBlack\nBinary\n0.0028\nYes\nNA\n\n\nHispanic\nBinary\n-0.0547\nYes\nNA\n\n\nDegree\nBinary\n0.0481\nYes\nNA\n\n\nMarried\nBinary\n0.0085\nYes\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nCode: Create Figure 4.3\n# Create Figure 4.3\nlove.plot(coffee_formula,\n          data = coffee_data, \n          weights = list(Replication = coffee_rep_pmodel,\n                         Logit = coffee_logit_weight,\n                         Boosting= coffee_boosted_weight),\n          var.order = \"unadjusted\", binary = \"std\", continuous = \"std\",\n          abs = TRUE, colors = c(\"#333333\", \"#2780e3\", \"darkblue\",\"darkred\"), \n          shapes = rep(\"circle filled\", 4),\n          line = TRUE, thresholds = 0.1, s.d.denom = \"treated\", use.grid = F) +\n  labs(title = \"Variable Balance Using Different Balance Methods\",\n       x = \"Absolute Standardised Mean Differences\",\n       fill = \"Method\") +\n  scale_x_continuous(breaks = seq(0,0.6,length.out=7),\n                     expand = expansion(c(0, 0.05))) + \n  custom_ggplot_theme \n\n\n\n\n\n\n\n\nFigure 4.3: Visual representation of Table 4.5 called a love plot and displays the standardised mean difference (SMD) of covariates in Jena et al. (2012). Additionally, the unadjusted SMDs are displayed for comparison. Variables are ordered by the SMD in the unadjusted data.\n\n\n\n\n\nThere are three notable findings:\n\nPSM has performed very poorly relative to IPW even when matching dropps a significant number of observations.\nA GBM model has resulted in better covariate balance than logistic regression for most covariates. Using a \\(0.1\\) guidline for determining balance, logistic regression leaves \\(5\\) variables unbalanced and the GBM leaves \\(3\\) variables unbalanced. Additionally, the degree of unbalance is larger for logistic regression.\nLogistic regression has a satisfactory average SMD of \\(0.077\\). Boosting has an average SMD of \\(0.0498\\) which is excellent and narrowly meets a rigorous threshold of \\(0.05\\).\nThe covariate with the highest SMD is household age (\\(0.245\\)) in logistic regression and bad weather (\\(0.191\\)) in the GBM.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Replication Case Study</span>"
    ]
  },
  {
    "objectID": "chapters/coffee_replication.html#results",
    "href": "chapters/coffee_replication.html#results",
    "title": "4  Replication Case Study",
    "section": "4.4 Results",
    "text": "4.4 Results\nNow that satisfactory covariate balance is achieved, the treatment effect can be estimated under logistic regression, the GBM, and then compared to the result in the paper. Note that the estimand in the paper is intended to be the average treatment effect (ATT) but dropped observations mean the actual treatment effect is the average treatment effect on matched (ATM) individuals. In theory, better covariate balance should lead to a better estimate of the ATT so a comparison of the estimates is interesting. As in Section 3.2.3, the results will be completed using G-computation with the lm_weightit() and avg_comparisons() functions.\n\n\nCode: Create Table 4.6\n# Create Table 4.6\ncoffee_att_formula &lt;- update.formula(as.formula(\n  paste(\"~\", paste(attr(terms(coffee_formula), \"term.labels\"), \n                   collapse = \" + \"))), \n  percapitaincome_day_maleeq ~ certified * .)\n\n\ncoffee_logit_fit &lt;- lm_weightit(coffee_att_formula,\n                    data = coffee_data, weightit = coffee_logit_weight)\n\ncoffee_boosted_fit &lt;- lm_weightit(coffee_att_formula,\n                                 data = coffee_data, \n                                 weightit = coffee_boosted_weight)\n\n\ncoffee_logit_att &lt;- avg_comparisons(coffee_logit_fit, variables = \"certified\")\n\ncoffee_boosted_att &lt;- avg_comparisons(coffee_boosted_fit, \n                                      variables = \"certified\")\n\ncoffee_comparisons_tab &lt;- rbind(replicated_result_tbl, \n                                extract_comparison_results(coffee_logit_att),\n                             extract_comparison_results(coffee_boosted_att))\n\nrownames(coffee_comparisons_tab) &lt;- c(\"Rep. Result (Logistic with PSM)\",\n                                      \"Logistic Regression and IPW\", \n                                      \"Generalized Boosting Machine and IPW\")\n\nknitr::kable(coffee_comparisons_tab, digits = 4)\n\n\n\n\nTable 4.6: Estimates of the average treatment effect on the treated of certification on per capita income across different propensity score models and methods. Created using WeightIt, MatchIt, and Cobalt packages.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nSE\nP.Value\nLower.CI\nUpper.CI\n\n\n\n\nRep. Result (Logistic with PSM)\n-0.1538\n0.9898\n0.8350\n-1.6009\n1.2934\n\n\nLogistic Regression and IPW\n-1.5824\n0.6072\n0.0092\n-2.7724\n-0.3924\n\n\nGeneralized Boosting Machine and IPW\n-1.0187\n0.5196\n0.0499\n-2.0372\n-0.0003\n\n\n\n\n\n\n\n\nTable 4.6 shows the estimates of the treatment effect across different methods. Recall that Jena et al. (2012) estimate a an effect of \\(-0.15\\) implying that daily income reduces by \\(\\$0.15\\) if a farmer becomes certified. This result is not statistically significant.\nThe IPW estimate is \\(-1.58\\) implying that certification leads to a \\(\\$1.58\\) decrease in daily income. This coefficient is much larger than than the original paper by a magnitude of \\(10\\). Additionally, this estimate is statistically significant at the \\(1\\%\\) level. The GBM estimate is \\(-1.02\\) which predicts a decrease in daily income by \\(\\$1.02\\) when a farmer becomes certified. This finding is statistically significant at the \\(5\\%\\) level.\nThe most interesting result is that the estimates become even more negative. One may expect that the result from a better balanced sample would become positive to align with theoretical motivations for certification policies. Jena et al. (2012) presented two explanations for why certification shows no positive impact. First, the authors note that the prices offered by certified cooperatives are not significantly different from those provided by non-certified cooperatives. Second, a substantial portion of coffee—about \\(75\\%\\) is sold to private traders, who often pay higher prices to non-certified farmers. Additionally, from qualitative interviews with farmers, the authors note that policies and arrangements within different cooperatives exhibit heterogeneity so the impact of certification may relate more to the structure of the cooperatives not merely being certified.\nThe reason for a large difference is twofold. First, better covariate balance by using a GBM and IPW and should result in a more robust estimate. Of course better covariate balance alone does not guarantee robust results but it is a step in the right direction. Second, weighting on the inverse of the propensity scores instead of matching may significantly effect the estimate of the treatment effect especially when matching results in dropped observations.\nAn additional answer is the impact of reverse causality. A general problem is causal inference is that the direction of causality is not always known. While it is most intuitive that coffee certification would impact income, it is also possible that per capita income might determine their certification. Suppose that proponents of fair trade and certification are correct that certification will increase income and benefit livelihood. If farmers are aware of this, then perhaps the lowest income farmers are most likely to attempt to become certified to increase their income. Additionally, income likely has a reverse causal relationship with many of the explanatory variables. For example, a higher income may lead to better access to credit and the accumulation of land.\nIn summary, the analysis demonstrates that using more advanced methods like GBM and IPW not only improves covariate balance but also leads to significantly larger and more negative estimates of the treatment effect compared to the original study. This suggests that previous estimates may have underestimated the negative impact of certification on per capita income. The findings highlight the importance of methodological rigor in estimating causal effects and raise critical questions about the broader implications of certification policies, particularly when considering potential reverse causality and the varying structures of cooperatives. This analysis underscores the need for careful interpretation of treatment effects, especially in policy-relevant research.\n\n\n\n\n\n\nAbadie, Alberto, and Guido W. Imbens. 2006. “Large sample properties of matching estimators for average treatment effects.” Econometrica 74 (1): 235–67. https://doi.org/10.1111/j.1468-0262.2006.00655.x.\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.\n\n\nAbadie, Alberto, and Guido W. Imbens. 2006. “Large sample properties of matching estimators for average treatment effects.” Econometrica 74 (1): 235–67. https://doi.org/10.1111/j.1468-0262.2006.00655.x.\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Replication Case Study</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "5  Conclusion and Summary",
    "section": "",
    "text": "In conclusion, propensity score methods are useful causal inference tools when working with observational data. While logistic regression is commonly used, machine learning approaches, can improve the calibration of propensity scores and lead to better covariate balance. Thus, a better estimate of the treatment effect can be obtained. Particularly, gradient boosting machines perform well with strong theoretical properties.\nThe impact of fair trade certification for coffee producers in developing countries is an interesting problem in causal inference. Replicating Jena et al. (2012) with machine learning propensity scores results in a \\(10\\) fold increase in the estimate treatment effect of certification on per capita income which is a notable finding. The significant change in the covariate balance under a machine learning propensity score is a testament to the value of machine learning for propensity scores in this observational situation.\nMoving forward, a critical area of research at the intersection of machine learning and causal inference is the exploration of treatment effect heterogeneity, which refers to the variation in treatment effects across different individuals or subgroups. This concept is particularly relevant in fields like targeted medicine and policy, where understanding how different groups respond to a treatment or policy can lead to more effective and equitable outcomes.\nExisting methods to estimate heterogeneous treatment effects include causal trees and causal forests, which are designed to detect and quantify this heterogeneity. Additionally, metalearners, such as T-learners and S-learners, are powerful frameworks that adapt machine learning algorithms to estimate treatment effects for different subgroups. A particularly interesting area for future research is making these machine learning algorithms more interpretable, so they can be readily applied in real-world decision-making. Causal rule ensembles, for example, combine the predictive power of machine learning with the interpretability of rule-based models, making it easier to understand the underlying reasons for treatment effect heterogeneity.\n\n\n\n\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.\n\n\nJena, Pradyot Ranjan, Bezawit Beyene Chichaibelu, Till Stellmacher, and Ulrike Grote. 2012. “The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia.” Agricultural Economics (United Kingdom) 43 (4): 429–40. https://doi.org/10.1111/j.1574-0862.2012.00594.x.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion and Summary</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "R Version Control\nPlease note that future updates to these packages may impact replication of results.\nPackage\nVersion\nCitation\n\n\n\n\nbase\n4.4.1\nR Core Team (2024)\n\n\ncausaldata\n0.1.3\nHuntington-Klein and Barrett (2021)\n\n\ncobalt\n4.5.5\nGreifer (2024a)\n\n\ndata.table\n1.15.4\nBarrett et al. (2024)\n\n\ngbm\n2.2.2\nRidgeway and Developers (2024)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nknitr\n1.48.1\nXie (2014); Xie (2015); Xie (2024)\n\n\nmarginaleffects\n0.21.0\nArel-Bundock, Greifer, and Heiss (Forthcoming)\n\n\nMatchIt\n4.5.5\nHo et al. (2011)\n\n\nMatchItSE\n1.0\n(MatchItSE?)\n\n\npatchwork\n1.2.0\nPedersen (2024)\n\n\nrandomForest\n4.7.1.1\nLiaw and Wiener (2002)\n\n\nrenv\n1.0.7\nUshey and Wickham (2024)\n\n\nrmarkdown\n2.27\nXie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\nWeightIt\n1.2.0\nGreifer (2024b)",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapters/appendix.html",
    "href": "chapters/appendix.html",
    "title": "Appendix A — Datasets",
    "section": "",
    "text": "A.1 National Supported Work Data\nThe National Supported Work (NSW) Demonstration Job Training Program dataset originates from a large-scale social experiment conducted in the 1970s in the United States aimed at evaluating the impact of job training on employment and earnings among disadvantaged groups, including ex-addicts, ex-offenders, youth dropouts, and long-term unemployed women. The data contains a wide range of covariates including as age, education, pre-treatment earnings, marital status, and race.\nThe study is a randomized controlled trial (RCT) design which is rare for jobs and employment data. Participants were randomly assigned to either a treatment group, which received job training, or a control group, which did not. This randomization is notable as it as it simplifies the calculation of a treatment effect.\nInitially LaLonde (1986) used the NSW dataset to compare experimental and non-experimental estimators of the treatment effect. His findings highlighted significant discrepancies between the two, underscoring the importance of randomization in estimating causal effects. This study has been widely cited and forms the basis for many discussions on the validity of non-experimental methods. Following this, Dehejia and Wahba (1999), who revisited LaLonde’s analysis and compared many different contemporary methods with varying results.\nFor these reason it is commonly used in the literature as a toy dataset. It serves as a practical example for students learning about causal inference, allowing them to understand and apply different econometric methods.\nlibrary('causaldata')\ndata(\"nsw_mixtape\", package = \"causaldata\")\nnsw_data &lt;- as.data.frame(nsw_mixtape)\n\nnsw_data$data_id &lt;- seq(1,length(nsw_data$data_id))\n\nnsw_data$degree &lt;- abs(nsw_data$nodegree-1)\n\nnsw_data$nodegree &lt;- NULL",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "chapters/appendix.html#coffee-data-from-jena2012",
    "href": "chapters/appendix.html#coffee-data-from-jena2012",
    "title": "Appendix A — Datasets",
    "section": "A.2 Coffee Data from Jena et al. (2012)",
    "text": "A.2 Coffee Data from Jena et al. (2012)\nThe data used in the study by Jena et al. (2012) focuses on smallholder coffee farmers in Ethiopia. It includes a comprehensive survey of coffee-producing households, capturing various socioeconomic and agricultural variables. Key data points include household income, coffee production levels, prices received for coffee (both certified and non-certified), costs associated with certification, and access to markets. Additionally, the dataset encompasses demographic information such as household size, education levels, and access to resources like credit and extension services. This rich dataset allows for a detailed analysis of the impact of coffee certification on the livelihoods of these farmers, providing insights into both the benefits and challenges associated with certification programs.\nThe data is best accessed from Lampach and Morawetz (2016) where the data is available in the supplimentary information: https://www.tandfonline.com/doi/full/10.1080/00036846.2016.1153795. This data is also included on this project’s github under datasets/.\n\nlibrary(haven)\ncoffee_data &lt;-  read_dta(\"datasets/Jena_etAl_LampachMorawetz.dta\")\n\ncoffee_data &lt;- zap_formats(coffee_data)\n\ncoffee_data &lt;- coffee_data[-c(56,84,156 ),]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Datasets</span>"
    ]
  }
]