<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Propensity Scores and Machine Learning – Machine Learning and the Propensity Score</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/implimentation_workflow.html" rel="next">
<link href="../chapters/intro_background.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="[[2]{.chapter-number}&nbsp; [Propensity Scores and Machine Learning]{.chapter-title}]{#sec-propensity .quarto-section-identifier}">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=An introduction to propensity score methods for reducing the effects of confounding in observational studies;,citation_abstract=The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propen-sity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.  Taylor &amp;amp;amp; Francis Group, LLC.;,citation_author=Peter Austin;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=3;,citation_doi=10.1080/00273171.2011.568786;,citation_issn=00273171;,citation_pmid=21818162;,citation_volume=46;,citation_journal_title=Multivariate Behavioral Research;">
<meta name="citation_reference" content="citation_title=The Changing Risk and Burden of Wildfire in the Us;,citation_abstract=Recent dramatic and deadly increases in global wildfire activity have increased attention on the causes of wildfires, their consequences, and how risk from fire might be mitigated. Here we bring together data on the changing risk and societal burden of wildfire in the US. We estimate that nearly 50 million homes are currently in the wildland-urban interface in the US, a number increasing by 1 million houses every 3 years. Using a statistical model that links satellite-based fire and smoke data to pollution monitoring stations, we estimate that wildfires have accounted for up to 25% of PM2.5 in recent years across the US, and up to half in some Western regions. We then show that ambient exposure to smoke-based PM2.5 does not follow traditional socioeconomic exposure gradients. Finally, using stylized scenarios, we show that fuels management interventions have large but uncertain impacts on health outcomes, and that future health impacts from climate-change-induced wildfire smoke could approach projected overall increases in temperature-related mortality from climate change. We draw lessons for research and policy.;,citation_author=Marshall Burke;,citation_author=Anne Driscoll;,citation_author=Jenny Xue;,citation_author=Sam Heft-Neal;,citation_author=Jennifer Burney;,citation_author=Michael W. Wara;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2139/ssrn.3637724;,citation_journal_title=SSRN Electronic Journal;">
<meta name="citation_reference" content="citation_title=A tutorial on propensity score estimation for multiple treatments using generalized boosted models;,citation_abstract=The use of propensity scores to control for pretreatment imbalances on observed variables in non-randomized or observational studies examining the causal effects of treatments or interventions has become widespread over the past decade. For settings with two conditions of interest such as a treatment and a control, inverse probability of treatment weighted estimation with propensity scores estimated via boosted models has been shown in simulation studies to yield causal effect estimates with desirable properties. There are tools (e.g., the twang package in R) and guidance for implementing this method with two treatments. However, there is not such guidance for analyses of three or more treatments. The goals of this paper are twofold: (1) to provide step-by-step guidance for researchers who want to implement propensity score weighting for multiple treatments and (2) to propose the use of generalized boosted models (GBM) for estimation of the necessary propensity score weights. We define the causal quantities that may be of interest to studies of multiple treatments and derive weighted estimators of those quantities. We present a detailed plan for using GBM to estimate propensity scores and using those scores to estimate weights and causal effects. We also provide tools for assessing balance and overlap of pretreatment variables among treatment groups in the context of multiple treatments. A case study examining the effects of three treatment programs for adolescent substance abuse demonstrates the methods.  2013 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Daniel F. Mccaffrey;,citation_author=Beth Ann Griffin;,citation_author=Daniel Almirall;,citation_author=Mary Ellen Slaughter;,citation_author=Rajeev Ramchand;,citation_author=Lane F. Burgette;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=19;,citation_doi=10.1002/sim.5753;,citation_issn=02776715;,citation_pmid=23508673;,citation_volume=32;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=A blueprint for synthetic control methodology: A causal inference tool for evaluating natural experiments in population health;,citation_author=Ben Barr;,citation_author=Xingna Zhang;,citation_author=Mark Green;,citation_author=Iain Buchan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_doi=10.1136/bmj.o2712;,citation_isbn=2021054101;,citation_issn=17561833;,citation_pmid=36418028;,citation_volume=379;,citation_journal_title=Bmj;">
<meta name="citation_reference" content="citation_title=Wildfire Smoke and COVID-19;,citation_author=Natural Disasters and Severe Weather;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://www.cdc.gov/disasters/covid-19/wildfire{\_}smoke{\_}covid-19.html;">
<meta name="citation_reference" content="citation_title=Making Sense of Random Forest Probabilities: a Kernel Perspective;,citation_abstract=A random forest is a popular tool for estimating probabilities in machine learning classification tasks. However, the means by which this is accomplished is unprincipled: one simply counts the fraction of trees in a forest that vote for a certain class. In this paper, we forge a connection between random forests and kernel regression. This places random forest probability estimation on more sound statistical footing. As part of our investigation, we develop a model for the proximity kernel and relate it to the geometry and sparsity of the estimation problem. We also provide intuition and recommendations for tuning a random forest to improve its probability estimates.;,citation_author=Matthew A. Olson;,citation_author=Abraham J. Wyner;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://arxiv.org/abs/1812.05792;">
<meta name="citation_reference" content="citation_title=Applied Causal Inference Powered by ML and AI;,citation_abstract=An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.;,citation_author=Victor Chernozhukov;,citation_author=Christian Hansen;,citation_author=Nathan Kallus;,citation_author=Martin Spindler;,citation_author=Syrgkanis Vasilis;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;">
<meta name="citation_reference" content="citation_title=A local generalized method of moments estimator;,citation_abstract=A local Generalized Method of Moments Estimator is proposed for nonparametrically estimating unknown functions that are defined by conditional moment restrictions.  2006 Elsevier B.V. All rights reserved.;,citation_author=Arthur Lewbel;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=1;,citation_doi=10.1016/j.econlet.2006.08.011;,citation_issn=01651765;,citation_volume=94;,citation_journal_title=Economics Letters;">
<meta name="citation_reference" content="citation_title=7 Instrumental Variables Causal Inference :;,citation_author=The Mixtape;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Causal Forest Estimation of Heterogeneous Household Response to Time-Of-Use Electricity Pricing Schemes;,citation_author=Eoghan O Neill;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/arXiv:1810.09179v3;">
<meta name="citation_reference" content="citation_title=Large sample properties of matching estimators for average treatment effects;,citation_abstract=Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N 1/2-consistent in general and describe conditions under which matching estimators do attain N 1/2-consistency. Second, we show that even in settings where matching estimators are N 1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.  The Econometric Society 2006.;,citation_author=Alberto Abadie;,citation_author=Guido W. Imbens;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=1;,citation_doi=10.1111/j.1468-0262.2006.00655.x;,citation_issn=00129682;,citation_volume=74;,citation_journal_title=Econometrica;">
<meta name="citation_reference" content="citation_title=Cross-validated:What is the difference between the G-formula , G- estmation , G-computation and G-methods;,citation_author=James Robins;">
<meta name="citation_reference" content="citation_title=The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia;,citation_abstract=What is the impact of product certification on small-scale farmers’ livelihoods? To what extent does the participation of Ethiopian small-scale coffee farmers in certified local cooperative structures improve their socioeconomic situation? To answer these questions, this article employs household data of 249 coffee farmers from six different cooperatives collected in the Jimma zone of Southwestern Ethiopia in 2009. Findings show that the certification of coffee cooperatives has in total a low impact on small-scale coffee producers’ livelihoods mainly due to (1) low productivity, (2) insignificant price premium, and (3) poor access to credit and information from the cooperative. Differences in production and organizational capacities between the local cooperatives are mirrored in the extent of the certification benefits for the smallholders. &amp;amp;amp;quot;Good&amp;quot; cooperatives have reaped the benefits of certification, whereas &quot;bad&quot; ones did not fare well. In this regard the &quot;cooperative effect&quot; overlies the &quot;certification effect&quot;.  2012 International Association of Agricultural Economists.;,citation_author=Pradyot Ranjan Jena;,citation_author=Bezawit Beyene Chichaibelu;,citation_author=Till Stellmacher;,citation_author=Ulrike Grote;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=4;,citation_doi=10.1111/j.1574-0862.2012.00594.x;,citation_issn=01695150;,citation_volume=43;,citation_journal_title=Agricultural Economics (United Kingdom);">
<meta name="citation_reference" content="citation_title=Variable selection for propensity score models;,citation_abstract=Despite the growing popularity of propensity score (PS) methods in epidemiology, relatively little has been written in the epidemiologic literature about the problem of variable selection for PS models. The authors present the results of two simulation studies designed to help epidemiologists gain insight into the variable selection problem in a PS analysis. The simulation studies illustrate how the choice of variables that are included in a PS model can affect the bias, variance, and mean squared error of an estimated exposure effect. The results suggest that variables that are unrelated to the exposure but related to the outcome should always be included in a PS model. The inclusion of these variables will decrease the variance of an estimated exposure effect without increasing bias. In contrast, including variables that are related to the exposure but not to the outcome will increase the variance of the estimated exposure effect without decreasing bias. In very small studies, the inclusion of variables that are strongly related to the exposure but only weakly related to the outcome can be detrimental to an estimate in a mean squared error sense. The addition of these variables removes only a small amount of bias but can increase the variance of the estimated exposure effect. These simulation studies and other analytical results suggest that standard model-building tools designed to create good predictive models of the exposure will not always lead to optimal PS models, particularly in small studies. Copyright  2006 by the Johns Hopkins Bloomberg School of Public Health All rights reserved.;,citation_author=M. Alan Brookhart;,citation_author=Sebastian Schneeweiss;,citation_author=Kenneth J. Rothman;,citation_author=Robert J. Glynn;,citation_author=Jerry Avorn;,citation_author=Til Stürmer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=12;,citation_doi=10.1093/aje/kwj149;,citation_issn=00029262;,citation_pmid=16624967;,citation_volume=163;,citation_journal_title=American Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=STATS 361: Causal Inference;,citation_author=Stefan Wager;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Residuals and Influence in Regression;,citation_author=Sanford Weisberg;,citation_author=R D Cook;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;">
<meta name="citation_reference" content="citation_title=[ On the Application of Probability Theory to Agricultural Experiments . Essay on Principles . Section 9 .] Comment : Neyman ( 1923 ) and Causal Inference in Experiments and Observational Studies Author ( s ): Donald B . Rubin Source : Statistical Science;,citation_author=Donald B. Rubin;,citation_publication_date=1923;,citation_cover_date=1923;,citation_year=1923;">
<meta name="citation_reference" content="citation_title=Springer Texts in Statistics An Introduction to Statistical Learning wth application in R;,citation_abstract=An Introduction to Statistical Learning provides an accessible overview of the fi eld of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fi elds ranging from biology to fi nance to marketing to astrophysics in the past twenty years. Th is book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classifi cation, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in sci-ence, industry, and other fi elds, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical soft ware platform.;,citation_author=Gareth James;,citation_author=undefined DanielaWitten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_isbn=9781461471370;,citation_issn=01621459;,citation_pmid=10911016;">
<meta name="citation_reference" content="citation_title=Different worlds Confirmatory versus exploratory research;,citation_abstract=Simon Schwab and Leonhard Held explain the differences between confirmatory and exploratory research and the dangers of confusing the two concepts.;,citation_author=Simon Schwab;,citation_author=Leonhard Held;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_doi=10.1111/1740-9713.01369;,citation_issn=17409713;,citation_volume=17;,citation_journal_title=Significance;">
<meta name="citation_reference" content="citation_title=Does matching overcome LaLonde’s critique of nonexperimental estimators?;,citation_abstract=This paper applies cross-sectional and longitudinal propensity score matching estimators to data from the National Supported Work (NSW) Demonstration that have been previously analyzed by LaLonde (1986) and Dehejia and Wahba (1999, 2002). We find that estimates of the impact of NSW based on propensity score matching are highly sensitive to both the set of variables included in the scores and the particular analysis sample used in the estimation. Among the estimators we study, the difference-in-differences matching estimator performs the best. We attribute its performance to the fact that it eliminates potential sources of temporally invariant bias present in the NSW data, such as geographic mismatch between participants and nonparticipants and the use of a dependent variable measured in different ways for the two groups. Our analysis demonstrates that while propensity score matching is a potentially useful econometric tool, it does not represent a general solution to the evaluation problem.  2004 Elsevier B.V. All rights reserved.;,citation_author=Jeffrey A. Smith;,citation_author=Petra E. Todd;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_doi=10.1016/j.jeconom.2004.04.011;,citation_isbn=1301405353;,citation_issn=03044076;,citation_volume=125;">
<meta name="citation_reference" content="citation_title=American Economic Association Evaluating the Econometric Evaluations of Training Programs with Experimental Data Author ( s ): Robert J . LaLonde Source : The American Economic Review , Vol . 76 , No . 4 ( Sep ., 1986 ), pp . 604-620 Published by : Americ;,citation_abstract=This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric proce- dures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.;,citation_author=Robert J. LaLonde;,citation_publication_date=1986;,citation_cover_date=1986;,citation_year=1986;,citation_issue=4;,citation_volume=76;,citation_journal_title=The American Economic Review;">
<meta name="citation_reference" content="citation_title=Causal Inference in the Presence of Interference in Sponsored Search Advertising;,citation_abstract=In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the likelihood of a user clicking on a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.;,citation_author=Razieh Nabi;,citation_author=Joel Pfeiffer;,citation_author=Denis Charles;,citation_author=Emre Kıcıman;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2010.07458;,citation_doi=10.3389/fdata.2022.888592;,citation_issn=2624909X;,citation_volume=5;,citation_journal_title=Frontiers in Big Data;">
<meta name="citation_reference" content="citation_title=Excess of COVID-19 cases and deaths due to fine particulate matter exposure during the 2020 wildfires in the United States;,citation_abstract=The year 2020 brought unimaginable challenges in public health, with the confluence of the COVID-19 pandemic and wildfires across the western United States. Wildfires produce high levels of fine particulate matter (PM2.5). Recent studies reported that short-term exposure to PM2.5 is associated with increased risk of COVID-19 cases and deaths. We acquired and linked publicly available daily data on PM2.5, the number of COVID-19 cases and deaths, and other confounders for 92 western U.S. counties that were affected by the 2020 wildfires. We estimated the association between short-term exposure to PM2.5 during the wildfires and the epidemiological dynamics of COVID-19 cases and deaths. We adjusted for several time-varying confounding factors (e.g., weather, seasonality, long-term trends, mobility, and population size). We found strong evidence that wildfires amplified the effect of short-term exposure to PM2.5 on COVID-19 cases and deaths, although with substantial heterogeneity across counties.;,citation_author=Xiaodan Zhou;,citation_author=Kevin Josey;,citation_author=Leila Kamareddine;,citation_author=Miah C. Caine;,citation_author=Tianjia Liu;,citation_author=Loretta J. Mickley;,citation_author=Matthew Cooper;,citation_author=Francesca Dominici;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=33;,citation_doi=10.1126/sciadv.abi8789;,citation_issn=23752548;,citation_pmid=34389545;,citation_volume=7;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=Biased Random Forest for Dealing with the Class Imbalance Problem;,citation_abstract=The class imbalance issue has been a persistent problem in machine learning that hinders the accurate predictive analysis of data in many real-world applications. The class imbalance problem exists when the number of instances present in a class (or classes) is significantly fewer than the number of instances belonging to another class (or classes). Sufficiently recognizing the minority class during classification is a problem as most algorithms employed to learn from data input are biased toward the majority class. The underlying issue is made more complex with the presence of data difficult factors embedded in such data input. This paper presents a novel and effective ensemble-based method for dealing with the class imbalance problem. This paper is motivated by the idea of moving the oversampling from the data level to the algorithm level, instead of increasing the minority instances in the data sets, the algorithms in this paper aims to ’oversample the classification ensemble’ by increasing the number of classifiers that represent the minority class in the ensemble, i.e., random forest. The proposed biased random forest algorithm employs the nearest neighbor algorithm to identify the critical areas in a given data set. The standard random forest is then fed with more random trees generated based on the critical areas. The results show that the proposed algorithm is very effective in dealing with the class imbalance problem.;,citation_author=Mohammed Bader-El-Den;,citation_author=Eleman Teitei;,citation_author=Todd Perry;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=7;,citation_doi=10.1109/TNNLS.2018.2878400;,citation_issn=21622388;,citation_pmid=30475733;,citation_volume=30;,citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=Causal inference;,citation_abstract=We have examined only a few of the basic questions about causal inference that result from Reichenbach’s two principles. We have not considered what happens when the probability distribution is a mixture of distributions from different causal structures, or how unmeasured common causes can be detected, or what inferences can reliably be drawn about causal relations among unmeasured variables, or the exact advantages that experimental control offers. A good deal is known about these questions, and there is a good deal more to find out.  1991 Kluwer Academic Publishers.;,citation_author=undefined Mixtape;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_doi=10.1007/BF00388284;,citation_issn=01650106;,citation_volume=35;">
<meta name="citation_reference" content="citation_title=Improving propensity score weighting using machine learning;,citation_abstract=Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART-based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non-linear and non-additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non-linearity or non-additivity alone. However, under conditions of both moderate non-additivity and moderate non-linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright  2009 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Brian K. Lee;,citation_author=Justin Lessler;,citation_author=Elizabeth A. Stuart;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_doi=10.1002/sim.3782;,citation_issn=02776715;,citation_pmid=19960510;,citation_volume=29;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=Causal Inference: What If (1st ed.);,citation_author=A. Miguel Hernan;,citation_author=M. James Robins;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.1201/9781315374932;,citation_volume=[1] A. M.;">
<meta name="citation_reference" content="citation_title=Measuring the effectiveness of protected area networks in reducing deforestation;,citation_abstract=Global efforts to reduce tropical deforestation rely heavily on the establishment of protected areas. Measuring the effectiveness of these areas is difficult because the amount of deforestation that would have occurred in the absence of legal protection cannot be directly observed. Conventional methods of evaluating the effectiveness of protected areas can be biased because protection is not randomly assigned and because protection can induce deforestation spillovers (displacement) to neighboring forests. We demonstrate that estimates of effectiveness can be substantially improved by controlling for biases along dimensions that are observable, measuring spatial spillovers, and testing the sensitivity of estimates to potential hidden biases. We apply matching methods to evaluate the impact on deforestation of Costa Rica’s renowned protected-area system between 1960 and 1997. We find that protection reduced deforestation: approximately 10% of the protected forests would have been deforested had they not been protected. Conventional approaches to evaluating conservation impact, which fail to control for observable covariates correlated with both protection and deforestation, substantially overestimate avoided deforestation (by over 65%, based on our estimates). We also find that deforestation spillovers from protected to unprotected forests are negligible. Our conclusions are robust to potential hidden bias, as well as to changes in modeling assumptions. Our results show that, with appropriate empirical methods, conservation scientists and policy makers can better understand the relationships between human and natural systems and can use this to guide their attempts to protect critical ecosystem services.  2008 by The National Academy of Sciences of the USA.;,citation_author=Kwaw S. Andam;,citation_author=Paul J. Ferraro;,citation_author=Alexander Pfaff;,citation_author=G. Arturo Sanchez-Azofeifa;,citation_author=Juan A. Robalino;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=42;,citation_doi=10.1073/pnas.0800437105;,citation_issn=00278424;,citation_pmid=18854414;,citation_volume=105;,citation_journal_title=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=Classification and Regression Trees;,citation_author=L Breiman;,citation_author=Jerome H Friedman;,citation_author=Richard A Olshen;,citation_author=C J Stone;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_fulltext_html_url=https://api.semanticscholar.org/CorpusID:29458883;,citation_volume=40;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=Climate drivers of global wildfire burned area;,citation_abstract=Wildfire is an integral part of the Earth system, but at the same time it can pose serious threats to human society and to certain types of terrestrial ecosystems. Meteorological conditions are a key driver of wildfire activity and extent, which led to the emergence of the use of fire danger indices that depend solely on weather conditions. The Canadian Fire Weather Index (FWI) is a widely used fire danger index of this kind. Here, we evaluate how well the FWI, its components, and the climate variables from which it is derived, correlate with observation-based burned area (BA) for a variety of world regions. We use a novel technique, according to which monthly BA are grouped by size for each Global Fire Emissions Database (GFED) pyrographic region. We find strong correlations of BA anomalies with the FWI anomalies, as well as with the underlying deviations from their climatologies for the four climate variables from which FWI is estimated, namely, temperature, relative humidity, precipitation, and wind. We quantify the relative sensitivity of the observed BA to each of the four climate variables, finding that this relationship strongly depends on the pyrographic region and land type. Our results indicate that the BA anomalies strongly correlate with FWI anomalies at a GFED region scale, compared to the strength of the correlation with individual climate variables. Additionally, among the individual climate variables that comprise the FWI, relative humidity and temperature are the most influential factors that affect the observed BA. Our results support the use of the composite fire danger index FWI, as well as its sub-indices, the Build-Up Index (BUI) and the Initial Spread Index (ISI), comparing to single climate variables, since they are found to correlate better with the observed forest or non-forest BA, for the most regions across the globe.;,citation_author=Manolis Grillakis;,citation_author=Apostolos Voulgarakis;,citation_author=Anastasios Rovithakis;,citation_author=Konstantinos D. Seiradakis;,citation_author=Aristeidis Koutroulis;,citation_author=Robert D. Field;,citation_author=Matthew Kasoar;,citation_author=Athanasios Papadopoulos;,citation_author=Mihalis Lazaridis;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_doi=10.1088/1748-9326/ac5fa1;,citation_issn=17489326;,citation_volume=17;,citation_journal_title=Environmental Research Letters;">
<meta name="citation_reference" content="citation_title=Evaluating uses of data mining techniques in propensity score estimation: a simulation study;,citation_author=Soko Setoguchi;,citation_author=Sebastian Schneeweiss;,citation_author=Alan M. Brookhart;,citation_author=Robert J. Glynn;,citation_author=Francis E. Cook;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=March;,citation_doi=10.1002/pds;,citation_issn=1053-8569;,citation_volume=17;,citation_journal_title=pharmacoepidemiology and drug safety;">
<meta name="citation_reference" content="citation_title=Credibility of propensity score matching estimates. An example from Fair Trade certification of coffee producers;,citation_abstract=Propensity score matching (PSM) is an increasingly popular method for evaluation studies in agricultural and development economics. However, statisticians and econometricians have stressed that results rely on untestable assumptions, and therefore, guidelines for researchers on how to improve credibility have been developed. We follow one of these guidelines with a data set analysed by other authors to evaluate the impact of Fair Trade certification on the income of coffee producers. We provide thereby a best practice example of how to evaluate the credibility of PSM estimates. We find that a thorough assessment of the assumptions made renders the data we use not suitable for a credible PSM estimation of the effects of treatment. We conclude that the debate about the impact of Fair Trade certification would greatly benefit from a detailed reporting of credibility checking.;,citation_author=Nicolas Lampach;,citation_author=Ulrich B. Morawetz;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=44;,citation_doi=10.1080/00036846.2016.1153795;,citation_issn=14664283;,citation_volume=48;,citation_journal_title=Applied Economics;">
<meta name="citation_reference" content="citation_title=Some practical guidance for the implementation of propensity score matching;,citation_abstract=Propensity score matching (PSM) has become a popular approach to estimate causal treatment effects. It is widely applied when evaluating labour market policies, but empirical examples can be found in very diverse fields of study. Once the researcher has decided to use PSM, he is confronted with a lot of questions regarding its implementation. To begin with, a first decision has to be made concerning the estimation of the propensity score. Following that one has to decide which matching algorithm to choose and determine the region of common support. Subsequently, the matching quality has to be assessed and treatment effects and their standard errors have to be estimated. Furthermore, questions like ’what to do if there is choice-based sampling?’ or ’when to measure effects?’ can be important in empirical studies. Finally, one might also want to test the sensitivity of estimated treatment effects with respect to unobserved heterogeneity or failure of the common support condition. Each implementation step involves a lot of decisions and different approaches can be thought of. The aim of this paper is to discuss these implementation issues and give some guidance to researchers who want to use PSM for evaluation purposes.  2008 The Authors Journal compilation  2008 Blackwell Publishing Ltd.;,citation_author=Marco Caliendo;,citation_author=Sabine Kopeinig;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=1;,citation_doi=10.1111/j.1467-6419.2007.00527.x;,citation_issn=09500804;,citation_volume=22;,citation_journal_title=Journal of Economic Surveys;">
<meta name="citation_reference" content="citation_title=Understanding Inverse Probability of Treatment Weighting (IPTW) in Causal Inference An Intuitive Explanation of IPTW and a Comparison to Multivariate Regression;,citation_author=Jonah Breslow;,citation_author=undefined Follow;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;">
<meta name="citation_reference" content="citation_title=Metalearners for estimating heterogeneous treatment effects using machine learning;,citation_abstract=There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.;,citation_author=Sören R. Künzel;,citation_author=Jasjeet S. Sekhon;,citation_author=Peter J. Bickel;,citation_author=Bin Yu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03461;,citation_issue=10;,citation_doi=10.1073/pnas.1804597116;,citation_issn=10916490;,citation_pmid=30770453;,citation_volume=116;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Propensity score model overfitting led to inflated variance of estimated odds ratios;,citation_abstract=Objective Simulation studies suggest that the ratio of the number of events to the number of estimated parameters in a logistic regression model should be not less than 10 or 20 to 1 to achieve reliable effect estimates. Applications of propensity score approaches for confounding control in practice, however, do often not consider these recommendations. Study Design and Setting We conducted extensive Monte Carlo and plasmode simulation studies to investigate the impact of propensity score model overfitting on the performance in estimating conditional and marginal odds ratios using different established propensity score inference approaches. We assessed estimate accuracy and precision as well as associated type I error and type II error rates in testing the null hypothesis of no exposure effect. Results For all inference approaches considered, our simulation study revealed considerably inflated standard errors of effect estimates when using overfitted propensity score models. Overfitting did not considerably affect type I error rates for most inference approaches. However, because of residual confounding, estimation performance and type I error probabilities were unsatisfactory when using propensity score quintile adjustment. Conclusion Overfitting of propensity score models should be avoided to obtain reliable estimates of treatment or exposure effects in individual studies.;,citation_author=Tibor Schuster;,citation_author=Wilfrid Kouokam Lowe;,citation_author=Robert W. Platt;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.jclinepi.2016.05.017;,citation_doi=10.1016/j.jclinepi.2016.05.017;,citation_issn=18785921;,citation_pmid=27498378;,citation_volume=80;,citation_journal_title=Journal of Clinical Epidemiology;,citation_publisher=Elsevier Inc;">
<meta name="citation_reference" content="citation_title=What Makes Forest-Based Heterogeneous Treatment Effect Estimators Work?;,citation_abstract=Estimation of heterogeneous treatment effects (HTE) is of prime importance in many disciplines, from personalized medicine to economics among many others. Random forests have been shown to be a flexible and powerful approach to HTE estimation in both randomized trials and observational studies. In particular “causal forests” introduced by Athey, Tibshirani and Wager (Ann. Statist. 47 (2019) 1148–1178), along with the R implementation in package grf were rapidly adopted. A related approach, called “model-based forests” that is geared toward randomized trials and simultaneously captures effects of both prognostic and predictive variables, was introduced by Seibold, Zeileis and Hothorn (Stat. Methods Med. Res. 27 (2018) 3104–3125) along with a modular implementation in the R package model4you. Neither procedure is directly applicable to the estimation of individualized predictions of excess postpartum blood loss caused by a cesarean section in comparison to vaginal delivery. Clearly, randomization is hardly possible in this setup, and thus model-based forests lack clinical trial data to address this question. On the other hand, the skewed and interval-censored postpartum blood loss observations violate assumptions made by causal forests. Here we present a tailored model-based forest for skewed and interval-censored data to infer possible predictive prepartum characteristics and their impact on excess postpartum blood loss caused by a cesarean section. As a methodological basis, we propose a unifying view on causal and model-based forests that goes beyond the theoretical motivations and investigates which computational elements make causal forests so successful and how these can be blended with the strengths of model-based forests. To do so, we show that both methods can be understood in terms of the same parameters and model assumptions for an additive model under L2 loss. This theoretical insight allows us to implement several flavors of “model-based causal forests” and dissect their different elements in silico. The original causal forests and model-based forests are compared with the new blended versions in a benchmark study exploring both randomized trials and observational settings. In the randomized setting, both approaches performed akin. If confounding was present in the data-generating process, we found local centering of the treatment indicator with the corresponding propensities to be the main driver for good performance. Local centering of the outcome was less important and might be replaced or enhanced by simultaneous split selection with respect to both prognostic and predictive effects. This lays the foundation for future research combining random forests for HTE estimation with other types of models.;,citation_author=Y. Susanne Dandl;,citation_author=Christian Haslinger;,citation_author=Torsten Hothorn;,citation_author=Heidi Seibold;,citation_author=Erik Sverdrup;,citation_author=Stefan Wager;,citation_author=Achim Zeileis;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://arxiv.org/abs/2206.10323;,citation_issue=1;,citation_doi=10.1214/23-AOAS1799;,citation_issn=19417330;,citation_volume=18;,citation_journal_title=Annals of Applied Statistics;">
<meta name="citation_reference" content="citation_title=Root-N-Consistent Semiparametric Regression;,citation_author=P.m. Robinson;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;,citation_fulltext_html_url=https://www.jstor.org/stable/1912705;,citation_issue=4;,citation_volume=56;,citation_journal_title=The Econometric Socieity;">
<meta name="citation_reference" content="citation_title=Estimating Treatment Effects with Causal Forests: An Application;,citation_abstract=We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and discusses resulting practical and conceptual challenges. This note will appear in an upcoming issue of Observational Studies, Empirical Investigation of Methods for Heterogeneity, that compiles several analyses of the same dataset.;,citation_author=Susan Athey;,citation_author=Stefan Wager;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1902.07409;,citation_issue=2;,citation_doi=10.1353/obs.2019.0001;,citation_issn=27673324;,citation_volume=5;,citation_journal_title=Observational Studies;">
<meta name="citation_reference" content="citation_title=Machine Learning Methods for Estimating Heterogeneous Causal Effect;,citation_abstract=In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit’s attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit’s outcome. The challenge is that the “ground truth” for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine. Keywords:;,citation_author=Susan Athey;,citation_author=Guido Imbens;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1504.01132;">
<meta name="citation_reference" content="citation_title=Causal inference for climate change events from satellite image time series using computer vision and deep learning;,citation_abstract=We propose a method for causal inference using satellite image time series, in order to determine the treatment effects of interventions which impact climate change, such as deforestation. Simply put, the aim is to quantify the ’before versus after’ effect of climate related human driven interventions, such as urbanization; as well as natural disasters, such as hurricanes and forest fires. As a concrete example, we focus on quantifying forest tree cover change/ deforestation due to human led causes. The proposed method involves the following steps. First, we uae computer vision and machine learning/deep learning techniques to detect and quantify forest tree coverage levels over time, at every time epoch. We then look at this time series to identify changepoints. Next, we estimate the expected (forest tree cover) values using a Bayesian structural causal model and projecting/forecasting the counterfactual. This is compared to the values actually observed post intervention, and the difference in the two values gives us the effect of the intervention (as compared to the non intervention scenario, i.e. what would have possibly happened without the intervention). As a specific use case, we analyze deforestation levels before and after the hyperinflation event (intervention) in Brazil (which ended in 1993-94), for the Amazon rainforest region, around Rondonia, Brazil. For this deforestation use case, using our causal inference framework can help causally attribute change/reduction in forest tree cover and increasing deforestation rates due to human activities at various points in time.;,citation_author=Vikas Ramachandra;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1910.11492;">
<meta name="citation_reference" content="citation_title=A Primer for Applying Propensity-Score Matching;,citation_abstract=The use of microeconometric techniques to estimate the effects of development policies has become a common approach not only for scholars, but also for policy-makers engaged in designing, implementing and evaluating projects in different fields. Among these techniques, Propensity-Score Matching (PSM) is increasingly applied in the policy evaluation community. This technical note provides a guide to the key aspects of implementing PSM methodology for an audience of practitioners interested in understanding its applicability to specific evaluation problems. The note summarizes the basic conditions under which PSM can be used to estimate the impact of a program and the data required. It explains how the Conditional Independence Assumption, combined with the Overlap Condition, reduces selection bias when participation in a program is determined by observable characteristics. It also describes different matching algorithms and some tests to assess the quality of the matching. Case studies are used throughout to illustrate important concepts in impact evaluation and PSM. In the annexes, the note provides an outline of the main technical aspects and a list of statistical and econometric software for implementing PSM.;,citation_author=Carolyn Heinrich;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_fulltext_html_url=http://www.iadb.org/document.cfm?id=35320229;,citation_issue=August;,citation_issn={\textless}null{\textgreater};,citation_journal_title=Development;">
<meta name="citation_reference" content="citation_title=Quasi-oracle estimation of heterogeneous treatment effects;,citation_abstract=Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.;,citation_author=X. Nie;,citation_author=S. Wager;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/1712.04912;,citation_issue=2;,citation_doi=10.1093/biomet/asaa076;,citation_issn=14643510;,citation_volume=108;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Increasing trends in high-severity fire in the southwestern USA from 1984 to 2015;,citation_abstract=In the last three decades, over 4.1 million hectares have burned in Arizona and New Mexico and the largest fires in documented history have occurred in the past two decades. Changes in burn severity over time, however, have not been well documented in forest and woodland ecosystems in the southwestern US. Using remotely sensed burn severity data from 1621 fires (&amp;amp;amp;gt;404 ha), we assessed trends from 1984 to 2015 in Arizona and New Mexico in (1) number of fires and total area burned in all vegetation types; (2) area burned, area of high-severity, and percent of high-severity fire in all forest and woodland areas; and (3) area burned, area of high-severity, and percent of high-severity in seven different grouped forest and woodland vegetation types (Ecological Response Unit [ERU] Fire Regime Types). Number of fires and area burned increased across the Southwest regardless of vegetation type. The significant increasing trends held for area burned, area of high-severity, and percent of high-severity fire in all forest and woodland ecosystems. Area burned and area burned severely increased in all seven ERU Fire Regime Types while percent of high-severity fire increased in two ERUs: Mixed Conifer Frequent Fire and Mixed Conifer with Aspen/Spruce Fir. Managers must face the implications of increasing, uncharacteristic high-severity fire in many ecosystems as climate change and human pressures continue to affect fire regimes.;,citation_author=Megan P. Singleton;,citation_author=Andrea E. Thode;,citation_author=Andrew J. Sánchez Meador;,citation_author=Jose M. Iniguez;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1016/j.foreco.2018.11.039;,citation_issue=November 2018;,citation_doi=10.1016/j.foreco.2018.11.039;,citation_issn=03781127;,citation_volume=433;,citation_journal_title=Forest Ecology and Management;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Causal Rule Ensemble: Interpretable Discovery and Inference of Heterogeneous Treatment Effects;,citation_abstract=In health and social sciences, it is critically important to identify subgroups of the study population where a treatment has notable heterogeneity in the causal effects with respect to the average treatment effect. Data-driven discovery of heterogeneous treatment effects (HTE) via decision tree methods has been proposed for this task. Despite its high interpretability, the single-tree discovery of HTE tends to be highly unstable and to find an oversimplified representation of treatment heterogeneity. To accommodate these shortcomings, we propose Causal Rule Ensemble (CRE), a new method to discover heterogeneous subgroups through an ensemble-of-trees approach. CRE has the following features: 1) provides an interpretable representation of the HTE; 2) allows extensive exploration of complex heterogeneity patterns; and 3) guarantees high stability in the discovery. The discovered subgroups are defined in terms of interpretable decision rules, and we develop a general two-stage approach for subgroup-specific conditional causal effects estimation, providing theoretical guarantees. Via simulations, we show that the CRE method has a strong discovery ability and a competitive estimation performance when compared to state-of-the-art techniques. Finally, we apply CRE to discover subgroups most vulnerable to the effects of exposure to air pollution on mortality for 35.3 million Medicare beneficiaries across the contiguous U.S.;,citation_author=Falco J. Bargagli-Stoffi;,citation_author=Riccardo Cadei;,citation_author=Kwonsang Lee;,citation_author=Francesca Dominici;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=http://arxiv.org/abs/2009.09036;">
<meta name="citation_reference" content="citation_title=An introduction to g methods;,citation_abstract=Robins’ generalized methods (g methods) provide consistent estimates of contrasts (e.g. differences, ratios) of potential outcomes under a less restrictive set of identification conditions than do standard regression methods (e.g. linear, logistic, Cox regression). Uptake of g methods by epidemiologists has been hampered by limitations in understanding both conceptual and technical details. We present a simple worked example that illustrates basic concepts, while minimizing technical complications.;,citation_author=Ashley I. Naimi;,citation_author=Stephen R. Cole;,citation_author=Edward H. Kennedy;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_doi=10.1093/ije/dyw323;,citation_issn=14643685;,citation_pmid=28039382;,citation_volume=46;,citation_journal_title=International Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=Overfitting in propensity score model: a commentary on “propensity score model overfitting led to inflated variance of estimated odds ratios” by Schuster et&nbsp;al.;,citation_author=David Hajage;,citation_author=Florence Tubach;,citation_author=Yann De Rycke;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.jclinepi.2017.05.011;,citation_doi=10.1016/j.jclinepi.2017.05.011;,citation_issn=18785921;,citation_pmid=28549930;,citation_volume=88;,citation_journal_title=Journal of Clinical Epidemiology;,citation_publisher=Elsevier Inc.;">
<meta name="citation_reference" content="citation_title=Regression Shrinkage and Selection via the Lasso;,citation_abstract=We propose a new method for estimation in linear models. The ’lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.;,citation_author=Robert Tibshirani;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=fastly-default{\%}3Aff57285d6b8854126d21a135984fc4ca{\&amp;amp;amp;}ab{\_}segments={\&amp;}origin={\&amp;}initiator={\&amp;}acceptTC=1;,citation_issue=1;,citation_volume=58;,citation_journal_title=Journal of the Royal Statistical Society;">
<meta name="citation_reference" content="citation_title=Policy Learning With Observational Data;,citation_abstract=In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application‐specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.;,citation_author=Susan Athey;,citation_author=Stefan Wager;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/1702.02896;,citation_issue=1;,citation_doi=10.3982/ecta15732;,citation_issn=0012-9682;,citation_volume=89;,citation_journal_title=Econometrica;">
<meta name="citation_reference" content="citation_title=Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys;,citation_abstract=Modern survey methods may be subject to non-observable bias, from various sources. Among online surveys, for example, selection bias is prevalent, due to the sampling mechanism commonly used, whereby participants self-select from a subgroup whose characteristics differ from those of the target population. Several techniques have been proposed to tackle this issue. One such is Propensity Score Adjustment (PSA), which is widely used and has been analysed in various studies. The usual method of estimating the propensity score is logistic regression, which requires a reference probability sample in addition to the online nonprobability sample. The predicted propensities can be used for reweighting using various estimators. However, in the online survey context, there are alternatives that might outperform logistic regression regarding propensity estimation. The aim of the present study is to determine the efficiency of some of these alternatives, involving Machine Learning (ML) classification algorithms. PSA is applied in two simulation scenarios, representing situations commonly found in online surveys, using logistic regression and ML models for propensity estimation. The results obtained show that ML algorithms remove selection bias more effectively than logistic regression when used for PSA, but that their efficacy depends largely on the selection mechanism employed and the dimensionality of the data.;,citation_author=Ramón Ferri-García;,citation_author=María Del Mar Rueda;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=4;,citation_doi=10.1371/journal.pone.0231500;,citation_isbn=1111111111;,citation_issn=19326203;,citation_pmid=32320429;,citation_volume=15;,citation_journal_title=PLoS ONE;">
<meta name="citation_reference" content="citation_title=The changing risk and burden of wildfire in the United States;,citation_abstract=Recent dramatic and deadly increases in global wildfire activity have increased attention on the causes of wildfires, their consequences, and how risk from wildfire might be mitigated. Here we bring together data on the changing risk and societal burden of wildfire in the United States. We estimate that nearly 50 million homes are currently in the wildland-urban interface in the United States, a number increasing by 1 million houses every 3 y. To illustrate how changes in wildfire activity might affect air pollution and related health outcomes, and how these linkages might guide future science and policy, we develop a statistical model that relates satellite-based fire and smoke data to information from pollution monitoring stations. Using the model, we estimate that wildfires have accounted for up to 25% of PM2.5 (particulate matter with diameter &amp;amp;amp;lt;2.5 $\mu$m) in recent years across the United States, and up to half in some Western regions, with spatial patterns in ambient smoke exposure that do not follow traditional socioeconomic pollution exposure gradients. We combine the model with stylized scenarios to show that fuel management interventions could have large health benefits and that future health impacts from climate-change-induced wildfire smoke could approach projected overall increases in temperature-related mortality from climate change-but that both estimates remain uncertain. We use model results to highlight important areas for future research and to draw lessons for policy.;,citation_author=Marshall Burke;,citation_author=Anne Driscoll;,citation_author=Sam Heft-Neal;,citation_author=Jiani Xue;,citation_author=Jennifer Burney;,citation_author=Michael Wara;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1073/PNAS.2011048118;,citation_issn=10916490;,citation_pmid=33431571;,citation_volume=118;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Comparison of various machine learning algorithms for estimating generalized propensity score;,citation_abstract=In this paper, we conducted a simulation study to evaluate the performance of four algorithms: multinomial logistic regression (MLR), bagging (BAG), random forest (RF), and gradient boosting (GB), for estimating generalized propensity score (GPS). Similar to the propensity score (PS), the ultimate goal of using GPS is to estimate unbiased average treatment effects (ATEs) in observational studies. We used the GPS estimates computed from these four algorithms with the generalized doubly robust (GDR) estimator to estimate ATEs in observational studies. We evaluated these ATE estimates in terms of bias and mean squared error (MSE). Simulation results show that overall, the GB algorithm produced the best ATE estimates based on these evaluation criteria. Thus, we recommend using the GB algorithm for estimating GPS in practice.;,citation_author=Chunhao Tu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1080/00949655.2019.1571059;,citation_issue=4;,citation_doi=10.1080/00949655.2019.1571059;,citation_issn=15635163;,citation_volume=89;,citation_journal_title=Journal of Statistical Computation and Simulation;">
<meta name="citation_reference" content="citation_title=Springer Series in Statistics The Elements of Statistical Learning;,citation_abstract=During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.;,citation_author=Trevor Hastie;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf;,citation_issue=2;,citation_isbn=9780387848570;,citation_issn=03436993;,citation_pmid=15512507;,citation_volume=27;,citation_journal_title=The Mathematical Intelligencer;">
<meta name="citation_reference" content="citation_title=Causal Inference : An Introduction;,citation_abstract=Lately, the concept of causality has been gaining popularity in the domain of machine learning and artificial intelligence due to its…;,citation_author=Siddhant Haldar;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://medium.com/analytics-vidhya/causal-inference-an-introduction-f424df7c76ef{\%}0Ahttp://files/135/causal-inference-an-introduction-f424df7c76ef.html;,citation_journal_title=Medium;">
<meta name="citation_reference" content="citation_title=Member-only story Understanding Causal Trees How to use regression trees to estimate heterogeneous treatment effects;,citation_abstract=Listen Share More Cover, image by Author causal inference, we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, …) on an outcome of interest (a disease, firm revenue, customer satisfaction, …). However, knowing that a This member-only story is on us. Upgrade to access all of Medium.;,citation_author=Matteo Courthoud;,citation_author=undefined Follow;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;">
<meta name="citation_reference" content="citation_title=Directed Acyclic Graphs;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2307/j.ctv1c29t27.6;,citation_inbook_title=Causal inference: The mixtape;">
<meta name="citation_reference" content="citation_title=An Introduction to Recursive Partitioning for Heterogeneous Causal Effects Estimation Using causalTree package;,citation_author=Susan Athey;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=On the application of probability theory to agricultural experiments. Essay on principles. Section 9;,citation_abstract=In the portion of the paper translated here, Neyman introduces a model for the analysis of field experiments conducted for the purpose of comparing a number of crop varieties, which makes use of a double-indexed array of unknown potential yields, one index corresponding to varieties and the other to plots. The yield corresponding to only one variety will be observed on any given plot, but through an urn model embodying sampling without replacement from this doubly indexed array, Neyman obtains a formula for the variance of the difference between the averages of the observed yields of two varieties. This variance involves the variance over all plots of the potential yields and the correlation coefficient r between the potential yields of the two varieties on the same plot. Since it is impossible to estimate r directly, Neyman advises taking r = 1, observing that in practice this may lead to using too large an estimated standard deviation, when comparing two variety means.  1990, Institute of Mathematical Statistics. All Rights Reserved.;,citation_author=Jerzy Splawa-Neyman;,citation_publication_date=1923;,citation_cover_date=1923;,citation_year=1923;,citation_fulltext_html_url=https://www.mimuw.edu.pl/{~}noble/courses/BayesianNetworks/90NeymanTranslation.pdf;,citation_doi=10.1214/ss/1177012031;,citation_issn=08834237;">
<meta name="citation_reference" content="citation_title=Matching and Subclassification;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2307/j.ctv1c29t27.8;,citation_inbook_title=Causal inference: The mixtape;">
<meta name="citation_reference" content="citation_title=Generalized random forests;,citation_abstract=We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.;,citation_author=Susan Athey;,citation_author=Julie Tibshirani;,citation_author=Stefan Wager;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1610.01271;,citation_issue=2;,citation_doi=10.1214/18-AOS1709;,citation_issn=00905364;,citation_volume=47;,citation_journal_title=Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Policy Evaluation: New Tools for Causal Inference;,citation_abstract=While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, pre-occupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the &amp;amp;amp;quot;fundamental problem of causal inference&amp;quot;) prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the $\backslash$textit{unconfoundedness} and positivity assumptions (also known as exchangeability and overlap assumptions). This article reviews popular supervised machine learning algorithms, including the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g. the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g. targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates.;,citation_author=Noémi Kreif;,citation_author=Karla DiazOrdaz;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1903.00402;,citation_doi=10.1093/acrefore/9780190625979.013.256;,citation_journal_title=Oxford Research Encyclopedia of Economics and Finance;">
<meta name="citation_reference" content="citation_title=The central role of the propensity score in observational studies for causal effects;,citation_abstract=The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.;,citation_author=Paul R. Rosenbaum;,citation_author=Donald B. Rubin;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=1;,citation_doi=10.1017/CBO9780511810725.016;,citation_isbn=9780511810725;,citation_volume=70;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package;,citation_abstract=The Toolkit for Weighting and Analysis of Nonequivalent Groups , twang, contains a set of functions and procedures to support causal modeling of observational data through the estimation and evaluation of propensity scores and associated weights. This package was ... $\backslash$n;,citation_author=Greg Ridgeway;,citation_author=Dan Mccaffrey;,citation_author=Andrew Morral;,citation_author=Matthew Cefalu;,citation_author=Lane Burgette;,citation_author=Beth Ann Griffin;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_doi=10.7249/tl136.1;">
<meta name="citation_reference" content="citation_title=Estimating individual treatment effect: Generalization bounds and algorithms;,citation_abstract=There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms leam a &amp;amp;amp;quot;balanced&amp;quot; representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.;,citation_author=Uri Shalit;,citation_author=Fredrik D. Johansson;,citation_author=David Sontag;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1606.03976;">
<meta name="citation_reference" content="citation_title=Causal inference in statistics: An overview;,citation_abstract=This review presents empirical researcherswith recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called &amp;amp;amp;quot;causal effects&amp;quot; or &quot;policy evaluation&quot;) (2) queries about probabilities of counterfactuals, (including assessment of &quot;regret,&quot; &quot;attribution&quot; or &quot;causes of effects&quot;) and (3) queries about direct and indirect effects (also known as &quot;mediation&quot;). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.;,citation_author=Judea Pearl;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_doi=10.1214/09-SS057;,citation_issn=19357516;,citation_volume=3;">
<meta name="citation_reference" content="citation_title=Excess of COVID-19 cases and deaths due to fine particulate matter exposure during the 2020 wildfires in the United States;,citation_abstract=The year 2020 brought unimaginable challenges in public health, with the confluence of the COVID-19 pandemic and wildfires across the western United States. Wildfires produce high levels of fine particulate matter (PM2.5). Recent studies reported that short-term exposure to PM2.5 is associated with increased risk of COVID-19 cases and deaths. We acquired and linked publicly available daily data on PM2.5, the number of COVID-19 cases and deaths, and other confounders for 92 western U.S. counties that were affected by the 2020 wildfires. We estimated the association between short-term exposure to PM2.5 during the wildfires and the epidemiological dynamics of COVID-19 cases and deaths. We adjusted for several time-varying confounding factors (e.g., weather, seasonality, long-term trends, mobility, and population size). We found strong evidence that wildfires amplified the effect of short-term exposure to PM2.5 on COVID-19 cases and deaths, although with substantial heterogeneity across counties.;,citation_author=Xiaodan Zhou;,citation_author=Kevin Josey;,citation_author=Leila Kamareddine;,citation_author=Miah C. Caine;,citation_author=Tianjia Liu;,citation_author=Loretta J. Mickley;,citation_author=Matthew Cooper;,citation_author=Francesca Dominici;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=33;,citation_doi=10.1126/sciadv.abi8789;,citation_issn=23752548;,citation_pmid=34389545;,citation_volume=7;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003;,citation_abstract=Propensity-score methods are increasingly being used to reduce the impact of treatment-selection bias in the estimation of treatment effects using observational data. Commonly used propensity-score methods include covariate adjustment using the propensity score, stratification on the propensity score, and propensityscore matching. Empirical and theoretical research has demonstrated that matching on the propensity score eliminates a greater proportion of baseline differences between treated and untreated subjects than does stratification on the propensity score. However, the analysis of propensity-score-matched samples requires statistical methods appropriate for matched-pairs data. We critically evaluated 47 articles that were published between 1996 and 2003 in the medical literature and that employed propensity-score matching. We found that only two of the articles reported the balance of baseline characteristics between treated and untreated subjects in the matched sample and used correct statistical methods to assess the degree of imbalance. Thirteen (28 per cent) of the articles explicitly used statistical methods appropriate for the analysis of matched data when estimating the treatment effect and its statistical significance. Common errors included using the log-rank test to compare Kaplan–Meier survival curves in the matched sample, using Cox regression, logistic regression, chi-squared tests, t-tests, and Wilcoxon rank sum tests in the matched sample, thereby failing to account for the matched nature of the data. We provide guidelines for the analysis and reporting of studies that employ propensity-score matching.;,citation_author=Peter C. Austin;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=April;,citation_doi=10.1002/sim.3150;,citation_volume=27;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=The contribution of wildfire to PM2.5 trends in the USA;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.unep.org/resources/report/spreading-wildfire-rising-threat-extraordinary-landscape-fires?gad{\_}source=1{\&amp;amp;amp;}gclid=CjwKCAjwwr6wBhBcEiwAfMEQs4akEBSfMrNFii0aVgYlS-DOo-zlKl{\_}yb8q0JC5zm5LzDeC{\_}eM6YQxoCPSMQAvD{\_}BwE;">
<meta name="citation_reference" content="citation_title=A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting;,citation_abstract=Propensity score matching (PSM) and propensity score weighting (PSW) are popular tools to estimate causal effects in observational studies. We address two open issues: how to estimate propensity scores and assess covariate balance. Using simulations, we compare the performance of PSM and PSW based on logistic regression and machine learning algorithms (CART; Bagging; Boosting; Random Forest; Neural Networks; naive Bayes). Additionally, we consider several measures of covariate balance (Absolute Standardized Average Mean (ASAM) with and without interactions; measures based on the quantile-quantile plots; ratio between variances of propensity scores; area under the curve (AUC)) and assess their ability in predicting the bias of PSM and PSW estimators. We also investigate the importance of tuning of machine learning parameters in the context of propensity score methods. Two simulation designs are employed. In the first, the generating processes are inspired to birth register data used to assess the effect of labor induction on the occurrence of caesarean section. The second exploits more general generating mechanisms. Overall, among the different techniques, random forests performed the best, especially in PSW. Logistic regression and neural networks also showed an excellent performance similar to that of random forests. As for covariate balance, the simplest and commonly used metric, the ASAM, showed a strong correlation with the bias of causal effects estimators. Our findings suggest that researchers should aim at obtaining an ASAM lower than 10% for as many variables as possible. In the empirical study we found that labor induction had a small and not statistically significant impact on caesarean section.;,citation_author=Massimo Cannas;,citation_author=Bruno Arpino;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=4;,citation_doi=10.1002/bimj.201800132;,citation_issn=15214036;,citation_pmid=31090108;,citation_volume=61;,citation_journal_title=Biometrical Journal;">
<meta name="citation_reference" content="citation_title=Causal Inference : 4 Potential Outcomes Causal Model;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=https://doi.org/10.2307/j.ctv1c29t27;,citation_isbn=9780300255881;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Methamphetamine use and HIV risk behavior among men who inject drugs: Causal inference using coarsened exact matching;,citation_abstract=Background: Understanding the association between methamphetamine (MA) use and HIV risk behavior among people who inject drugs (PWID) will assist policy-makers and program managers to sharpen the focus of HIV prevention interventions. This study examines the relationship between MA use and HIV risk behavior among men who inject drugs (MWID) in Tehran, Iran, using coarsened exact matching (CEM). Methods: Data for these analyses were derived from a cross-sectional study conducted between June and July 2016. We assessed three outcomes of interest - all treated as binary variables, including distributive and receptive needle and syringe (NS) sharing and condomless sex during the month before interview. Our primary exposure of interest was whether study participants reported any MA use in the month prior to the interview. Firstly, we report the descriptive statistics for the pooled samples and matched sub-samples using CEM. The pooled and matched estimates of the associations and their 95% CI were estimated using a logistic regression model. Results: Overall, 500 MWID aged between 18 and 63 years (mean = 28.44, SD = 7.22) were recruited. Imbalances in the measured demographic characteristics and risk behaviors between MA users and non-users were attenuated using matching. In the matched samples, the regression models showed participants who reported MA use were 1.82 times more likely to report condomless sex (OR = 1.82 95% CI 1.51, 4.10; P = 0.031), and 1.35 times more likely to report distributive NS sharing in the past 30 days, as compared to MA non-users (OR = 1.35 95% CI 1.15-1.81). Finally, there was a statistically significant relationship between MA use and receptive NS sharing in the past month. People who use MA in the last month had higher odds of receptive NS sharing when compared to MA non-users (OR = 4.2 95% CI 2.7, 7.5; P = 0.013). Conclusions: Our results show a significant relationship between MA use and HIV risk behavior among MWID in Tehran, Iran. MA use was related with increased NS sharing, which is associated with higher risk for HIV exposure and transmission.;,citation_author=Mehdi Noroozi;,citation_author=Peter Higgs;,citation_author=Alireza Noroozi;,citation_author=Bahram Armoon;,citation_author=Bentolhoda Mousavi;,citation_author=Rosa Alikhani;,citation_author=Mohammad Rafi Bazrafshan;,citation_author=Ali Nazeri Astaneh;,citation_author=Azadeh Bayani;,citation_author=Ladan Fattah Moghaddam;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=66;,citation_doi=10.1186/s12954-020-00411-1;,citation_issn=14777517;,citation_pmid=32957982;,citation_volume=17;,citation_journal_title=Harm Reduction Journal;,citation_publisher=Harm Reduction Journal;">
<meta name="citation_reference" content="citation_title=Model-Based Direct Ad justment;,citation_abstract=Direct adjustment or standardization applies population weights to sub- class means in an effort to estimate population quantities from a sample that is not representative of the population. Direct adjustment has several attractive features, but when there are many subclasses it can attach large weights to small quantities of data, often in a fairly erratic manner. In the extreme, direct adjustment can attach infinite weight to nonexistent data, a noticeable inconvenience in practice. This article proposes a method of model-based direct adjustment that preserves the attractive features of conventional direct adjustment while stabilizing the weights attached to small subclasses. The sample mean and conventional direct adjustment are both special cases of model-based direct adjustment under two different extreme models for the subclass-specific selection proba- bilities. The discussion of this method provides some insights into the behavior of true and estimated propensity scores: the estimated scores are better than the true ones for almost the same reason that direct adjustment can outperform the sample mean in a simple random sample. The method is applied to a nonrandom sample in an effort to estimate a discrete distribution of essay scores in the College Board’s Advanced Placement Examination in Biology.;,citation_author=Paul R Rosenbaum;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_issue=398;,citation_volume=82;">
<meta name="citation_reference" content="citation_title=Causal Inference: What If;,citation_author=A. Miguel Hernan;,citation_author=M. James Robins;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1201/9781315374932;,citation_isbn=9781315374932;">
<meta name="citation_reference" content="citation_title=Causal inference using regression on the treatment variable;,citation_abstract=9.1 Causal inference and predictive comparisons So far, we have been interpreting regressions predictively: given the values of several inputs, the fitted model allows us to predict y, considering the n data points as a simple random sample from a hypothetical infinite &amp;amp;amp;quot; superpopulation &amp;quot; or probability distribution. Then we can make comparisons across different combinations of values for these inputs. This chapter and the next consider causal inference, which concerns what would happen to an outcome y as a result of a hypothesized &quot; treatment &quot; or intervention. In a regression framework, the treatment can be written as a variable T : 1 T i = 1 if unit i receives the &quot; treatment &quot; 0 if unit i receives the &quot; control, &quot; or, for a continuous treatment, T i = level of the &quot; treatment &quot; assigned to unit i. In the usual regression context, predictive inference relates to comparisons between units, whereas causal inference addresses comparisons of different treatments if applied to the same units. More generally, causal inference can be viewed as a special case of prediction in which the goal is to predict what would have happened under different treatment options. We shall discuss this theoretical framework more thoroughly in Section 9.2. Causal interpretations of regression coefficients can only be justified by relying on much stricter assumptions than are needed for predictive inference. To motivate the detailed study of regression models for causal effects, we present two simple examples in which predictive comparisons do not yield appropriate causal inferences. Hypothetical example of zero causal effect but positive predictive comparison;,citation_author=Andrew Gelman;,citation_author=Jennifer Hill;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_doi=10.1017/cbo9780511790942.012;,citation_journal_title=Data Analysis Using Regression and Multilevel/Hierarchical Models;">
<meta name="citation_reference" content="citation_title=Random Forests;,citation_abstract=Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same The distribution for all trees in the forest. generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.;,citation_author=Leo Brieman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_doi=https://doi.org/10.1023/A:1010933404324;,citation_issn=2158107X;,citation_volume=45;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=The health effects of ambient PM2.5 and potential mechanisms;,citation_abstract=The impacts of ambient PM2.5 on public health have become great concerns worldwide, especially in the developing countries. Epidemiological and toxicological studies have shown that PM2.5 does not only induce cardiopulmonary disorders and/or impairments, but also contributes to a variety of other adverse health effects, such as driving the initiation and progression of diabetes mellitus and eliciting adverse birth outcomes. Of note, recent findings have demonstrated that PM2.5 may still pose a hazard to public health even at very low levels (far below national standards) of exposure. The proposed underlying mechanisms whereby PM2.5 causes adverse effects to public health include inducing intracellular oxidative stress, mutagenicity/genotoxicity and inflammatory responses. The present review aims to provide an brief overview of new insights into the molecular mechanisms linking ambient PM2.5 exposure and health effects, which were explored with new technologies in recent years.;,citation_author=Shaolong Feng;,citation_author=Dan Gao;,citation_author=Fen Liao;,citation_author=Furong Zhou;,citation_author=Xinming Wang;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.ecoenv.2016.01.030;,citation_doi=10.1016/j.ecoenv.2016.01.030;,citation_issn=10902414;,citation_pmid=26896893;,citation_volume=128;,citation_journal_title=Ecotoxicology and Environmental Safety;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Do production contracts raise farm productivity? an instrumental variables approach;,citation_abstract=Estimating how the use of production contracts affects farm productivity is difficult when unobservable factors are correlated with both the decision to contract and productivity. To account for potential selection bias, this study uses the local availability of production contracts as an instrument for whether a farm uses a contract in order to estimate the impact of contract use on total factor productivity. Results indicate that use of a production contract is associated with a large increase in productivity for feeder-to-finish hog farms in the United States. The instrumental variable method makes it credible to assert that the observed association is a causal relationship rather than simply a correlation. Copyright 2008 Northeastern Agricultural and Resource Economics Association.;,citation_author=Nigel Key;,citation_author=William D. McBride;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=2;,citation_doi=10.1017/S1068280500002987;,citation_issn=10682805;,citation_volume=37;,citation_journal_title=Agricultural and Resource Economics Review;">
<meta name="citation_reference" content="citation_title=Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania;,citation_abstract=On April 1, 1992, New Jersey’s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law we surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage. We also compare employment changes at stores in New Jersey that were initially paying high wages (above $5) to the changes at lower-wage stores. We find no indication that the rise in the minimum wage reduced employment.;,citation_author=David Card;,citation_author=Alan B Krueger;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=4;,citation_doi=10.3386/w4509;,citation_volume=84;,citation_journal_title=American Economic Review;">
<meta name="citation_reference" content="citation_title=Greedy Function Approximation: A Gradient Boosting Machine;,citation_author=Jerome H. Friedman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_fulltext_html_url=https://www.jstor.org/stable/2699986;,citation_issue=5;,citation_volume=29;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Confidence intervals for random forests: The jackknife and the infinitesimal jackknife;,citation_abstract=We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = ⊖(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = ⊖(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.  2014 Stefan Wager, Trevor Hastie and Bradley Efron.;,citation_author=Stefan Wager;,citation_author=Trevor Hastie;,citation_author=Bradley Efron;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1311.4555;,citation_issn=15337928;,citation_pmid=25580094;,citation_volume=15;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Metalearners for estimating heterogeneous treatment effects using machine learning;,citation_abstract=There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.;,citation_author=Sören R. Künzel;,citation_author=Jasjeet S. Sekhon;,citation_author=Peter J. Bickel;,citation_author=Bin Yu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03461;,citation_issue=10;,citation_doi=10.1073/pnas.1804597116;,citation_issn=10916490;,citation_pmid=30770453;,citation_volume=116;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Causal Inference of Human Resources Key Performance Indicators;,citation_abstract=The purpose of this study is to examine the relationship between attrition rates and key per- formance indicators in a corporate workforce by using the propensity score (PS) matching. The study shows the possibilities of using logistic regression and propensity score matching methods in human capital strategic decisions. The data used here was from a fictional data set created by IBM data scientists based on active and separated employees to uncover the factors that lead to employee attrition. For each of the 1,470 employee records, information was generated about de- mographic characteristics such as age, gender, marital status, education level, employment status and culture, compensation, and performance factors. Two logistic equations are defined for two key performance objectives, culture and work life balance. A logistic regression analysis on each equation, with support from contrast estimation, reveals a comparison between the most and least favorable responses to key performance indicators is most significant. After successfully balancing a treatment and control group using the nearest neighbor matching technique on propensity score estimates from the logistic regression, a paired t-test reveals a statistically significant difference for the work life balance key performance indi- cator. This result is interpreted as having the highest probability of successfully reducing attrition when the focus is on increasing employee responses to satisfaction levels of work life balance in comparison to other key performance indicators.;,citation_author=Matthew Kovach;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/;,citation_dissertation_institution=Bowling Green State University;">
<meta name="citation_reference" content="citation_title=Machine Learning for Causal Inference;,citation_author=Sheng Li;,citation_author=Zhixuan Chu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1007/978-3-031-35051-1;,citation_isbn=9783031350504;">
<meta name="citation_reference" content="citation_title=Joint culpability: The effects of medical marijuana laws on crime;,citation_abstract=Most U.S. states have passed medical marijuana laws. In this paper, we study the effects of these laws on violent and property crime. We first estimate models that control for city fixed effects and flexible city-specific time trends. To supplement this regression analysis, we use the synthetic control method which can relax the parallel trend assumption and better account for heterogeneous policy effects. Both the regression analysis and the synthetic control method suggest no causal effects of medical marijuana laws on violent or property crime at the national level. We also find no strong effects within individual states, except for in California where the medical marijuana law reduced both violent and property crime by 20%.;,citation_author=Yu-Wei Luke Chu;,citation_author=Wilbur Townsend;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1016/j.jebo.2018.07.003;,citation_doi=10.1016/j.jebo.2018.07.003;,citation_issn=01672681;,citation_volume=159;,citation_journal_title=Journal of Economic Behavior and Organization;,citation_publisher=Elsevier B.V.;">
<meta name="citation_reference" content="citation_title=Air pollution and COVID-19 mortality in the United States: Strengths and limitations of an ecological regression analysis;,citation_abstract=Assessing whether long-term exposure to air pollution increases the severity of COVID-19 health outcomes, including death, is an important public health objective. Limitations in COVID-19 data availability and quality remain obstacles to conducting conclusive studies on this topic. At present, publicly available COVID-19 outcome data for representative populations are available only as area-level counts. Therefore, studies of long-term exposure to air pollution and COVID-19 outcomes using these data must use an ecological regression analysis, which precludes controlling for individual-level COVID-19 risk factors. We describe these challenges in the context of one of the first preliminary investigations of this question in the United States, where we found that higher historical PM2.5 exposures are positively associated with higher county-level COVID-19 mortality rates after accounting for many area-level confounders. Motivated by this study, we lay the groundwork for future research on this important topic, describe the challenges, and outline promising directions and opportunities.;,citation_author=X. Wu;,citation_author=R. C. Nethery;,citation_author=M. B. Sabath;,citation_author=D. Braun;,citation_author=F. Dominici;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=45;,citation_doi=10.1126/SCIADV.ABD4049;,citation_issn=23752548;,citation_pmid=33148655;,citation_volume=6;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=The state of applied econometrics: Causality and policy evaluation;,citation_author=Susan Athey;,citation_author=Guido W. Imbens;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1607.00699;,citation_issue=2;,citation_doi=10.1257/jep.31.2.3;,citation_issn=08953309;,citation_volume=31;,citation_journal_title=Journal of Economic Perspectives;">
<meta name="citation_reference" content="citation_title=Reflection on modern methods: When worlds collide - Prediction, machine learning and causal inference;,citation_abstract=Causal inference requires theory and prior knowledge to structure analyses, and is not usuallythought of as an arena for the application of prediction modelling. However, contemporary causal inference methods, premised on counterfactual or potential outcomes approaches, often include processing steps before the final estimation step. The purposes of this paper are: (i) to overview the recent emergence of prediction underpinning steps in contemporary causal inference methods as a usefulperspective on contemporary causal inference methods, and (ii) explore the role of machine learning(as one approach to ’best prediction’) in causal inference. Causal inference methods covered include propensity scores, inverse probability of treatment weights (IPTWs), G computation and targeted maximum likelihood estimation (TMLE). Machine learning has been used more for propensity scores and TMLE, and there is potential for increased use in G computation and estimation of IPTWs.;,citation_author=Tony Blakely;,citation_author=John Lynch;,citation_author=Koen Simons;,citation_author=Rebecca Bentley;,citation_author=Sherri Rose;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=6;,citation_doi=10.1093/ije/dyz132;,citation_issn=14643685;,citation_pmid=31298274;,citation_volume=49;,citation_journal_title=International Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=A tutorial on propensity score estimation for multiple treatments using generalized boosted models;,citation_abstract=The use of propensity scores to control for pretreatment imbalances on observed variables in non-randomized or observational studies examining the causal effects of treatments or interventions has become widespread over the past decade. For settings with two conditions of interest such as a treatment and a control, inverse probability of treatment weighted estimation with propensity scores estimated via boosted models has been shown in simulation studies to yield causal effect estimates with desirable properties. There are tools (e.g., the twang package in R) and guidance for implementing this method with two treatments. However, there is not such guidance for analyses of three or more treatments. The goals of this paper are twofold: (1) to provide step-by-step guidance for researchers who want to implement propensity score weighting for multiple treatments and (2) to propose the use of generalized boosted models (GBM) for estimation of the necessary propensity score weights. We define the causal quantities that may be of interest to studies of multiple treatments and derive weighted estimators of those quantities. We present a detailed plan for using GBM to estimate propensity scores and using those scores to estimate weights and causal effects. We also provide tools for assessing balance and overlap of pretreatment variables among treatment groups in the context of multiple treatments. A case study examining the effects of three treatment programs for adolescent substance abuse demonstrates the methods.  2013 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Daniel F. Mccaffrey;,citation_author=Beth Ann Griffin;,citation_author=Daniel Almirall;,citation_author=Mary Ellen Slaughter;,citation_author=Rajeev Ramchand;,citation_author=Lane F. Burgette;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=19;,citation_doi=10.1002/sim.5753;,citation_issn=02776715;,citation_pmid=23508673;,citation_volume=32;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=Group Average Treatment Effects for Observational Studies;,citation_abstract=The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. The groups can be understood as a broader aggregation of the conditional average treatment effect (CATE) where the number of groups is set in advance. In economics, this approach is similar to pre-analysis plans. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias, we implement a doubly-robust estimator in the first stage. We use machine learning methods to learn the conditional mean functions as well as the propensity score. The group average treatment effect is then estimated via a linear projection model. The linear model is easy to interpret, provides p-values and confidence intervals, and limits the danger of finding spurious heterogeneity due to small subgroups in the CATE. To control for confounding in the linear model, we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We find that our proposed method has lower absolute errors as well as smaller bias than the benchmark doubly-robust estimator. We further introduce a bagging type averaging for the CATE function for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.;,citation_author=Daniel Jacob;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1911.02688;">
<meta name="citation_reference" content="citation_title=Recursive partitioning for heterogeneous causal effects;,citation_abstract=In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without &amp;amp;amp;quot;sparsity&amp;quot; assumptions. We propose an &quot;honest&quot; approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the &quot;ground truth&quot; for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90% confidence intervals, whereas coverage ranges between 74% and 84% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22%.;,citation_author=Susan Athey;,citation_author=Guido Imbens;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1504.01132;,citation_issue=27;,citation_doi=10.1073/pnas.1510489113;,citation_issn=10916490;,citation_pmid=27382149;,citation_volume=113;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=RFRSF: Employee Turnover Prediction Based on Random Forests and Survival Analysis;,citation_abstract=In human resource management, employee turnover problem is heavily concerned by managers since the leave of key employees can bring great loss to the company. However, most existing researches are employee-centered, which ignored the historical events of turnover behaviors or the longitudinal data of job records. In this paper, from an event-centered perspective, we design a hybrid model based on survival analysis and machine learning, and propose a turnover prediction algorithm named RFRSF, which combines survival analysis for censored data processing and ensemble learning for turnover behavior prediction. In addition, we take strategies to handle employees with multiple turnover records so as to construct survival data with censored records. We compare RFRSF with several baseline methods on a real dataset crawled from one of the biggest online professional social platforms of China. The results show that the survival analysis model can significantly benefit the employee turnover prediction performance.;,citation_author=Ziwei Jin;,citation_author=Jiaxing Shang;,citation_author=Qianwen Zhu;,citation_author=Chen Ling;,citation_author=Wu Xie;,citation_author=Baohua Qiang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1007/978-3-030-62008-0_35;,citation_isbn=9783030620073;,citation_issn=16113349;,citation_volume=12343 LNCS;,citation_journal_title=Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);">
<meta name="citation_reference" content="citation_title=Propensity score estimation with boosted regression for evaluating causal effects in observational studies;,citation_abstract=Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pre-treatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This article demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. The authors illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences and substantially alter the apparent relative effects of adolescent substance abuse treatment.;,citation_author=Daniel F. McCaffrey;,citation_author=Greg Ridgeway;,citation_author=Andrew R. Morral;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=4;,citation_doi=10.1037/1082-989X.9.4.403;,citation_issn=1082989X;,citation_pmid=15598095;,citation_volume=9;,citation_journal_title=Psychological Methods;">
<meta name="citation_reference" content="citation_title=Bagging predictors;,citation_author=Leo Breiman;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_doi=10.3390/risks8030083;,citation_issn=22279091;,citation_volume=24;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Estimation and Inference of Heterogeneous Treatment Effects using Random Forests;,citation_abstract=Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman’s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.;,citation_author=Stefan Wager;,citation_author=Susan Athey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1080/01621459.2017.1319839;,citation_issue=523;,citation_doi=10.1080/01621459.2017.1319839;,citation_issn=1537274X;,citation_volume=113;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=The contribution of wildfire to PM2.5 trends in the USA;,citation_abstract=Steady improvements in ambient air quality in the USA over the past several decades, in part a result of public policy1,2, have led to public health benefits1–4. However, recent trends in ambient&nbsp;concentrations of&nbsp;particulate matter with diameters less than 2.5 $\mu$m (PM2.5), a pollutant regulated under the Clean Air Act1, have stagnated or begun to reverse throughout much of the USA5. Here we use a combination of ground- and satellite-based air pollution data from 2000 to 2022 to quantify the contribution of wildfire smoke to these PM2.5 trends. We find that since at least 2016, wildfire smoke has influenced trends in average annual PM2.5 concentrations in nearly three-quarters of states in the contiguous USA, eroding about 25% of previous multi-decadal progress in reducing PM2.5 concentrations on average in those states, equivalent to 4 years of air quality progress, and more than 50% in many western states. Smoke influence on trends in the number of days with extreme PM2.5 concentrations is detectable by 2011, but the influence can be detected primarily in western and mid-western states. Wildfire-driven increases in ambient PM2.5 concentrations are unregulated under current air pollution law6 and, in the absence of further interventions, we show that the contribution&nbsp;of wildfire to regional and national air quality trends is likely to grow as the climate continues to warm.;,citation_author=Marshall Burke;,citation_author=Marissa L. Childs;,citation_author=Brandon Cuesta;,citation_author=Minghao Qiu;,citation_author=Jessica Li;,citation_author=Carlos F. Gould;,citation_author=Sam Heft-Neal;,citation_author=Michael Wara;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=7984;,citation_doi=10.1038/s41586-023-06522-6;,citation_issn=14764687;,citation_pmid=37730996;,citation_volume=622;,citation_journal_title=Nature;,citation_publisher=Springer US;">
<meta name="citation_reference" content="citation_title=Marginal structural models in clinical research: When and how to use them?;,citation_abstract=Marginal structural models are a multi-step estimation procedure designed to control for the effect of confounding variables that change over time, and are affected by previous treatment. When a time-varying confounder is affected by prior treatment standard methods for confounding control are inappropriate, because over time the covariate plays both the role of confounder and mediator of the effect of treatment on outcome. Marginal structural models first calculate a weight to assign to each observation. These weights reflect the extent to which observations with certain characteristics (covariate values) are under-represented or over-represented in the sample with the respect to a target population in which these characteristics are balanced across treatment groups. Then, marginal structural models estimate the outcome of interest taking into account these weights. Marginal structural models are a powerful method for confounding control in longitudinal study designs that collect time-varying information on exposure, outcome and other covariates.;,citation_author=Tyler Williamson;,citation_author=Pietro Ravani;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=February;,citation_doi=10.1093/ndt/gfw341;,citation_issn=14602385;,citation_pmid=28201767;,citation_volume=32;,citation_journal_title=Nephrology Dialysis Transplantation;">
<meta name="citation_reference" content="citation_title=Optimizing variance-bias trade-off in the TWANG package for estimation of propensity scores;,citation_abstract=While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique [generalized boosted models (GBM)] to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner’s general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.;,citation_author=Layla Parast;,citation_author=Daniel F. McCaffrey;,citation_author=Lane F. Burgette;,citation_author=Fernando Hoces Guardia;,citation_author=Daniela Golinelli;,citation_author=Jeremy N. V. Miles;,citation_author=Beth Ann Griffin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=3-4;,citation_doi=10.1007/s10742-016-0168-2;,citation_issn=15729400;,citation_volume=17;,citation_journal_title=Health Services and Outcomes Research Methodology;,citation_publisher=Springer US;">
<meta name="citation_reference" content="citation_title=We are all social scientists now: How big data, machine learning, and causal inference work together;,citation_author=Justin Grimmer;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=1;,citation_doi=10.1017/S1049096514001784;,citation_issn=15375935;,citation_volume=48;,citation_journal_title=American Political Science Association;">
<meta name="citation_reference" content="citation_title=Recent developments in the econometrics of program evaluation;,citation_abstract=Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.;,citation_author=Guido W. Imbens;,citation_author=Jeffrey M. Wooldridge;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=1;,citation_doi=10.1257/jel.47.1.5;,citation_issn=00220515;,citation_volume=47;,citation_journal_title=Journal of Economic Literature;">
<meta name="citation_reference" content="citation_title=Agricultural extension and technology adoption for food security: Evidence from Uganda;,citation_abstract=We evaluate causal impacts of a large-scale agricultural extension program for smallholder women farmers on technology adoption and food security in Uganda through a regression discontinuity design that exploits an arbitrary distance-to-branch threshold for village program eligibility. We find eligible farmers used better basic cultivation methods, achieved improved food security. Given minimal changes in adoption of relatively expensive inputs, we attribute these gains to improved cultivation methods that require low upfront monetary investment. Farmers also modified their shockcoping methods. These results highlight the role of information and training in boosting agricultural productivity among poor farmers and, indirectly, improving food security.;,citation_author=Yao Pan;,citation_author=Stephen C. Smith;,citation_author=Munshi Sulaiman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_doi=10.1093/ajae/aay012;,citation_issn=14678276;,citation_volume=100;,citation_journal_title=American Journal of Agricultural Economics;">
<meta name="citation_reference" content="citation_title=Practical Guide to Honest Causal Forests for Identifying Heterogeneous Treatment Effects;,citation_abstract=&amp;amp;amp;quot;Heterogeneous treatment effects&amp;quot;is a term which refers to conditional average treatment effects (i.e., CATEs) that vary across population subgroups. Epidemiologists are often interested in estimating such effects because they can help detect populations that may particularly benefit from or be harmed by a treatment. However, standard regression approaches for estimating heterogeneous effects are limited by preexisting hypotheses, test a single effect modifier at a time, and are subject to the multiple-comparisons problem. In this article, we aim to offer a practical guide to honest causal forests, an ensemble tree-based learning method which can discover as well as estimate heterogeneous treatment effects using a data-driven approach. We discuss the fundamentals of tree-based methods, describe how honest causal forests can identify and estimate heterogeneous effects, and demonstrate an implementation of this method using simulated data. Our implementation highlights the steps required to simulate data sets, build honest causal forests, and assess model performance across a variety of simulation scenarios. Overall, this paper is intended for epidemiologists and other population health researchers who lack an extensive background in machine learning yet are interested in utilizing an emerging method for identifying and estimating heterogeneous treatment effects.;,citation_author=Neal Jawadekar;,citation_author=Katrina Kezios;,citation_author=Michelle C. Odden;,citation_author=Jeanette A. Stingone;,citation_author=Sebastian Calonico;,citation_author=Kara Rudolph;,citation_author=Adina Zeki Al Hazzouri;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=7;,citation_doi=10.1093/aje/kwad043;,citation_issn=14766256;,citation_pmid=36843042;,citation_volume=192;,citation_journal_title=American Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed;,citation_abstract=Matching-type estimators using the propensity score are the major workhorse in active labour market policy evaluation. This work investigates if machine learning algorithms for estimating the propensity score lead to more credible estimation of average treatment effects on the treated using a radius matching framework. Considering two popular methods, the results are ambiguous: We find that using LASSO based logit models to estimate the propensity score delivers more credible results than conventional methods in small and medium sized high dimensional datasets. However, the usage of Random Forests to estimate the propensity score may lead to a deterioration of the performance in situations with a low treatment share. The application reveals a positive effect of the training programme on days in employment for long-term unemployed. While the choice of the “first stage” is highly relevant for settings with low number of observations and few treated, machine learning and conventional estimation becomes more similar in larger samples and higher treatment shares.;,citation_author=Daniel Goller;,citation_author=Michael Lechner;,citation_author=Andreas Moczall;,citation_author=Joachim Wolff;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=March;,citation_doi=10.1016/j.labeco.2020.101855;,citation_issn=09275371;,citation_volume=65;,citation_journal_title=Labour Economics;,citation_publisher=Elsevier B.V.;">
<meta name="citation_reference" content="citation_title=Machine Learning Methods Economists Should Know Athey and Imbens 2019;,citation_abstract=We discuss the relevance of the recent Machine Learning (ML) literature for eco- nomics and econometrics. First we discuss the differences in goals, methods and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the machine learning literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics, methods that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, problems that include causal inference for average treat- ment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.;,citation_author=Susan Athey;,citation_author=Guido W Imbens;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://www.annualreviews.org/content/journals/10.1146/annurev-economics-080217-053433;,citation_doi=10.1146/annurev-economics-080217-053433;,citation_volume=11;,citation_journal_title=Annual Review of Economics;">
<meta name="citation_reference" content="citation_title=Estimating Causal Effects of Treatments in Experimental and Observational Studies;,citation_abstract=A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible, but the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases.;,citation_author=Donald B. Rubin;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_issue=5;,citation_doi=10.1002/j.2333-8504.1972.tb00631.x;,citation_issn=0424-6144;,citation_volume=66;,citation_journal_title=Journal of Educational Psychology;">
<meta name="citation_reference" content="citation_title=TDS: Heterogeneous Treatment Effect and Meta Learners;,citation_author=Zain Ahmed;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=An experiment in cotwin control: Adaptation to space travel;,citation_abstract=Monozygotic (MZ) twins who experience different environmental exposures offer an informative, naturally occurring cotwin control design. Given their genetic identity, differences in their physical, medical, and behavioral outcomes can be linked to their experiential differences. In space research, a broad range of ethological studies has been conducted on the effects of microgravity on sensorimotor activity, as well as on social group behavior during orbital flights, in a large array of isolated and confined environments, both for short-term, mid-term, and long-term missions. The study of MZ twin astronaut Scott Kelly, who spent almost 1-year at the International Space Station, while his cotwin Mark Kelly stayed on earth, is a unique opportunity to identify factors affecting astronauts’ health. This experiment of nature can also reveal the extent to which a space mission modifies different adaptive systems at the genetic and epigenetic levels.;,citation_author=Carole Tafforin;,citation_author=Nancy L. Segal;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.sciencedirect.com/book/9780128215142/twin-research-for-everyone?via=ihub=;,citation_doi=https://doi.org/10.1016/C2019-0-02208-X;,citation_isbn=9780128215159;,citation_inbook_title=Twin research for everyone;">
<meta name="citation_reference" content="citation_title=Why Propensity Scores Should Not Be Used for Matching;,citation_abstract=We show that propensity score matching (PSM), an enormously popular method of preprocessing data for causal inference, often accomplishes the opposite of its intended goal - thus increasing imbalance, inefficiency, model dependence, and bias. The weakness of PSM comes from its attempts to approximate a completely randomized experiment, rather than, as with other matching methods, a more efficient fully blocked randomized experiment. PSM is thus uniquely blind to the often large portion of imbalance that can be eliminated by approximating full blocking with other matching methods. Moreover, in data balanced enough to approximate complete randomization, either to begin with or after pruning some observations, PSM approximates random matching which, we show, increases imbalance even relative to the original data. Although these results suggest researchers replace PSM with one of the other available matching methods, propensity scores have other productive uses.;,citation_author=Gary King;,citation_author=Richard Nielsen;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=4;,citation_doi=10.1017/pan.2019.11;,citation_issn=14764989;,citation_volume=27;,citation_journal_title=Political Analysis;">
<meta name="citation_reference" content="citation_title=Using synthetic controls: Feasibility, data requirements, and methodological aspects;,citation_abstract=Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.;,citation_author=Alberto Abadie;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1257/jel.20191450;,citation_issn=00220515;,citation_volume=59;,citation_journal_title=Journal of Economic Literature;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=causaldata: Example data sets for causal inference textbooks;,citation_author=Nick Huntington-Klein;,citation_author=Malcolm Barrett;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=causaldata;">
<meta name="citation_reference" content="citation_title=cobalt: Covariate balance tables and plots;,citation_author=Noah Greifer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=cobalt;">
<meta name="citation_reference" content="citation_title=data.table: Extension of “data.frame”;,citation_author=Tyson Barrett;,citation_author=Matt Dowle;,citation_author=Arun Srinivasan;,citation_author=Jan Gorecki;,citation_author=Michael Chirico;,citation_author=Toby Hocking;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=data.table;">
<meta name="citation_reference" content="citation_title=gbm: Generalized boosted regression models;,citation_author=Greg Ridgeway;,citation_author=GBM Developers;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=gbm;">
<meta name="citation_reference" content="citation_title=kableExtra: Construct complex table with “kable” and pipe syntax;,citation_author=Hao Zhu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=kableExtra;">
<meta name="citation_reference" content="citation_title=knitr: A general-purpose package for dynamic report generation in r;,citation_author=Yihui Xie;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://yihui.org/knitr/;">
<meta name="citation_reference" content="citation_title=Dynamic documents with R and knitr;,citation_author=Yihui Xie;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://yihui.org/knitr/;">
<meta name="citation_reference" content="citation_title=knitr: A comprehensive tool for reproducible research in R;,citation_author=Yihui Xie;,citation_editor=Victoria Stodden;,citation_editor=Friedrich Leisch;,citation_editor=Roger D. Peng;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_inbook_title=Implementing reproducible computational research;">
<meta name="citation_reference" content="citation_title=How to interpret statistical models using marginaleffects in R and Python;,citation_author=Vincent Arel-Bundock;,citation_author=Noah Greifer;,citation_author=Andrew Heiss;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=MatchIt: Nonparametric preprocessing for parametric causal inference;,citation_author=Daniel E. Ho;,citation_author=Kosuke Imai;,citation_author=Gary King;,citation_author=Elizabeth A. Stuart;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=8;,citation_doi=10.18637/jss.v042.i08;,citation_volume=42;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=patchwork: The composer of plots;,citation_author=Thomas Lin Pedersen;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=patchwork;">
<meta name="citation_reference" content="citation_title=Classification and regression by randomForest;,citation_author=Andy Liaw;,citation_author=Matthew Wiener;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_fulltext_html_url=https://CRAN.R-project.org/doc/Rnews/;,citation_issue=3;,citation_volume=2;,citation_journal_title=R News;">
<meta name="citation_reference" content="citation_title=renv: Project environments;,citation_author=Kevin Ushey;,citation_author=Hadley Wickham;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=renv;">
<meta name="citation_reference" content="citation_title=rmarkdown: Dynamic documents for r;,citation_author=JJ Allaire;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Jonathan McPherson;,citation_author=Javier Luraschi;,citation_author=Kevin Ushey;,citation_author=Aron Atkins;,citation_author=Hadley Wickham;,citation_author=Joe Cheng;,citation_author=Winston Chang;,citation_author=Richard Iannone;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://github.com/rstudio/rmarkdown;">
<meta name="citation_reference" content="citation_title=R markdown: The definitive guide;,citation_author=Yihui Xie;,citation_author=J. J. Allaire;,citation_author=Garrett Grolemund;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://bookdown.org/yihui/rmarkdown;,citation_isbn=9781138359338;">
<meta name="citation_reference" content="citation_title=R markdown cookbook;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Emily Riederer;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://bookdown.org/yihui/rmarkdown-cookbook;,citation_isbn=9780367563837;">
<meta name="citation_reference" content="citation_title=showtext: Using fonts more easily in r graphs;,citation_author=Yixuan Qiu;,citation_author=undefined See file AUTHORS for details.;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=showtext;">
<meta name="citation_reference" content="citation_title=WeightIt: Weighting for covariate balance in observational studies;,citation_author=Noah Greifer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=WeightIt;">
<meta name="citation_reference" content="citation_title=Welcome to the tidyverse;,citation_author=Hadley Wickham;,citation_author=Mara Averick;,citation_author=Jennifer Bryan;,citation_author=Winston Chang;,citation_author=Lucy D’Agostino McGowan;,citation_author=Romain François;,citation_author=Garrett Grolemund;,citation_author=Alex Hayes;,citation_author=Lionel Henry;,citation_author=Jim Hester;,citation_author=Max Kuhn;,citation_author=Thomas Lin Pedersen;,citation_author=Evan Miller;,citation_author=Stephan Milton Bache;,citation_author=Kirill Müller;,citation_author=Jeroen Ooms;,citation_author=David Robinson;,citation_author=Dana Paige Seidel;,citation_author=Vitalie Spinu;,citation_author=Kohske Takahashi;,citation_author=Davis Vaughan;,citation_author=Claus Wilke;,citation_author=Kara Woo;,citation_author=Hiroaki Yutani;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=43;,citation_doi=10.21105/joss.01686;,citation_volume=4;,citation_journal_title=Journal of Open Source Software;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/pscore_theory.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Propensity Scores and Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning and the Propensity Score</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mitchellcameron123/ML-PS.git" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-and-the-Propensity-Score.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=|url|" title="Facebook" class="quarto-navigation-tool px-1" aria-label="Facebook"><i class="bi bi-facebook"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/intro_background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/pscore_theory.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Propensity Scores and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/implimentation_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tutorial: Implimentation, Workflow, and Example with <code>WeightIt</code> and<code>gbm</code> in R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coffee_replication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Replication Case Study</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusion and Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#a-conventional-approach-propensity-scores-and-balance" id="toc-a-conventional-approach-propensity-scores-and-balance" class="nav-link active" data-scroll-target="#a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">2.1</span> A Conventional Approach: Propensity Scores and Balance</a>
  <ul class="collapse">
  <li><a href="#sec-assessing-balance" id="toc-sec-assessing-balance" class="nav-link" data-scroll-target="#sec-assessing-balance"><span class="header-section-number">2.1.1</span> Assessing balance</a></li>
  <li><a href="#propensity-score-modelling-with-logistic-regression" id="toc-propensity-score-modelling-with-logistic-regression" class="nav-link" data-scroll-target="#propensity-score-modelling-with-logistic-regression"><span class="header-section-number">2.1.2</span> Propensity Score Modelling with Logistic Regression</a></li>
  </ul></li>
  <li><a href="#probability-machines-probability-theory-and-machine-learning" id="toc-probability-machines-probability-theory-and-machine-learning" class="nav-link" data-scroll-target="#probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">2.2</span> Probability Machines: Probability Theory and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#choice-of-loss-function-and-probability-prediction" id="toc-choice-of-loss-function-and-probability-prediction" class="nav-link" data-scroll-target="#choice-of-loss-function-and-probability-prediction"><span class="header-section-number">2.2.1</span> Choice of Loss Function and Probability Prediction</a></li>
  <li><a href="#sec-bagg-rf-probmachines" id="toc-sec-bagg-rf-probmachines" class="nav-link" data-scroll-target="#sec-bagg-rf-probmachines"><span class="header-section-number">2.2.2</span> Bagging and Random Forest as Probability Machines</a></li>
  <li><a href="#sec-gbm" id="toc-sec-gbm" class="nav-link" data-scroll-target="#sec-gbm"><span class="header-section-number">2.2.3</span> Gradient Boosting Machines as Probability Machines</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting"><span class="header-section-number">2.2.4</span> Overfitting</a></li>
  </ul></li>
  <li><a href="#sec-mlps-sims" id="toc-sec-mlps-sims" class="nav-link" data-scroll-target="#sec-mlps-sims"><span class="header-section-number">2.3</span> Comparison of Machine Learning Algorithms: Simulation Results</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mitchellcameron123/ML-PS.git/edit/main/chapters/pscore_theory.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-propensity" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Propensity Scores and Machine Learning</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="a-conventional-approach-propensity-scores-and-balance" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-conventional-approach-propensity-scores-and-balance"><span class="header-section-number">2.1</span> A Conventional Approach: Propensity Scores and Balance</h2>
<p>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from <a href="intro_background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, <a href="intro_background.html#eq-ate-estimate" class="quarto-xref">Equation&nbsp;<span>1.3</span></a> leads to a biased estimate of the treatment effect.</p>
<p>For example, consider the causal question: <em>“How much does obtaining a bachelors degree increase lifetime earnings?”</em>. Individuals who complete a bachelor’s degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the <em>cause</em> of higher incomes when the effect is confounded by the differences in covariates between groups. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of <em>covariate balance</em> which is a key concept behind underlying propensity score methods.</p>
<div id="nte-balance-intution" class="callout callout-style-default callout-note callout-titled" title="What is Covariate Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.1: What is Covariate Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are <em>balanced</em>. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that <a href="intro_background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> is satisfied when covariates are balanced.</p>
</div>
</div>
<p>In bachelor’s degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor’s degree’s impact on earnings because the individuals are exchangeable and satisfy <a href="intro_background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>1.7</span></a>. The covariates are said to be “conditioned on” by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a <em>dimensionality</em> problem as the dimension of the number of covariates increases.</p>
<p><span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="references.html#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as <span class="math inline">\(e(X)\)</span> and be expressed as:</p>
<p><span id="eq-pscore"><span class="math display">\[
e(X)=P(T=1|X).
\tag{2.1}\]</span></span></p>
<p>A prediction of the probability of treatment based on the covariates is the best summary of each covariate. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects.</p>
<p>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in <a href="intro_background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>1.7</span></a>. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="citation" data-cites="C5Mixtape2021">(<a href="references.html#ref-C5Mixtape2021" role="doc-biblioref">Cunningham 2021, chap. 4</a>)</span> who provides applications and coded examples in R, Python, and Stata.</p>
<div id="nte-balance-pscore" class="callout callout-style-default callout-note callout-titled" title="Balance and Propensity Scores">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.2: Balance and Propensity Scores
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that an RCT will satisfy <a href="intro_background.html#eq-independence" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy <a href="intro_background.html#eq-conditional-independence" class="quarto-xref">Equation&nbsp;<span>1.7</span></a> as the potential outcomes are independent of the treatment status conditioned on some covariates.</p>
<p>The propensity score is conditioned on the covariates because the covariates are the predictors of the propensity score. Conditioning on the propensity score aims to replicate an RCT in observational data by balancing covariates between groups. When observations are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates.</p>
</div>
</div>
<p>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</p>
<p>At a high level, the conditioned property of the propensity score is translated into a model by using PSM or IPW. <span class="citation" data-cites="King2019">King and Nielsen (<a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span> provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</p>
<section id="sec-assessing-balance" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="sec-assessing-balance"><span class="header-section-number">2.1.1</span> Assessing balance</h3>
<p>Balance assessment is an important step to ensure that conditioning on the propensity score has been successful. A commonly recommended measure of covariate balance is the standardized mean difference or SMD. This is the difference in the mean of each covariate between treatment groups standardized by a standardization factor so that it is on the same scale for all covariates.</p>
<p>SMDs close to zero indicate good balance. <span class="citation" data-cites="Austin2011">P. Austin (<a href="references.html#ref-Austin2011" role="doc-biblioref">2011</a>)</span> notes that <span class="math inline">\(0.1\)</span> is a common “threshold” for determining if a variable is balanced. This threshold is a guideline to the approximate region that indicates a covariate is balanced and should not be interpreted as a binary rule.</p>
</section>
<section id="propensity-score-modelling-with-logistic-regression" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="propensity-score-modelling-with-logistic-regression"><span class="header-section-number">2.1.2</span> Propensity Score Modelling with Logistic Regression</h3>
<p>A conventional propensity score model uses logistic regression to predict a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is a heuristically driven art rather than science. <span class="citation" data-cites="Brookhart2006 Heinrich2010">(<a href="references.html#ref-Brookhart2006" role="doc-biblioref">Brookhart et al. 2006</a>; <a href="references.html#ref-Heinrich2010" role="doc-biblioref">Heinrich 2010</a>)</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are not statistically significant. Notably, many researchers do not discuss the specification of propensity models in their papers. <span class="citation" data-cites="Austin2008">P. C. Austin (<a href="references.html#ref-Austin2008" role="doc-biblioref">2008</a>)</span> reviews <span class="math inline">\(47\)</span> papers that use propensity scores and find few perform adequate model selection, assess balance, or apply correct statistical tests.</p>
<p>It is important to note that the true propensity score is never observable. A propensity score that is close to the theoretically true probability is well calibrated. Poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. Propensity scores may be poorly calibrated as covariates may be omitted by error, poorly measured, or be unobservable. Logistic regression may not predict calibrated scores if the true relationship is non-linear or involves complex interactions between covariates. Another important note is that the propensity model itself does not have an informative <em>causal</em> interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for each variable which is not informative of the desired estimand.</p>
<p>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</p>
</section>
</section>
<section id="probability-machines-probability-theory-and-machine-learning" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="probability-machines-probability-theory-and-machine-learning"><span class="header-section-number">2.2</span> Probability Machines: Probability Theory and Machine Learning</h2>
<p>Supervised machine learning usually focuses on classifying observations into groups, or predicting continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability an observation belongs to a certain class.Machine learning methods that predict probabilities are sometimes called probability machines.</p>
<p>Probability machines are valuable in applications requiring calibrated probability predictions. For example, probability machines can predict loan defaults or other adverse events in finance. In marketing, they estimate the likelihood of customer response to a campaign. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</p>
<p>Probability machines offer many advantages over parametric methods like logistic regression:</p>
<ol type="1">
<li><p><strong>Improved Calibration</strong>: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</p></li>
<li><p><strong>Flexible Modelling</strong>: Unlike parametric methods like logistic regression, probability machines don’t rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</p></li>
<li><p><strong>Efficient Feature Selection</strong>: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</p></li>
<li><p><strong>Handling Missing Data</strong>: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</p></li>
<li><p><strong>Simplified Data Exploration</strong>: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</p></li>
</ol>
<p>In causal inference, probability machines can predict better calibrated propensity scores and better estimate treatment effects. This discussion aims to clarify the use of probability machines in causal inference given the unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</p>
<div id="nte-cart" class="callout callout-style-default callout-note callout-titled" title="A Particularly Important Method: Classification and Regression Trees">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.3: A Particularly Important Method: Classification and Regression Trees
</div>
</div>
<div class="callout-body-container callout-body">
<p>Moving forward, a particularly important model is the Classification and Regression Trees. <span class="citation" data-cites="Breiman1984">L. Breiman et al. (<a href="references.html#ref-Breiman1984" role="doc-biblioref">1984</a>)</span> introduces method, commonly known as CART, that partitions data according to a splitting criterion, resulting in an “if this, then that” interpretation. CART models are also widely known as a decision trees. The splits are recursive, meaning splits are applied upon previous splits, such as trees breaking into branches and then leaves. The splits are also <em>greedy</em> as each potential split only considers information available at that split instead of past or future splits. Each parent node is split to create two child nodes and the final nodes of a CART model are called terminal nodes.</p>
<p>For example, when classifying pets into cats versus dogs, the first split might be<em>“if barks”</em> and the second is <em>“heavier than 5 kg”</em>. The tree says <em>If it barks and is heavier than 5 kg, then it is a dog</em>.</p>
<p>A single classification tree typically uses the Gini index to determine its splits. Each split aims to maximize node purity, meaning the nodes contain the highest possible proportion of one class. The Gini index measures impurity, with lower gini values indicating higher purity. Intuitively, the aim of a classification tree’s loss function is to minimize the misclassification rate of observations. By selecting splits that reduce the Gini index, the tree minimise classification errors and accuracy.</p>
</div>
</div>
<section id="choice-of-loss-function-and-probability-prediction" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="choice-of-loss-function-and-probability-prediction"><span class="header-section-number">2.2.1</span> Choice of Loss Function and Probability Prediction</h3>
<p>The loss function measures the difference between a model’s predictions and the actual target values, serving as a measure of a model’s performance. The best model exists at the minimum of the loss function. Different loss functions influence a model’s behaviour so the choice of loss function is important. Classification models predict the category that each observation belongs to not the probability of each class. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Many classification loss functions minimize classification errors and improve accuracy as this results in the best classification. A loss function like the Gini index is effective for classification problems but it is unclear if this applies to probability problems. In other words, minimizing misclassification error may not lead to accurate probability predictions.</p>
<p>At a high level, to classify an observation, <span class="math inline">\(x_i\)</span> as an <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, a model needs to determine if <span class="math inline">\(\Pr(x_i=A)\)</span> is less than or greater than <span class="math inline">\(0.5\)</span>. If <span class="math inline">\(\widehat{\Pr}(x_i=A) &gt; 0.5\)</span>, then it is more likely to be an <span class="math inline">\(A\)</span> and if <span class="math inline">\(\widehat{\Pr}(x_i=A) &lt;0.5\)</span> then it is more likely to be a <span class="math inline">\(B\)</span>. Thus, if <span class="math inline">\(x_i\)</span> is an <span class="math inline">\(A\)</span> it is trivial if <span class="math inline">\(\widehat{\Pr}(x_i=A)\)</span> is <span class="math inline">\(0.51\)</span> or <span class="math inline">\(0.99\)</span> as this makes no difference to the classification as an <span class="math inline">\(A\)</span>. But the difference between <span class="math inline">\(\widehat{\Pr}(x_i=A) = 0.51\)</span> and <span class="math inline">\(0.99\)</span> is extreme for a probability machine. It is important to understand that classification models are optimized for classification accuracy rather than probability prediction. This distinction affects the performance of ensemble methods like random forests or bagging ensembles that use classification trees for probability prediction.</p>
</section>
<section id="sec-bagg-rf-probmachines" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="sec-bagg-rf-probmachines"><span class="header-section-number">2.2.2</span> Bagging and Random Forest as Probability Machines</h3>
<div id="nte-ensemble" class="callout callout-style-default callout-note callout-titled" title="A Quick Note on Ensemble Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.4: A Quick Note on Ensemble Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ensemble learning refers to a general framework of machine learning that combines together multiple simple models to create a better model. The philosophy behind ensemble learning is rooted in the wisdom of crowds principle, where the collective decision of multiple models often outperforms that of individual models. Often, ensemble learning methods use multiple CART models.</p>
<p>Bagging ensembles, random forest, and gradient boosting are all machine learning methods that combine many weaker models and so are all examples of ensemble learning.</p>
</div>
</div>
<p>Across an entire bagging ensemble <span class="citation" data-cites="Breiman1996">(see <a href="references.html#ref-Breiman1996" role="doc-biblioref">Leo Breiman 1996</a>)</span> or random forest <span class="citation" data-cites="Breiman2001">(see <a href="references.html#ref-Breiman2001" role="doc-biblioref">Brieman 2001</a>)</span>, class probabilities are determined through a <em>vote count</em> method. Within that ensemble, each tree makes a class prediction based on the majority class in a terminal node. For instance, if <span class="math inline">\(x_i\)</span> lies in a terminal node where <span class="math inline">\(80\%\)</span> of the observations are classified as an <span class="math inline">\(A\)</span>, that <em>individual tree</em> will classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span>. The ensemble’s overall prediction for <span class="math inline">\(x_i\)</span> is derived from the proportion of trees that classify <span class="math inline">\(x_i\)</span> as <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. Thus the ensemble <em>counts votes</em> for each class across the ensemble. Let <span class="math inline">\(T\)</span> be the total number of trees and <span class="math inline">\(b_t\)</span> be the <span class="math inline">\(t\)</span>-th tree in the ensemble. Let <span class="math inline">\(\mathbb{I}(b_t(x_i) = A)\)</span> be the indicator function that returns <span class="math inline">\(1\)</span> when <span class="math inline">\(b_t\)</span> predicts that observation <span class="math inline">\(x_i\)</span> belongs to class <span class="math inline">\(A\)</span>. The probability of class <span class="math inline">\(A\)</span> for observation <span class="math inline">\(x_i\)</span> is calculated as:</p>
<p><span id="eq-ensemble-vote-method"><span class="math display">\[
\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A).
\tag{2.2}\]</span></span></p>
<p><span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="references.html#ref-Olson2018" role="doc-biblioref">2018</a>)</span> notes a potential bias towards predictions of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> when trees in a bagged ensemble or random forest are highly correlated. Using a vote count method, an ensemble will bias predicted probabilities towards <span class="math inline">\(\hat{P}(x_i=A) \in \{0,1\}\)</span> when trees are highly correlated. Imagine that each tree in the ensemble is perfectly correlated implying that each tree will make the the same class prediction for each <span class="math inline">\(x_i\)</span>. For such an ensemble, the predicted probabilities will will be exactly <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> using a vote count. Of course an ensemble of identical trees is unrealistic but the intuition still applies in the real world where ensembles may have some degree of correlation. The larger the correlation, the more the probability predictions will exhibit a <em>divergence bias</em> towards <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</p>
<p>If <span class="math inline">\(x_i\)</span> has a known membership of <span class="math inline">\(A\)</span>, and an unknown <span class="math inline">\(\Pr_{\text{true}}(x_i=A) = 0.6\)</span>, the ensemble might classify <span class="math inline">\(x_i\)</span> correctly <span class="math inline">\(90\%\)</span> of the time leading to <span class="math inline">\(\widehat{\Pr}(x_i=A) = 0.9\)</span>. As a probability machine, the ensemble has overestimated the probability by <span class="math inline">\(0.3\)</span> even though a <span class="math inline">\(90\%\)</span> classification accuracy is excellent. To predict <span class="math inline">\(P_{\text{true}}(x_i=A) = 0.6\)</span>, an ensemble needs to incorrectly classify <span class="math inline">\(x_i\)</span> in <span class="math inline">\(40\%\)</span> of its trees. However, bagging ensembles and random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</p>
<p>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected bootstrapped sample of the data. Random forests further reduce tree correlation by considering a random number of variables at each split, commonly referred to as <span class="math inline">\(mtry\)</span> in software implementations. When <span class="math inline">\(mtry\)</span> is equal to to number of predictors, the model considers all variables at each split and the random forest is equal to a bagging ensemble. A lower <span class="math inline">\(mtry\)</span> should reduce the correlation between trees and decrease divergence bias as the structure of the tree is modified by the selected variables at each split. However, a lower <span class="math inline">\(mtry\)</span> also introduces other theoretical problems.</p>
<p>Consider the scenario where the binary outcome (treatment and control) of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If <span class="math inline">\(mtry\)</span> is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. Each predictor has a chance of <span class="math inline">\(\frac{mtry}{\text{number of predictors}}\)</span> of selection at each split implying a lower <span class="math inline">\(mtry\)</span> decreases the chance of a splitting on the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of <span class="math inline">\(x_i\)</span> in each tree. Such an ensemble may have highly unstable probability predictions.</p>
<p>Additionally, consider there is a class imbalance and the majority of observations are classified as <span class="math inline">\(A\)</span> not <span class="math inline">\(B\)</span>. The terminal nodes of each tree within an ensemble are more likely to contain the majority class. Consequently, there is a <em>majority class bias</em> as each tree in the ensemble is more likely to misclassifying an observation as an <span class="math inline">\(A\)</span> because the terminal nodes have a higher proportion of <span class="math inline">\(A\)</span> due to the higher proportion of <span class="math inline">\(A\)</span>’s in the data overall.</p>
<div id="nte-class-imbalance" class="callout callout-style-default callout-note callout-titled" title="Class Imbalance and Machine Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.5: Class Imbalance and Machine Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>When there is a difference in the number of observations in each class, this is called class imbalance. It is important to note that majority class bias exists in conventional classification tasks with machine learning. Bagging ensembles and random forest are well known to be sensitive to class imbalance meaning that class predictions are biased towards the majority <span class="citation" data-cites="Bader2019">(see <a href="references.html#ref-Bader2019" role="doc-biblioref">Bader-El-Den, Teitei, and Perry 2019</a>)</span>.</p>
<p>However, the class imbalance problem is particularly notable when predicting probabilities. The probability prediction from a vote count method is very sensitive to a change in the votes from each tree. Suppose that balanced data results in <span class="math inline">\(80/100\)</span> trees classifying <span class="math inline">\(B\)</span> as <span class="math inline">\(B\)</span> and imbalanced data (more <span class="math inline">\(A\)</span> than <span class="math inline">\(B\)</span>) reduces correct classifications of <span class="math inline">\(B\)</span> to <span class="math inline">\(60/100\)</span>. This results in a <span class="math inline">\(20\%\)</span> margin of error in probability estimates but the classification remains as <span class="math inline">\(B\)</span>.</p>
</div>
</div>
<p>Individually, a low <span class="math inline">\(mtry\)</span> can lead to unstable probability predictions and class imbalance can create bias towards the majority class. But probability machines are particularly effected when there is both a low <span class="math inline">\(mtry\)</span> <em>and</em> class imbalance. Because successive noisy splits (relating to a low <span class="math inline">\(mtry\)</span>) result in impure child nodes, the effects of majority class bias are exaggerated. Without the ability to separate the classes, the majority class will dominate terminal nodes. If the ensemble was able to split on informative covariates each time (<span class="math inline">\(mtry\)</span> is higher), then it should still be able to create pure splits even when there is some class imbalance. In other words, if there is a small class imbalance, reducing <span class="math inline">\(mtry\)</span> may reveal majority class bias not visible at higher <span class="math inline">\(mtry\)</span>’s. Equally, if there is low <span class="math inline">\(mtry\)</span>, then even a small class imbalance can lead to majority class bias.</p>
<p>To exemplify these theoretical points, the National Supported Work (NSW) programme is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with <span class="math inline">\(445\)</span> total participants, <span class="math inline">\(185\)</span> in the program group, and <span class="math inline">\(260\)</span> in the control group, so the true probability of treatment for each individual can be calculated as <span class="math inline">\(185/445=0.42\)</span> or <span class="math inline">\(42\%\)</span>. Thus, the “best” calibrated probability estimates will be close too <span class="math inline">\(42\%\)</span> for all observations. Further information about this data is found in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 6.1.1</span></a>.</p>
<p>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although <a href="#fig-rf-varimp" class="quarto-xref">Figure&nbsp;<span>2.2</span></a> and <a href="implimentation_workflow.html#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by <span class="citation" data-cites="Smith2005">Smith and Todd (<a href="references.html#ref-Smith2005" role="doc-biblioref">2005</a>)</span> who suggests that self-selection bias is evident in the NSW data.</p>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows both divergence bias and majority class bias using <code>randomForest()</code> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when <span class="math inline">\(mtry\)</span> is equal to the number of predictors and so specifying <code>mtry = 7</code> in the <code>randomForest()</code> function fits a bagging ensemble. Additionally, logistic regression using the <code>gbm()</code> function provides a meaningful comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure 2.1</a></summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>randomForest 4.7-1.1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Type rfNews() to see new features/changes/bug fixes.</code></pre>
</div>
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure 2.1</a></summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'ggplot2'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:randomForest':

    margin</code></pre>
</div>
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure 2.1</a></summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ragg)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(custom_ggplot_theme)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>nsw_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">as.factor</span>(treat) <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                          black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> marr)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>logit_preds <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="fu">binomial</span>())<span class="sc">$</span>fitted.values </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>rf_mtry1_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">randomForest</span>(nsw_formula, </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">data =</span> nsw_data), </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>                          <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>bagging_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(nsw_formula, <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>, </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data =</span> nsw_data)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>bagged_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagging_model, <span class="at">newdata =</span> nsw_data, <span class="at">type =</span> <span class="st">"prob"</span>)[, <span class="dv">2</span>]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plot_pmachines <span class="ot">&lt;-</span> <span class="cf">function</span>(pscores, plot_subtitle) {</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(nsw_data, <span class="fu">aes</span>(<span class="at">x =</span> pscores, <span class="at">fill =</span> <span class="fu">factor</span>(treat))) <span class="sc">+</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">linewidth =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>), </span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                      <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Participant"</span>)) <span class="sc">+</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">subtitle =</span> plot_subtitle, <span class="at">x =</span> <span class="st">"Propensity Scores"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>         <span class="at">fill =</span> <span class="st">"Group:"</span>) <span class="sc">+</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    custom_ggplot_theme</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(logit_preds, <span class="st">"Logistic Regression"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>) <span class="sc">+</span> </span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(rf_mtry1_preds, <span class="st">"Random Forest (mtry = 1)"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="cn">NULL</span>) <span class="sc">+</span> </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">plot_pmachines</span>(bagged_preds, <span class="st">"Bagging (Bootstrap Aggregation)"</span>)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">/</span> p2 <span class="sc">/</span> p3 <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>, <span class="at">theme =</span> custom_ggplot_theme)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-theory-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pscore_theory_files/figure-html/fig-rf-theory-demo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-theory-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Compares the density estimates of the propensity scores for control and participant groups in the National Supported Work programme. <code>randomForest()</code> fits a random forest with <code>mtry = 1</code> and bagging ensemble with <code>mtry = 7</code>. The default values of <code>ntree = 500</code> and <code>nodesize = 1</code> are used. A logistic regression model is included for a comparison.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure 2.2</a></summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ lubridate 1.9.3     ✔ tibble    3.2.1
✔ purrr     1.0.2     ✔ tidyr     1.3.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::combine()  masks randomForest::combine()
✖ dplyr::filter()   masks stats::filter()
✖ dplyr::lag()      masks stats::lag()
✖ ggplot2::margin() masks randomForest::margin()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-rf-theory-demo">Figure 2.2</a></summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>                      <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), </span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>             <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>, <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-rf-varimp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pscore_theory_files/figure-html/fig-rf-varimp-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-varimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Compares the variable importance assigned to each variable from a bagging ensemble fitted on data from the National Supported Work programme. <code>randomForest()</code> fits a baggin ensemble with <code>mtry = 7</code> with default <code>ntree = 500</code> and <code>nodesize = 1</code>.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows the logistic regression model has identified a central tendency and most propensities are between <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which roughly aligns with the true probability. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside <span class="math inline">\(0.25\)</span> and <span class="math inline">\(0.75\)</span> which is likely related to tree correlation. For the random forest with <span class="math inline">\(mtry=1\)</span>, a significant number of the treatment and control observations are centred near the control area (<span class="math inline">\(T=0\)</span>) with a wide range of other predictions. Recall that the control group is the majority class. Reducing <span class="math inline">\(mtry\)</span> from <span class="math inline">\(7\)</span> to <span class="math inline">\(1\)</span> reveals the majority class bias reinforcing the theoretical discussion that a combination of low <span class="math inline">\(mtry\)</span> and class imbalance is especially troubling. The models over predicts the majority class and has unstable predictions otherwise. Both random forest and bagging ensembles have performed poorly compared to the true probability of <span class="math inline">\(0.42\%\)</span>.</p>
<p>The tuning of <span class="math inline">\(mtry\)</span> faces double jeopardy and is another important area of discussion in probability machines. The selection of <span class="math inline">\(mtry\)</span> is typically carried out in with a classification loss function such as accuracy or out-of-bag error. <span class="citation" data-cites="Olson2018">Olson and Wyner (<a href="references.html#ref-Olson2018" role="doc-biblioref">2018</a>)</span> compares tuning <span class="math inline">\(mtry\)</span> measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of <span class="math inline">\(mtry\)</span> for classification differs from probability prediction.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In other words, if a grid search finds that <span class="math inline">\(mtry=3\)</span> is optimal for a classification task, this does not imply that <span class="math inline">\(mtry=3\)</span> is optimal for predicting probabilities so the tuning of <span class="math inline">\(mtry\)</span> is difficult.</p>
<p>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in <a href="#sec-mlps-sims" class="quarto-xref"><span>Section 2.3</span></a>. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</p>
<p>Heuristically, divergence bias and majority class bias will most affect a probability machine when there is considerable overlap of true probabilities between groups. Recall the meaning of <em>common support</em> and <em>overlap</em> from <a href="intro_background.html#sec-assumptions" class="quarto-xref"><span>Section 1.4</span></a>. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common support may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low <span class="math inline">\(mtry\)</span>, the terminal nodes of each tree may be relatively impure leading to a majority class bias. Alternatively, if true probabilities exist near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and there is a clear separation of class, divergence bias may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low <span class="math inline">\(mtry\)</span> may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</p>
</section>
<section id="sec-gbm" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="sec-gbm"><span class="header-section-number">2.2.3</span> Gradient Boosting Machines as Probability Machines</h3>
<!-- going to have to cut some of this out -->
<p>Moving beyond classification trees in random forests or bagging ensembles, <span class="citation" data-cites="Friedman2001">Friedman (<a href="references.html#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> introduced the <em>Gradient Boosting Machine</em> (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss function to reach a minimum. The update rule for the model after each iteration can be expressed as:</p>
<p><span id="eq-gbm-update"><span class="math display">\[
\hat{p}_i^{(t)} = \hat{p}_i^{(t-1)} + \lambda \cdot b_t(x_i),
\tag{2.3}\]</span></span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the learning rate, and <span class="math inline">\(b_t(x_i)\)</span> is the <span class="math inline">\(b\)</span>-th regression tree fitted on the pseudo-residuals of the previous regression tree. In words, the current overall iteration <span class="math inline">\(t\)</span>, is a combination of the previous model plus the current iteration scaled by a learning rate.</p>
<p>A learning rate controls the contribution of each weak learner to the final model. By using a small learning rate, the machine learns slowly so that it can slowly descend the loss function. This allows for finer adjustments during the iterative process to better capture patterns in the data.</p>
<p>GBMs can be generalized to many different applications by minimizing a different loss function which can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple regression trees and a logistic function to transform regression predictions into probabilities. Specifically, the logistic function used is:</p>
<p><span id="eq-gbm-logit"><span class="math display">\[
\hat{p}_i = \frac{1}{1 + \exp(-\text{model}(x_i))}.
\tag{2.4}\]</span></span></p>
<p>This logistic function is the same as in logistic regression, so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood with logistic regression. Maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The full GBM model, <span class="math inline">\(f_T(x)\)</span> after <span class="math inline">\(T\)</span> iterations can be written as:</p>
<p><span id="eq-gbm-model"><span class="math display">\[
f_T(x_i) = b_1(x_i) + \lambda \sum_{t=1}^{T} b_t(x_i).
\tag{2.5}\]</span></span></p>
<p>Inside a base tree, each split considers all variables and makes the most informative split to descend the loss function using gradient descent. GBMs utilize many weak learners, such as a regression tree with a single split called a regression stump. However, additional splits enable the model to capture interactions between terms, which may increase probability calibration in complex or high-dimensional datasets.</p>
<p>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques, as well as allowing a probability distribution-based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. Since GBMs predict probabilities from a logistic function, they avoid problems associated with a vote count method. Additionally, there are no difficult parameters to tune, such as <span class="math inline">\(mtry\)</span> in a random forest. The implementation and workflow to fit a GBM for propensity scores is discussed in <a href="implimentation_workflow.html#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.1</span></a>.</p>
<p><a href="#fig-gbm-demo" class="quarto-xref">Figure&nbsp;<span>2.3</span></a> shows the propensity scores resulting from a GBM model using the <code>gbm, package</code> on the NSW data provides. <code>gbm</code> is a notable performance improvement to random forest and bagging shown in <a href="#fig-rf-theory-demo" class="quarto-xref">Figure&nbsp;<span>2.1</span></a>. Recall that a “better” model would predict probabilities near to <span class="math inline">\(42\%\)</span> as this is the treatment/control share in the randomised NSW data.</p>
<div class="cell">
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-gbm-demo">Figure 2.3</a></summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded gbm 2.2.2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3</code></pre>
</div>
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-gbm-demo">Figure 2.3</a></summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>nsw_gbm <span class="ot">&lt;-</span> <span class="fu">gbm</span>(treat <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    marr, <span class="at">distribution =</span> <span class="st">"bernoulli"</span>, <span class="at">data =</span> nsw_data, <span class="at">n.trees =</span> <span class="dv">10000</span>, </span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">shrinkage =</span> <span class="fl">0.0001</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>boosted_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(nsw_gbm, <span class="at">type =</span> <span class="st">"response"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Using 10000 trees...</code></pre>
</div>
<details class="code-fold">
<summary>Show the Code to Create <a href="fig-gbm-demo">Figure 2.3</a></summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_pmachines</span>(boosted_preds, <span class="st">"Gradient Boosting Machine with `gbm`"</span>) <span class="sc">+</span> </span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">25</span>)) <span class="sc">+</span> </span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">labs</span>(</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>) <span class="sc">+</span> </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Scale for y is already present.
Adding another scale for y, which will replace the existing scale.</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-gbm-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gbm-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pscore_theory_files/figure-html/fig-gbm-demo-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gbm-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Density estimates of the propensity scores for control and participant groups in the National Supported Work programme using the <code>gbm</code> package with <code>distribution = "bernoulli"</code>, <code>data = nsw_data</code>, <code>n.trees = 10000</code>, and <code>shrinkage = 0.0001</code>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="overfitting" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">2.2.4</span> Overfitting</h3>
<p>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting can prevent overfitting to ensure a model can generalise to unseen data.</p>
<p>However, the model is not required to generalise a propensity score model as different datasets will have a different model. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</p>
<div id="nte-overfit-logistic" class="callout callout-style-default callout-note callout-titled" title="Overfitting in Logistic Regression">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2.6: Overfitting in Logistic Regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is <span class="citation" data-cites="Schuster2016">Schuster, Lowe, and Platt (<a href="references.html#ref-Schuster2016" role="doc-biblioref">2016</a>)</span>, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> receive artificially inflated weighting.</p>
</div>
</div>
<p><span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="references.html#ref-Lee2010" role="doc-biblioref">2010</a>)</span> simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</p>
<p>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</p>
</section>
</section>
<section id="sec-mlps-sims" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-mlps-sims"><span class="header-section-number">2.3</span> Comparison of Machine Learning Algorithms: Simulation Results</h2>
<p>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="citation" data-cites="McCaffrey2004 Setoguchi2008 Lee2010 Cannas2019 Tu2019 Goller2020 Ferri2020">(see <a href="references.html#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="references.html#ref-Setoguchi2008" role="doc-biblioref">Setoguchi et al. 2008</a>; <a href="references.html#ref-Lee2010" role="doc-biblioref">Lee, Lessler, and Stuart 2010</a>; <a href="references.html#ref-Cannas2019" role="doc-biblioref">Cannas and Arpino 2019</a>; <a href="references.html#ref-Tu2019" role="doc-biblioref">Tu 2019</a>; <a href="references.html#ref-Goller2020" role="doc-biblioref">Goller et al. 2020</a>; <a href="references.html#ref-Ferri2020" role="doc-biblioref">Ferri-García and Del Mar Rueda 2020</a>)</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</p>
<p><span class="citation" data-cites="Tu2019">Tu (<a href="references.html#ref-Tu2019" role="doc-biblioref">2019</a>)</span> compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</p>
<p>Despite poor theoretical properties as a probability machine, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="references.html#ref-Lee2010" role="doc-biblioref">2010</a>)</span> find that bagging results in the lowest standard error across many datasets.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</p>
<p>Additionally, <span class="citation" data-cites="Lee2010">Lee, Lessler, and Stuart (<a href="references.html#ref-Lee2010" role="doc-biblioref">2010</a>)</span> finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best <span class="math inline">\(95\%\)</span> coverage in all simulation scenarios, with <span class="math inline">\(98.6\%\)</span> coverage.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="references.html#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</p>
<p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="references.html#ref-Goller2020" role="doc-biblioref">2020</a>)</span> adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</p>
<p>Based on a review of the literature, the findings can be distilled into five important points:</p>
<ol type="1">
<li><p>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</p></li>
<li><p>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="citation" data-cites="Rosenbaum1983">(<a href="references.html#ref-Rosenbaum1983" role="doc-biblioref">Rosenbaum and Rubin 1983</a>)</span>.</p></li>
<li><p>Bagging ensembles perform poorly, a finding replicated across multiple studies.</p></li>
<li><p>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</p></li>
<li><p>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</p></li>
<li><p>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</p></li>
<li><p>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</p></li>
</ol>
<p>Short concluding remark</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Austin2011" class="csl-entry" role="listitem">
Austin, Peter. 2011. <span>“<span class="nocase">An introduction to propensity score methods for reducing the effects of confounding in observational studies</span>.”</span> <em>Multivariate Behavioral Research</em> 46 (3): 399–424. <a href="https://doi.org/10.1080/00273171.2011.568786">https://doi.org/10.1080/00273171.2011.568786</a>.
</div>
<div id="ref-Austin2008" class="csl-entry" role="listitem">
Austin, Peter C. 2008. <span>“<span class="nocase">A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003</span>.”</span> <em>Statistics in Medicine</em> 27 (April): 2037–49. <a href="https://doi.org/10.1002/sim.3150">https://doi.org/10.1002/sim.3150</a>.
</div>
<div id="ref-Bader2019" class="csl-entry" role="listitem">
Bader-El-Den, Mohammed, Eleman Teitei, and Todd Perry. 2019. <span>“<span class="nocase">Biased Random Forest for Dealing with the Class Imbalance Problem</span>.”</span> <em>IEEE Transactions on Neural Networks and Learning Systems</em> 30 (7): 2163–72. <a href="https://doi.org/10.1109/TNNLS.2018.2878400">https://doi.org/10.1109/TNNLS.2018.2878400</a>.
</div>
<div id="ref-Breiman1996" class="csl-entry" role="listitem">
Breiman, Leo. 1996. <span>“<span class="nocase">Bagging predictors</span>.”</span> <em>Machine Learning</em> 24: 123–40. <a href="https://doi.org/10.3390/risks8030083">https://doi.org/10.3390/risks8030083</a>.
</div>
<div id="ref-Breiman1984" class="csl-entry" role="listitem">
Breiman, L, Jerome H Friedman, Richard A Olshen, and C J Stone. 1984. <span>“<span class="nocase">Classification and Regression Trees</span>.”</span> <em>Biometrics</em> 40: 874. <a href="https://api.semanticscholar.org/CorpusID:29458883">https://api.semanticscholar.org/CorpusID:29458883</a>.
</div>
<div id="ref-Breiman2001" class="csl-entry" role="listitem">
Brieman, Leo. 2001. <span>“<span>Random Forests</span>.”</span> <em>Machine Learning</em> 45: 5–32. https://doi.org/<a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>.
</div>
<div id="ref-Brookhart2006" class="csl-entry" role="listitem">
Brookhart, M. Alan, Sebastian Schneeweiss, Kenneth J. Rothman, Robert J. Glynn, Jerry Avorn, and Til Stürmer. 2006. <span>“<span class="nocase">Variable selection for propensity score models</span>.”</span> <em>American Journal of Epidemiology</em> 163 (12): 1149–56. <a href="https://doi.org/10.1093/aje/kwj149">https://doi.org/10.1093/aje/kwj149</a>.
</div>
<div id="ref-Cannas2019" class="csl-entry" role="listitem">
Cannas, Massimo, and Bruno Arpino. 2019. <span>“<span class="nocase">A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting</span>.”</span> <em>Biometrical Journal</em> 61 (4): 1049–72. <a href="https://doi.org/10.1002/bimj.201800132">https://doi.org/10.1002/bimj.201800132</a>.
</div>
<div id="ref-C5Mixtape2021" class="csl-entry" role="listitem">
Cunningham, Scott. 2021. <span>“<span class="nocase">Matching and Subclassification</span>.”</span> In <em>Causal Inference: The Mixtape</em>, 175–240. Yale University Press. <a href="https://doi.org/10.2307/j.ctv1c29t27.8">https://doi.org/10.2307/j.ctv1c29t27.8</a>.
</div>
<div id="ref-Ferri2020" class="csl-entry" role="listitem">
Ferri-García, Ramón, and María Del Mar Rueda. 2020. <span>“<span class="nocase">Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys</span>.”</span> <em>PLoS ONE</em> 15 (4): 1–19. <a href="https://doi.org/10.1371/journal.pone.0231500">https://doi.org/10.1371/journal.pone.0231500</a>.
</div>
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="https://www.jstor.org/stable/2699986">https://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-Goller2020" class="csl-entry" role="listitem">
Goller, Daniel, Michael Lechner, Andreas Moczall, and Joachim Wolff. 2020. <span>“<span class="nocase">Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed</span>.”</span> <em>Labour Economics</em> 65 (March). <a href="https://doi.org/10.1016/j.labeco.2020.101855">https://doi.org/10.1016/j.labeco.2020.101855</a>.
</div>
<div id="ref-Heinrich2010" class="csl-entry" role="listitem">
Heinrich, Carolyn. 2010. <span>“<span class="nocase">A Primer for Applying Propensity-Score Matching</span>.”</span> <em>Development</em>, no. August: 59. <a href="http://www.iadb.org/document.cfm?id=35320229">http://www.iadb.org/document.cfm?id=35320229</a>.
</div>
<div id="ref-King2019" class="csl-entry" role="listitem">
King, Gary, and Richard Nielsen. 2019. <span>“<span class="nocase">Why Propensity Scores Should Not Be Used for Matching</span>.”</span> <em>Political Analysis</em> 27 (4): 435–54. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-Lee2010" class="csl-entry" role="listitem">
Lee, Brian K., Justin Lessler, and Elizabeth A. Stuart. 2010. <span>“<span class="nocase">Improving propensity score weighting using machine learning</span>.”</span> <em>Statistics in Medicine</em> 29: 337–46. <a href="https://doi.org/10.1002/sim.3782">https://doi.org/10.1002/sim.3782</a>.
</div>
<div id="ref-McCaffrey2004" class="csl-entry" role="listitem">
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. <span>“<span class="nocase">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</span>.”</span> <em>Psychological Methods</em> 9 (4): 403–25. <a href="https://doi.org/10.1037/1082-989X.9.4.403">https://doi.org/10.1037/1082-989X.9.4.403</a>.
</div>
<div id="ref-Olson2018" class="csl-entry" role="listitem">
Olson, Matthew A., and Abraham J. Wyner. 2018. <span>“<span class="nocase">Making Sense of Random Forest Probabilities: a Kernel Perspective</span>,”</span> 1–35. <a href="http://arxiv.org/abs/1812.05792">http://arxiv.org/abs/1812.05792</a>.
</div>
<div id="ref-Rosenbaum1983" class="csl-entry" role="listitem">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“<span class="nocase">The central role of the propensity score in observational studies for causal effects</span>.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1017/CBO9780511810725.016">https://doi.org/10.1017/CBO9780511810725.016</a>.
</div>
<div id="ref-Schuster2016" class="csl-entry" role="listitem">
Schuster, Tibor, Wilfrid Kouokam Lowe, and Robert W. Platt. 2016. <span>“<span class="nocase">Propensity score model overfitting led to inflated variance of estimated odds ratios</span>.”</span> <em>Journal of Clinical Epidemiology</em> 80: 97–106. <a href="https://doi.org/10.1016/j.jclinepi.2016.05.017">https://doi.org/10.1016/j.jclinepi.2016.05.017</a>.
</div>
<div id="ref-Setoguchi2008" class="csl-entry" role="listitem">
Setoguchi, Soko, Sebastian Schneeweiss, Alan M. Brookhart, Robert J. Glynn, and Francis E. Cook. 2008. <span>“<span class="nocase">Evaluating uses of data mining techniques in propensity score estimation: a simulation study</span>.”</span> <em>Pharmacoepidemiology and Drug Safety</em> 17 (March): 546–55. <a href="https://doi.org/10.1002/pds">https://doi.org/10.1002/pds</a>.
</div>
<div id="ref-Smith2005" class="csl-entry" role="listitem">
Smith, Jeffrey A., and Petra E. Todd. 2005. <em><span class="nocase">Does matching overcome LaLonde’s critique of nonexperimental estimators?</span></em> Vol. 125. 1-2 SPEC. ISS. <a href="https://doi.org/10.1016/j.jeconom.2004.04.011">https://doi.org/10.1016/j.jeconom.2004.04.011</a>.
</div>
<div id="ref-Tibshirani1996" class="csl-entry" role="listitem">
Tibshirani, Robert. 1996. <span>“<span class="nocase">Regression Shrinkage and Selection via the Lasso</span>.”</span> <em>Journal of the Royal Statistical Society</em> 58 (1): 267–88. <a href="https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=fastly-default{\%}3Aff57285d6b8854126d21a135984fc4ca{\&amp;}ab{\_}segments={\&amp;}origin={\&amp;}initiator={\&amp;}acceptTC=1">https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=fastly-default{\%}3Aff57285d6b8854126d21a135984fc4ca{\&amp;}ab{\_}segments={\&amp;}origin={\&amp;}initiator={\&amp;}acceptTC=1</a>.
</div>
<div id="ref-Tu2019" class="csl-entry" role="listitem">
Tu, Chunhao. 2019. <span>“<span class="nocase">Comparison of various machine learning algorithms for estimating generalized propensity score</span>.”</span> <em>Journal of Statistical Computation and Simulation</em> 89 (4): 708–19. <a href="https://doi.org/10.1080/00949655.2019.1571059">https://doi.org/10.1080/00949655.2019.1571059</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Note that tuning <span class="math inline">\(mtry\)</span> for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In this context, the coverage is the proportion of times that the true treatment effect is within the <span class="math inline">\(95\%\)</span> confidence interval across the number of simulations. This author implements <span class="math inline">\(1000\)</span> simulations of each scenario.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="Cannas2019">Cannas and Arpino (<a href="references.html#ref-Cannas2019" role="doc-biblioref">2019</a>)</span> provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <code>twang</code> and <code>gbm</code> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benefits discussed in <a href="#sec-gbm" class="quarto-xref"><span>Section 2.2.3</span></a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Goller2020">Goller et al. (<a href="references.html#ref-Goller2020" role="doc-biblioref">2020</a>)</span> calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the <em>true</em> treatment effect. Thus, the covariate balance table offers a clearer view of each method’s performance.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="citation" data-cites="Tibshirani1996">Tibshirani (<a href="references.html#ref-Tibshirani1996" role="doc-biblioref">1996</a>)</span> introduces LASSO regularization, short for Least Absolute Shrinkage and Selection Operator, is a technique used in linear regression to prevent overfitting by penalising the absolute size of the coefficients. It adds a penalty term to the ordinary least squares objective function, meaning that some coefficients may “shrink” to zero.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/intro_background.html" class="pagination-link" aria-label="Introduction and Background">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Background</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/implimentation_workflow.html" class="pagination-link" aria-label="Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tutorial: Implimentation, Workflow, and Example with <code>WeightIt</code> and<code>gbm</code> in R</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb19" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Propensity Scores and Machine Learning {#sec-propensity}</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="at">file =</span> <span class="st">"globals.RData"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Conventional Approach: Propensity Scores and Balance</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>In a randomised control trial (RCT), researchers believe treatment and control groups are similar because of randomisation. In this case, the similar groups are compatible and should not have systematic differences. For similar groups, the average treatment effect (ATT) is a contrast of means from @eq-ate-estimate. In observational data, the exposure to a treatment is unlikely to be random, implying there may be systematic differences between groups. Systematic differences refer to consistent variations or disparities between groups in the study. These differences are not due to random chance but rather indicate a pattern or trend, perhaps due to selection-bias. As groups are not comparable, @eq-ate-estimate leads to a biased estimate of the treatment effect.</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>For example, consider the causal question: *"How much does obtaining a bachelors degree increase lifetime earnings?"*. Individuals who complete a bachelor's degree are not selected at random for university programs (treatment) and may have different observable attributes than those who do not attend a university (control). Perhaps those who attend university have higher academic abilities, higher motivation, or grew up with parents with higher income. Because of these systematic group covariate differences, a simple comparison of mean income could lead to attributing university attendance as the *cause* of higher incomes when the effect is confounded by the differences in covariates between groups. In this example, the confounding covariates are academic ability, motivation, and parental income that impact the probability of someone obtaining a bachelors degree. This discussion introduces the idea of *covariate balance* which is a key concept behind underlying propensity score methods.</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-intution .callout-note title="What is Covariate Balance"}</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>Covariate balance is the idea that covariates are approximately equivalent across treatment and control groups. If the distribution of each covariate are the same for each group, then those covariates are *balanced*. If covariates are similar across groups, then there should not be any confounding. Equally, similar covariates across groups implies exchangability between groups as the two groups should be similar (thus can be exchanged). There is a conceptual equivalence between covariate balance, unconfoundedness, and exchangeability meaning that @eq-independence is satisfied when covariates are balanced. </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>In bachelor's degree example, suppose that comparable treatment and control individuals are matched together to create balanced pairs. Between these pairs, covariates are balanced such as the same academic ability, motivation, parent income, geographic residence etc. Comparing the balanced matched pairs should result in a robust estimate of a bachelor's degree's impact on earnings because the individuals are exchangeable and satisfy @eq-conditional-independence. The covariates are said to be "conditioned on" by matching individuals on these covariates. However, practically this matching is difficult to perform as exact matches cannot be made as the number of covariates increases. For example, finding two people with the same gender is simple but finding two people with the same gender, age, education, income, motivation, location, experience, and race is nearly impossible. Thus, there is a *dimensionality* problem as the dimension of the number of covariates increases.</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>@Rosenbaum1983 offer a valuable tool for analysing observational data called the propensity score. The propensity score is the probability of treatment assignment conditioned on observed covariates. Essentially, the propensity score reduces the dimension of the number of covariates to a single dimension to avoid the dimensionality problem. Let the propensity score be denoted as $e(X)$ and be expressed as:</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>e(X)=P(T=1|X).</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pscore}</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>A prediction of the probability of treatment based on the covariates is the best summary of each covariate. The covariate imbalance between bachelors degrees and controls arose from people self-selecting themselves into a bachelors degree programme because of these covariates. For example, people with higher motivation and academic ability are more likely to go to university. If it is the covariates that impact the probability of going to university, then a prediction of the probability of going to university based on these covariates should summarise the covariate effects. </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>Conditioning on this propensity score should balance the data and meet the conditional independence assumption stated in @eq-conditional-independence. There are many sources that offer a comprehensive guide to propensity score methods such as <span class="co">[</span><span class="ot">@C5Mixtape2021, Chapter 4</span><span class="co">]</span> who provides applications and coded examples in R, Python, and Stata.</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>::: {#nte-balance-pscore .callout-note title="Balance and Propensity Scores"}</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>Note that an RCT will satisfy @eq-independence as randomisation implies the potential outcomes are independent of the treatment assignment. Propensity score methods aim to satisfy @eq-conditional-independence as the potential outcomes are independent of the treatment status conditioned on some covariates. </span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>The propensity score is conditioned on the covariates because the covariates are the predictors of the propensity score. Conditioning on the propensity score aims to replicate an RCT in observational data by balancing covariates between groups. When observations are conditioned on their propensity score, differences in outcomes can be confidently attributed to the treatment itself, rather than to pre-existing differences in covariates.</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>Two common methods that use propensity scores are propensity score matching (PSM) and inverse propensity weighting (IPW). PSM creates matched sets with similar propensity scores. IPW creates a balanced pseudo-population, where observations are weighted on the inverse of the propensity score. The pseudo-population is created by up-weighting observations with a low propensity score and down-weighting observations with a high propensity score.</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>At a high level, the conditioned property of the propensity score is translated into a model by using PSM or IPW. @King2019 provide a notable criticism of propensity score matching, which is a very interesting read. In the following examples, IPW is used due to theoretical advantages and ease of software implementation.</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### Assessing balance {#sec-assessing-balance}</span></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>Balance assessment is an important step to ensure that conditioning on the propensity score has been successful. A commonly recommended measure of covariate balance is the standardized mean difference or SMD. This is the difference in the mean of each covariate between treatment groups standardized by a standardization factor so that it is on the same scale for all covariates. </span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>SMDs close to zero indicate good balance. @Austin2011 notes that $0.1$ is a common "threshold" for determining if a variable is balanced. This threshold is a guideline to the approximate region that indicates a covariate is balanced and should not be interpreted as a binary rule. </span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### Propensity Score Modelling with Logistic Regression</span></span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>A conventional propensity score model uses logistic regression to predict a probability between $0$ and $1$. Models may be specified to include interaction terms and polynomial terms so the model captures complex trends in the data. There are a range of approaches for specifying a propensity score model, but the process is a heuristically driven art rather than  science. <span class="co">[</span><span class="ot">@Brookhart2006; @Heinrich2010</span><span class="co">]</span>. One suggestion is to include two-way interaction terms between covariates and squared terms and then remove terms which are not statistically significant. Notably, many researchers do not discuss the specification of propensity models in their papers. @Austin2008 reviews $47$ papers that use propensity scores and find few perform adequate model selection, assess balance, or apply correct statistical tests.</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>It is important to note that the true propensity score is never observable. A propensity score that is close to the theoretically true probability is well calibrated. Poorly calibrated propensity scores may result in poor balance and biased estimation of the treatment effect. Propensity scores may be poorly calibrated as covariates may be omitted by error, poorly measured, or be unobservable. Logistic regression may not predict calibrated scores if the true relationship is non-linear or involves complex interactions between covariates. Another important note is that the propensity model itself does not have an informative *causal* interpretation. In logistic regression, the coefficients are the log-odds of the treatment assignment for each variable which is not informative of the desired estimand.</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>The first application of machine learning in causal inference was to predict propensity scores. Despite this, logistic regression still appears to be the most common model for predicting propensity scores.</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability Machines: Probability Theory and Machine Learning</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>Supervised machine learning usually focuses on classifying observations into groups, or predicting continuous outcomes. Probability prediction is a hybrid of these tasks, aiming to predict the continuous probability an observation belongs to a certain class.Machine learning methods that predict probabilities are sometimes called probability machines. </span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>Probability machines are valuable in applications requiring calibrated probability predictions. For example, probability machines can predict loan defaults or other adverse events in finance. In marketing, they estimate the likelihood of customer response to a campaign. Gamblers and bettors want robust probability predictions to enhance their betting strategies. Probability machines can be applied wherever calibrated probability predictions are needed.</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>Probability machines offer many advantages over parametric methods like logistic regression:</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Improved Calibration**: Probability machines often provide better-calibrated predictions by capturing complex data relationships.</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>**Flexible Modelling**: Unlike parametric methods like logistic regression, probability machines don't rely on assumptions of additivity or linearity, allowing them to model intricate relationships that parametric models miss.</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>**Efficient Feature Selection**: These machines automatically select features, making them ideal for high-dimensional datasets where manual selection is impractical.</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>**Handling Missing Data**: Probability machines handle missing data robustly, minimizing the need for extensive data reprocessing and imputation.</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>**Simplified Data Exploration**: By exploring complex data structures in a data-driven way, probability machines simplify model specification. For instance, tree-based models remain unaffected by adding squared or interaction terms, streamlining the modeling process.</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>In causal inference, probability machines can predict better calibrated propensity scores and better estimate treatment effects. This discussion aims to clarify the use of probability machines in causal inference given the unique requirements of propensity score specification. Probability machines are theoretically complex and there are unanswered questions in this space.</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>::: {#nte-cart .callout-note title="A Particularly Important Method: Classification and Regression Trees"}</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>Moving forward, a particularly important model is the Classification and Regression Trees. @Breiman1984 introduces method, commonly known as CART, that partitions data according to a splitting criterion, resulting in an "if this, then that" interpretation. CART models are also widely known as a decision trees. The splits are recursive, meaning splits are applied upon previous splits, such as trees breaking into branches and then leaves. The splits are also *greedy* as each potential split only considers information available at that split instead of past or future splits. Each parent node is split to create two child nodes and the final nodes of a CART model are called terminal nodes.  </span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>For example, when classifying pets into cats versus dogs, the first split might be*"if barks"* and the second is *"heavier than 5 kg"*.  The tree says *If it barks and is heavier than 5 kg, then it is a dog*.</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>A single classification tree typically uses the Gini index to determine its splits. Each split aims to maximize node purity, meaning the nodes contain the highest possible proportion of one class. The Gini index measures impurity, with lower gini values indicating higher purity. Intuitively, the aim of a classification tree’s loss function is to minimize the misclassification rate of observations. By selecting splits that reduce the Gini index, the tree minimise classification errors and accuracy.</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choice of Loss Function and Probability Prediction</span></span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>The loss function measures the difference between a model's predictions and the actual target values, serving as a measure of a model's performance. The best model exists at the minimum of the loss function. Different loss functions influence a model's behaviour so the choice of loss function is important. Classification models predict the category that each observation belongs to not the probability of each class. For instance, in fraud detection, banks use classifiers to distinguish between fraudulent and routine transactions. Many classification loss functions minimize classification errors and improve accuracy as this results in the best classification. A loss function like the Gini index is effective for classification problems but it is unclear if this applies to probability problems. In other words, minimizing misclassification error may not lead to accurate probability predictions.</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>At a high level, to classify an observation, $x_i$ as an $A$ or $B$, a model needs to determine if $\Pr(x_i=A)$ is less than or greater than $0.5$. If $\widehat{\Pr}(x_i=A) &gt; 0.5$, then it is more likely to be an $A$ and if $\widehat{\Pr}(x_i=A) &lt;0.5$ then it is more likely to be a $B$. Thus, if $x_i$ is an $A$ it is trivial if $\widehat{\Pr}(x_i=A)$ is $0.51$ or $0.99$ as this makes no difference to the classification as an $A$. But the difference between $\widehat{\Pr}(x_i=A) = 0.51$ and $0.99$ is extreme for a probability machine. It is important to understand that classification models are optimized for classification accuracy rather than probability prediction. This distinction affects the performance of ensemble methods like random forests or bagging ensembles that use classification trees for probability prediction.</span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bagging and Random Forest as Probability Machines {#sec-bagg-rf-probmachines}</span></span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a>::: {#nte-ensemble .callout-note title="A Quick Note on Ensemble Learning"}</span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a>Ensemble learning refers to a general framework of machine learning that combines together multiple simple models to create a better model. The philosophy behind ensemble learning is rooted in the wisdom of crowds principle, where the collective decision of multiple models often outperforms that of individual models. Often, ensemble learning methods use multiple CART models.</span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a>Bagging ensembles, random forest, and gradient boosting are all machine learning methods that combine many weaker models and so are all examples of ensemble learning. </span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a>Across an entire bagging ensemble <span class="co">[</span><span class="ot">see @Breiman1996</span><span class="co">]</span> or random forest <span class="co">[</span><span class="ot">see @Breiman2001</span><span class="co">]</span>, class probabilities are determined through a *vote count* method. Within that ensemble, each tree makes a class prediction based on the majority class in a terminal node. For instance, if $x_i$ lies in a terminal node where $80\%$ of the observations are classified as an $A$, that *individual tree* will classify $x_i$ as $A$. The ensemble's overall prediction for $x_i$ is derived from the proportion of trees that classify $x_i$ as $A$ or $B$. Thus the ensemble *counts votes* for each class across the ensemble. Let $T$ be the total number of trees and $b_t$ be the $t$-th tree in the ensemble. Let $\mathbb{I}(b_t(x_i) = A)$ be the indicator function that returns $1$ when $b_t$ predicts that observation $x_i$ belongs to class $A$. The probability of class $A$ for observation $x_i$ is calculated as:</span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a>\Pr(x_i = A) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(b_t(x_i) = A). </span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ensemble-vote-method}</span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a>@Olson2018 notes a potential bias towards predictions of $0$ or $1$ when trees in a bagged ensemble or random forest are highly correlated. Using a vote count method, an ensemble will bias predicted probabilities towards $\hat{P}(x_i=A) \in <span class="sc">\{</span>0,1<span class="sc">\}</span>$ when trees are highly correlated. Imagine that each tree in the ensemble is perfectly correlated implying that each tree will make the the same class prediction for each $x_i$. For such an ensemble, the predicted probabilities will will be exactly $0$ or $1$ using a vote count. Of course an ensemble of identical trees is unrealistic but the intuition still applies in the real world where ensembles may have some degree of correlation. The larger the correlation, the more the probability predictions will exhibit a *divergence bias* towards $0$ and $1$. Notably, divergence bias is not problematic in classification applications, as a larger number of trees correctly classifying the observation is encouraging.</span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>If $x_i$ has a known membership of $A$, and an unknown $\Pr_{\text{true}}(x_i=A) = 0.6$, the ensemble might classify $x_i$ correctly $90\%$ of the time leading to $\widehat{\Pr}(x_i=A) = 0.9$. As a probability machine, the ensemble has overestimated the probability by $0.3$ even though a $90\%$ classification accuracy is excellent. To predict $P_{\text{true}}(x_i=A) = 0.6$, an ensemble needs to incorrectly classify $x_i$ in $40\%$ of its trees. However, bagging ensembles and random forests are designed to maximize classification accuracy and there is no incentive for the model to intentionally achieve a specific misclassification rate that aligns with the true probability.</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>To reduce tree correlation, bagging ensembles use bootstrap aggregation and train each tree on a randomly selected bootstrapped sample of the data. Random forests further reduce tree correlation by considering a random number of variables at each split, commonly referred to as $mtry$ in software implementations. When $mtry$ is equal to to number of predictors, the model considers all variables at each split and the random forest is equal to a bagging ensemble. A lower $mtry$ should reduce the correlation between trees and decrease divergence bias as the structure of the tree is modified by the selected variables at each split. However, a lower $mtry$ also introduces other theoretical problems.</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a>Consider the scenario where the binary outcome (treatment and control) of the ensemble is strongly related to a single predictor and weakly related to other noisy predictors. If $mtry$ is low then each split may not consider the strong predictor and more commonly splits on weak or noisy predictors. Each predictor has a chance of $\frac{mtry}{\text{number of predictors}}$ of selection at each split implying a lower $mtry$ decreases the chance of a splitting on the strong predictor. Splits on the weak or noisy predictors may not result in a meaningful increase in node purity and successive splits may result in impure terminal nodes that poorly predict the class of $x_i$ in each tree. Such an ensemble may have highly unstable probability predictions. </span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a>Additionally, consider there is a class imbalance and the majority of observations are classified as $A$ not $B$. The terminal nodes of each tree within an ensemble are more likely to contain the majority class. Consequently, there is a *majority class bias* as each tree in the ensemble is more likely to misclassifying an observation as an $A$ because the terminal nodes have a higher proportion of $A$ due to the higher proportion of $A$'s in the data overall.</span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a>::: {#nte-class-imbalance .callout-note title="Class Imbalance and Machine Learning"}</span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a>When there is a difference in the number of observations in each class, this is called class imbalance. It is important to note that majority class bias exists in conventional classification tasks with machine learning. Bagging ensembles and random forest are well known to be sensitive to class imbalance meaning that class predictions are biased towards the majority <span class="co">[</span><span class="ot">see @Bader2019</span><span class="co">]</span>.</span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>However, the class imbalance problem is particularly notable when predicting probabilities. The probability prediction from a vote count method is very sensitive to a change in the votes from each tree. Suppose that balanced data results in $80/100$ trees classifying $B$ as $B$ and imbalanced data (more $A$ than $B$) reduces correct classifications of $B$ to $60/100$. This results in a $20\%$ margin of error in probability estimates but the classification remains as $B$.</span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>Individually, a low $mtry$ can lead to unstable probability predictions and class imbalance can create bias towards the majority class. But probability machines are particularly effected when there is both a low $mtry$ *and* class imbalance. Because successive noisy splits (relating to a low $mtry$) result in impure child nodes, the effects of majority class bias are exaggerated. Without the ability to separate the classes, the majority class will dominate terminal nodes. If the ensemble was able to split on informative covariates each time ($mtry$ is higher), then it should still be able to create pure splits even when there is some class imbalance. In other words, if there is a small class imbalance, reducing $mtry$ may reveal majority class bias not visible at higher $mtry$'s. Equally, if there is low $mtry$, then even a small class imbalance can lead to majority class bias.  </span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a>To exemplify these theoretical points, the National Supported Work (NSW) programme is a commonly discussed dataset in causal inference. The data results from a randomized controlled trial with $445$ total participants, $185$ in the program group, and $260$ in the control group, so the true probability of treatment for each individual can be calculated as $185/445=0.42$ or $42\%$. Thus, the "best" calibrated probability estimates will be close too $42\%$ for all observations. Further information about this data is found in @sec-data-nsw-jobs.</span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a>Randomisation should ensure that the probability of treatment is independent of the predictors and so all predictors should be noisy or weak. Although @fig-rf-varimp and @tbl-combined-btab suggest some covariates do have a greater impact on the probability of participating in the programme, which echoes research by @Smith2005 who suggests that self-selection bias is evident in the NSW data.</span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>@fig-rf-theory-demo shows both divergence bias and majority class bias using <span class="in">`randomForest()`</span> to fit both the random forest and bagging ensemble. Recall that a bagging ensemble is a random forest model when $mtry$ is equal to the number of predictors and so specifying <span class="in">`mtry = 7`</span> in the <span class="in">`randomForest()`</span> function fits a bagging ensemble. Additionally, logistic regression using the <span class="in">`gbm()`</span> function provides a meaningful comparison.</span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a><span class="in">```{r dev="ragg_png"}</span></span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a><span class="in">#| dev: "ragg_png"</span></span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-height: 8</span></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-rf-theory-demo</span></span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Show the Code to Create [Figure 2.1](fig-rf-theory-demo)" </span></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Compares the density estimates of the propensity scores for control and participant groups in the National Supported Work programme. `randomForest()` fits a random forest with `mtry = 1` and bagging ensemble with `mtry = 7`. The default values of `ntree = 500` and `nodesize = 1` are used. A logistic regression model is included for a comparison."</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a><span class="in">library(randomForest)</span></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a><span class="in">library(patchwork)</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a><span class="in">library(ggplot2)</span></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a><span class="in">library(ragg)</span></span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a><span class="in">theme_set(custom_ggplot_theme)</span></span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_formula &lt;- as.formula(as.factor(treat) ~ age + educ + re75 + </span></span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a><span class="in">                          black + hisp + degree + marr)</span></span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a><span class="in">logit_preds &lt;- glm(nsw_formula, data = nsw_data, </span></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a><span class="in">                   family = binomial())$fitted.values </span></span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a><span class="in">rf_mtry1_preds &lt;- predict(randomForest(nsw_formula, </span></span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a><span class="in">                          mtry = 1, data = nsw_data), </span></span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a><span class="in">                          newdata = nsw_data, type = "prob")[, 2]</span></span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-150"><a href="#cb19-150" aria-hidden="true" tabindex="-1"></a><span class="in">bagging_model &lt;- randomForest(nsw_formula, mtry = 7, importance = TRUE, </span></span>
<span id="cb19-151"><a href="#cb19-151" aria-hidden="true" tabindex="-1"></a><span class="in">                              data = nsw_data)</span></span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a><span class="in">bagged_preds &lt;- predict(bagging_model, newdata = nsw_data, type = "prob")[, 2]</span></span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a><span class="in">plot_pmachines &lt;- function(pscores, plot_subtitle) {</span></span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a><span class="in">  ggplot(nsw_data, aes(x = pscores, fill = factor(treat))) +</span></span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a><span class="in">    geom_density(alpha = 0.6, linewidth = 0.6) +</span></span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a><span class="in">    scale_fill_manual(values = c("#e5e5e5", "#2780e3"), </span></span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a><span class="in">                      labels = c("Control", "Participant")) +</span></span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a><span class="in">    labs(subtitle = plot_subtitle, x = "Propensity Scores", y = "Density", </span></span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a><span class="in">         fill = "Group:") +</span></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a><span class="in">    scale_x_continuous(expand = expansion(0), limits = c(0,1)) + </span></span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a><span class="in">    scale_y_continuous(expand = expansion(0), limits = c(0,10)) +</span></span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a><span class="in">    custom_ggplot_theme</span></span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a><span class="in">p1 &lt;- plot_pmachines(logit_preds, "Logistic Regression") + xlab(NULL) + </span></span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a><span class="in">  theme(legend.position = "none") + </span></span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "curve", x = 0.6, y = 5, xend = 0.42, yend = 0, </span></span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a><span class="in">           curvature = .3, arrow = arrow(length = unit(2, "mm"))) +</span></span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a><span class="in">  annotate(geom = "text", x = 0.6, y = 5, label = "True Probability", </span></span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a><span class="in">           hjust = "left", color = "#333333", size = 3)</span></span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a><span class="in">p2 &lt;- plot_pmachines(rf_mtry1_preds, "Random Forest (mtry = 1)") + xlab(NULL) + </span></span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a><span class="in">  theme(legend.position = "none")</span></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a><span class="in">p3 &lt;- plot_pmachines(bagged_preds, "Bagging (Bootstrap Aggregation)")</span></span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a><span class="in">p1 / p2 / p3 + plot_annotation(</span></span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a><span class="in">  title = "Density Plots of Propensity Scores for NSW Data", theme = custom_ggplot_theme)</span></span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| dev: "ragg_png"</span></span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-varimp</span></span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure 2.2](fig-rf-theory-demo)"</span></span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Compares the variable importance assigned to each variable from a bagging ensemble fitted on data from the National Supported Work programme. `randomForest()` fits a baggin ensemble with `mtry = 7` with default `ntree = 500` and `nodesize = 1`."</span></span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">importance</span>(bagging_model))</span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">vars =</span> <span class="fu">rownames</span>(imp), imp)</span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a>imp <span class="ot">&lt;-</span> imp[<span class="fu">order</span>(imp<span class="sc">$</span>MeanDecreaseGini),]</span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a>imp<span class="sc">$</span>vars <span class="ot">&lt;-</span> <span class="fu">factor</span>(imp<span class="sc">$</span>vars, <span class="at">levels =</span> <span class="fu">unique</span>(imp<span class="sc">$</span>vars))</span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a>imp <span class="sc">%&gt;%</span> </span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">matches</span>(<span class="st">"Mean"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> vars, <span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.8</span>, <span class="at">show.legend =</span> <span class="cn">TRUE</span>, </span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a>           <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="fl">0.8</span>), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="sc">~</span> <span class="fu">factor</span>(name, </span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a>                      <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"MeanDecreaseGini"</span>, <span class="st">"MeanDecreaseAccuracy"</span>)), </span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a>             <span class="at">scales =</span> <span class="st">"free_x"</span>) <span class="sc">+</span></span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#e5e5e5"</span>, <span class="st">"#2780e3"</span>)) <span class="sc">+</span></span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.04</span>))) <span class="sc">+</span></span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Variable Importance"</span>,</span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"% Decrease if Variable is Omitted from Model"</span>, <span class="at">y =</span> <span class="st">"Variable Name"</span></span>
<span id="cb19-213"><a href="#cb19-213" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> custom_ggplot_theme <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a>@fig-rf-theory-demo shows the logistic regression model has identified a central tendency and most propensities are between $0.25$ and $0.75$ which roughly aligns with the true probability. The bagging ensemble has clear evidence of divergence and the majority of predictions are outside $0.25$ and $0.75$ which is likely related to tree correlation. For the random forest with $mtry=1$, a significant number of the treatment and control observations are centred near the control area ($T=0$) with a wide range of other predictions. Recall that the control group is the majority class. Reducing $mtry$ from $7$ to $1$ reveals the majority class bias reinforcing the theoretical discussion that a combination of low $mtry$ and class imbalance is especially troubling. The models over predicts the majority class and has unstable predictions otherwise. Both random forest and bagging ensembles have performed poorly compared to the true probability of $0.42\%$. </span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a>The tuning of $mtry$ faces double jeopardy and is another important area of discussion in probability machines. The selection of $mtry$ is typically carried out in with a classification loss function such as accuracy or out-of-bag error. @Olson2018 compares tuning $mtry$ measured by classification accuracy and mean square error of known simulation probabilities and finds that the optimal value of $mtry$ for classification differs from probability prediction.<span class="ot">[^propensity-2]</span> In other words, if a grid search finds that $mtry=3$ is optimal for a classification task, this does not imply that $mtry=3$ is optimal for predicting probabilities so the tuning of $mtry$ is difficult. </span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-2]: </span>Note that tuning $mtry$ for the mean square of probability prediction is only possible by design of the simulation study and is not possible in applications, as the true probability is unknown.</span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a>Random forests and bagging ensembles seem to be troubled as probability machines but this does not mean that bagging and random forest cannot perform well. In various simulation studies, they perform excellently as discussed in @sec-mlps-sims. Perhaps the nature of the data is informative for the potential success of a random forest or bagging ensemble.</span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a>Heuristically, divergence bias and majority class bias will most affect a probability machine when there is considerable overlap of true probabilities between groups. Recall the meaning of *common support* and *overlap* from @sec-assumptions. If there is overlap and a central region of true probabilities, then the effects of divergence bias may be very pronounced. Similarly, common support may make it even harder to increase purity in child nodes, as the covariates will lack clear split points. When combined with weak predictors relating to a low $mtry$, the terminal nodes of each tree may be relatively impure leading to a majority class bias. Alternatively, if true probabilities exist near $0$ or $1$ and there is a clear separation of class, divergence bias may trivially effect probability estimation as the probabilities already exist in that region. If there is a clear separation of class, then weak predictors relating to a low $mtry$ may still create meaningful splits and pure terminal nodes. It is worth noting that propensity score methods require datasets with overlap to meet the assumptions required to determine causality.</span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Boosting Machines as Probability Machines {#sec-gbm}</span></span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- going to have to cut some of this out --&gt;</span></span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a>Moving beyond classification trees in random forests or bagging ensembles, @Friedman2001 introduced the *Gradient Boosting Machine* (GBM). A GBM sequentially constructs CART trees to correct errors made by previous trees. Employing a gradient descent process, each new tree is fit on the pseudo-residuals of the previous iteration. This means that with each iteration, the GBM takes a gradient step down the global loss function, incrementally minimizing the loss function to reach a minimum. The update rule for the model after each iteration can be expressed as:</span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a>\hat{p}_i^{(t)} = \hat{p}_i^{(t-1)} + \lambda \cdot b_t(x_i), </span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gbm-update}</span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-236"><a href="#cb19-236" aria-hidden="true" tabindex="-1"></a>where $\lambda$ is the learning rate, and $b_t(x_i)$ is the $b$-th regression tree fitted on the pseudo-residuals of the previous regression tree. In words, the current overall iteration $t$, is a combination of the previous model plus the current iteration scaled by a learning rate. </span>
<span id="cb19-237"><a href="#cb19-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-238"><a href="#cb19-238" aria-hidden="true" tabindex="-1"></a>A learning rate controls the contribution of each weak learner to the final model. By using a small learning rate, the machine learns slowly so that it can slowly descend the loss function. This allows for finer adjustments during the iterative process to better capture patterns in the data. </span>
<span id="cb19-239"><a href="#cb19-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a>GBMs can be generalized to many different applications by minimizing a different loss function which can be specified as any continuously differentiable function. For binary outcomes, a GBM employs multiple regression trees and a logistic function to transform regression predictions into probabilities. Specifically, the logistic function used is:</span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a>\hat{p}_i = \frac{1}{1 + \exp(-\text{model}(x_i))}.</span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gbm-logit}</span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a>This logistic function is the same as in logistic regression, so a GBM with a binary class is sometimes called boosted logistic regression. The ensemble aims to minimize the Bernoulli deviance, which is equivalent to maximizing the Bernoulli log-likelihood with logistic regression. Maximizing the log-likelihood ensures that the predicted probability distribution is as close as possible to the true probability distribution given the data. The full GBM model, $f_T(x)$ after $T$ iterations can be written as:</span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-249"><a href="#cb19-249" aria-hidden="true" tabindex="-1"></a>f_T(x_i) = b_1(x_i) + \lambda \sum_{t=1}^{T} b_t(x_i).</span>
<span id="cb19-250"><a href="#cb19-250" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gbm-model}</span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a>Inside a base tree, each split considers all variables and makes the most informative split to descend the loss function using gradient descent. GBMs utilize many weak learners, such as a regression tree with a single split called a regression stump. However, additional splits enable the model to capture interactions between terms, which may increase probability calibration in complex or high-dimensional datasets.</span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a>By outputting probability predictions and avoiding the flaws of vote methods in other ensemble techniques, as well as allowing a probability distribution-based loss function optimal for probability prediction, GBMs stand out as a highly effective probability machine. Since GBMs predict probabilities from a logistic function, they avoid problems associated with a vote count method. Additionally, there are no difficult parameters to tune, such as $mtry$ in a random forest. The implementation and workflow to fit a GBM for propensity scores is discussed in @sec-gbm-tune-workflow.</span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-256"><a href="#cb19-256" aria-hidden="true" tabindex="-1"></a>@fig-gbm-demo shows the propensity scores resulting from a GBM model using the <span class="in">`gbm, package`</span> on the NSW data provides. <span class="in">`gbm`</span> is a notable performance improvement to random forest and bagging shown in @fig-rf-theory-demo. Recall that a "better" model would predict probabilities near to $42\%$ as this is the treatment/control share in the randomised NSW data. </span>
<span id="cb19-257"><a href="#cb19-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a><span class="co">#| dev: "ragg_png"</span></span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-gbm-demo</span></span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-264"><a href="#cb19-264" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb19-265"><a href="#cb19-265" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show the Code to Create [Figure 2.3](fig-gbm-demo)"</span></span>
<span id="cb19-266"><a href="#cb19-266" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Density estimates of the propensity scores for control and participant groups in the National Supported Work programme using the `gbm` package with `distribution = \"bernoulli\"`, `data = nsw_data`, `n.trees = 10000`, and `shrinkage = 0.0001`."</span></span>
<span id="cb19-267"><a href="#cb19-267" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a>nsw_gbm <span class="ot">&lt;-</span> <span class="fu">gbm</span>(treat <span class="sc">~</span> age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> degree <span class="sc">+</span> </span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a>    marr, <span class="at">distribution =</span> <span class="st">"bernoulli"</span>, <span class="at">data =</span> nsw_data, <span class="at">n.trees =</span> <span class="dv">10000</span>, </span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a>    <span class="at">shrinkage =</span> <span class="fl">0.0001</span>)</span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-273"><a href="#cb19-273" aria-hidden="true" tabindex="-1"></a>boosted_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(nsw_gbm, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb19-274"><a href="#cb19-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_pmachines</span>(boosted_preds, <span class="st">"Gradient Boosting Machine with `gbm`"</span>) <span class="sc">+</span> </span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="dv">0</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">25</span>)) <span class="sc">+</span> </span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">xend =</span> <span class="fl">0.42</span>, <span class="at">yend =</span> <span class="dv">0</span>, </span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> .<span class="dv">3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="fl">0.6</span>, <span class="at">y =</span> <span class="dv">5</span>, <span class="at">label =</span> <span class="st">"True Probability"</span>, </span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">labs</span>(</span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">"Density Plots of Propensity Scores for NSW Data"</span>) <span class="sc">+</span> </span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a>  custom_ggplot_theme</span>
<span id="cb19-283"><a href="#cb19-283" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-284"><a href="#cb19-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-285"><a href="#cb19-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting</span></span>
<span id="cb19-286"><a href="#cb19-286" aria-hidden="true" tabindex="-1"></a>Overfitting is a common concern when fitting machine learning models, as models can capture noise and random variations in the training data. An overfit model will typically show excellent performance on the training data but will perform poorly on new, unseen data because it cannot generalise beyond the specific patterns of the training set. For instance, consider a machine learning algorithm used by a bank for fraud detection. In this scenario, an overfit model would struggle to classify transactions correctly as it has learned the noise and specific variation in the training data rather than the underlying patterns of fraud. Cross validation or test/train splitting can prevent overfitting to ensure a model can generalise to unseen data.</span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a>However, the model is not required to generalise a propensity score model as different datasets will have a different model. Instead, the emphasis of predicting propensity scores is to create balance in the data. A model is effective if it balances covariates between groups, even if it is overfit in a conventional sense.</span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a>::: {#nte-overfit-logistic .callout-note title="Overfitting in Logistic Regression"}</span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a>There is limited research on how overfitting a logistic regression model affects estimating treatment effects. In logistic regression, overfitting occurs when there are too many parameters and so the maximisation of the log-likelihood function is difficult because of noise. One study that investigates overfitting in this context is @Schuster2016, who suggest a general rule that the number of observations per parameter should be between 10 and 20. When overfitting occurs, the variance of the estimated treatment effect increases because noise amplifies the magnitude of the coefficients, resulting in a small bias towards $0$ or $1$ because of properties of the logit function. Specifically, when using (non-augmented) propensity score weighting, the estimate of the treatment effect will have high variance as propensity scores close to $0$ or $1$ receive artificially inflated weighting.</span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a>@Lee2010 simulates a comparison of machine learning methods for propensity score prediction and finds that an overfit CART model performs better than a pruned CART model in terms of balance and treatment effect estimation bias. While not conclusive, this suggests that conventionally overfit trees are appropriate and potentially beneficial for propensity score modelling.</span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a>If overfitting was to occur, this could be interpreted as balance between groups getting worse decreases with a higher model complexity. Although various software packages use a stopping rule to prevent this. As conventional advice states, creating balance should be the aim of estimating propensity scores.</span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparison of Machine Learning Algorithms: Simulation Results {#sec-mlps-sims}</span></span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a>A small body of simulation studies benchmarks probability machines for predicting propensity scores <span class="co">[</span><span class="ot">see @McCaffrey2004; @Setoguchi2008; @Lee2010; @Cannas2019; @Tu2019; @Goller2020; @Ferri2020</span><span class="co">]</span>. Although these studies tackle the same problem, differences in simulation design and model implementation lead to a diverse range of perspectives on this issue. This variety reflects the complexity of the propensity score prediction.</span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a>@Tu2019 compares logistic regression, boosting, bagging, and random forests across different sample sizes, conditions of linearity and additivity, and treatment effect strengths. Boosting achieves the lowest bias ATE estimate in most scenarios and the lowest mean square error in all scenarios. Bagging ensembles and random forests perform poorly in both ATE estimate bias and MSE. The author notes that poor performance in bagging ensembles is likely due to correlated trees in the ensemble, leading to divergence bias. Random forests perform significantly better than bagging but both methods performed worse than boosting or logistic regression.</span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a>Despite poor theoretical properties as a probability machine, @Lee2010 find that bagging results in the lowest standard error across many datasets.<span class="ot">[^propensity-3]</span> This result is not surprising given that the bagging ensembles are trained on bootstrapped datasets, leading to lower variance and standard error. Although, this advantage is not likely of practical interest given that the small performance gain in standard error is at the expense of a considerable increase of bias.</span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-305"><a href="#cb19-305" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-3]: </span>In this case, the standard error is the dispersion of the standardised mean difference (effect size) across 1000 simulated datasets.</span>
<span id="cb19-306"><a href="#cb19-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a>Additionally, @Lee2010 finds that logistic regression performs well in simple data structures with comparable bias to boosting and random forest, but with larger standard errors. In complex data structures, boosting shows low bias and outperforms logistic regression while maintaining low standard errors. Consequently, the study concludes that boosted CART achieves the best $95\%$ coverage in all simulation scenarios, with $98.6\%$ coverage.<span class="ot">[^propensity-4]</span></span>
<span id="cb19-308"><a href="#cb19-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-309"><a href="#cb19-309" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-4]: </span>In this context, the coverage is the proportion of times that the true treatment effect is within the $95\%$ confidence interval across the number of simulations. This author implements $1000$ simulations of each scenario.</span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a>@Cannas2019 also undergo a simulation study to assess machine learning methods for propensity score prediction. They compare logistic regression, CART, bagging ensembles, random forest, boosting, neural networks, and naive bayes and find that random forest, neural networks, and logistic regression perform the best. Notably, the simulation design only performs hyperparameter tuning for CART, random forest, and neural networks but not either of their boosting implementation. <span class="ot">[^propensity-5]</span> This is a weakness of their study design and thus their findings may be more informative of the relative performance of tuned versus untuned models. Although, the finding that random forest performs well when tuned is significant.</span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-5]: </span>@Cannas2019 provide a replication package for their simulation study online and their hyperparameter tuning is process transparent. The authors fit two GBMs using the <span class="in">`twang`</span> and <span class="in">`gbm`</span> package in R. The hyperparameter values provided to these untuned boosting models are contrary to heuristics and may lead boosting to perform poorly regardless of theoretical benefits discussed in @sec-gbm.</span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a>@Goller2020 adds diversity to the simulation study literature by exploring an economics context, experimenting with imbalances between treated and control observations, and incorporating LASSO and probit models.<span class="ot">[^propensity-6]</span> <span class="ot">[^lasso]</span> Probit regression achieves the best covariate balance, with LASSO also performing well. In contrast, the random forest model performs poorly, showing imbalance statistics with several orders of magnitude higher than those of probit or LASSO. To perform feature selection, a probit model with many interactions and polynomial terms is specified, and a LASSO penalty shrinks covariate coefficients to zero. Probit regression stands out for its superior covariate balance, while LASSO also delivers satisfactory results. The random forest model underperforms with significantly higher imbalance statistics compared to probit and LASSO.</span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a><span class="ot">[^propensity-6]: </span>@Goller2020 calculates the bias of the treatment effect using the average of the estimates from logistic regression, random forest, and LASSO models as the *true* treatment effect. Thus, the covariate balance table offers a clearer view of each method's performance.</span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a><span class="ot">[^lasso]: </span>@Tibshirani1996 introduces LASSO regularization, short for Least Absolute Shrinkage and Selection Operator, is a technique used in linear regression to prevent overfitting by penalising the absolute size of the coefficients. It adds a penalty term to the ordinary least squares objective function, meaning that some coefficients may "shrink" to zero.</span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a>Based on a review of the literature, the findings can be distilled into five important points:</span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Probability machines can predict propensity scores with excellent performance and their implementation should be considered in most scenarios. Although, a logistic regression approach may be preferred because of simplicity while still providing adequate performance in simple data structures.</span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>In cases of non-linearity or non-additivity in the data, probability machines often achieve better covariate balance and lower bias of treatment effect estimates than logistic regression. This is significant as propensity scores are frequently used in observational studies with complex data structures <span class="co">[</span><span class="ot">@Rosenbaum1983</span><span class="co">]</span>.</span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Bagging ensembles perform poorly, a finding replicated across multiple studies.</span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Random forests can perform excellently when hyperparameters are satisfactorily tuned.</span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Further research should consider parametric methods with LASSO, Ridge, or Elastic Net penalties to assist in feature selection. Simulation study evidence for predicting propensity scores is limited despite attractive properties of these methods.</span>
<span id="cb19-332"><a href="#cb19-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-333"><a href="#cb19-333" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>A tuned GBM stands out with strong theoretical support, excellent simulation performance, and superior software implementation and documentation. Specifically, this GBM will use the Bernoulli deviance as a loss function due to theoretical benefits. Implementations of GBMs such as AdaBoost.M1 have no simulation study evidence.</span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>A good practical approach seems to be a trial-and-error approach of fitting multiple model specifications, then considering covariate balance for each model.</span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a>Short concluding remark</span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a><span class="fu">save.image</span>(<span class="at">file =</span> <span class="st">"globals.RData"</span>)</span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mitchellcameron123/ML-PS.git/edit/main/chapters/pscore_theory.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>