<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.52">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Tutorial: Implimentation, Workflow, and Example with WeightIt andgbm in R – Machine Learning and the Propensity Score</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/coffee_replication.html" rel="next">
<link href="../chapters/pscore_theory.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script src="../site_libs/kePrint-0.0.1/kePrint.js"></script>

<link href="../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="[[3]{.chapter-number}&nbsp; [Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R]{.chapter-title}]{#sec-demo .quarto-section-identifier}">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=An introduction to propensity score methods for reducing the effects of confounding in observational studies;,citation_abstract=The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propen-sity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.  Taylor &amp;amp;amp; Francis Group, LLC.;,citation_author=Peter Austin;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=3;,citation_doi=10.1080/00273171.2011.568786;,citation_issn=00273171;,citation_pmid=21818162;,citation_volume=46;,citation_journal_title=Multivariate Behavioral Research;">
<meta name="citation_reference" content="citation_title=The Changing Risk and Burden of Wildfire in the Us;,citation_abstract=Recent dramatic and deadly increases in global wildfire activity have increased attention on the causes of wildfires, their consequences, and how risk from fire might be mitigated. Here we bring together data on the changing risk and societal burden of wildfire in the US. We estimate that nearly 50 million homes are currently in the wildland-urban interface in the US, a number increasing by 1 million houses every 3 years. Using a statistical model that links satellite-based fire and smoke data to pollution monitoring stations, we estimate that wildfires have accounted for up to 25% of PM2.5 in recent years across the US, and up to half in some Western regions. We then show that ambient exposure to smoke-based PM2.5 does not follow traditional socioeconomic exposure gradients. Finally, using stylized scenarios, we show that fuels management interventions have large but uncertain impacts on health outcomes, and that future health impacts from climate-change-induced wildfire smoke could approach projected overall increases in temperature-related mortality from climate change. We draw lessons for research and policy.;,citation_author=Marshall Burke;,citation_author=Anne Driscoll;,citation_author=Jenny Xue;,citation_author=Sam Heft-Neal;,citation_author=Jennifer Burney;,citation_author=Michael W. Wara;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2139/ssrn.3637724;,citation_journal_title=SSRN Electronic Journal;">
<meta name="citation_reference" content="citation_title=A tutorial on propensity score estimation for multiple treatments using generalized boosted models;,citation_abstract=The use of propensity scores to control for pretreatment imbalances on observed variables in non-randomized or observational studies examining the causal effects of treatments or interventions has become widespread over the past decade. For settings with two conditions of interest such as a treatment and a control, inverse probability of treatment weighted estimation with propensity scores estimated via boosted models has been shown in simulation studies to yield causal effect estimates with desirable properties. There are tools (e.g., the twang package in R) and guidance for implementing this method with two treatments. However, there is not such guidance for analyses of three or more treatments. The goals of this paper are twofold: (1) to provide step-by-step guidance for researchers who want to implement propensity score weighting for multiple treatments and (2) to propose the use of generalized boosted models (GBM) for estimation of the necessary propensity score weights. We define the causal quantities that may be of interest to studies of multiple treatments and derive weighted estimators of those quantities. We present a detailed plan for using GBM to estimate propensity scores and using those scores to estimate weights and causal effects. We also provide tools for assessing balance and overlap of pretreatment variables among treatment groups in the context of multiple treatments. A case study examining the effects of three treatment programs for adolescent substance abuse demonstrates the methods.  2013 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Daniel F. Mccaffrey;,citation_author=Beth Ann Griffin;,citation_author=Daniel Almirall;,citation_author=Mary Ellen Slaughter;,citation_author=Rajeev Ramchand;,citation_author=Lane F. Burgette;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=19;,citation_doi=10.1002/sim.5753;,citation_issn=02776715;,citation_pmid=23508673;,citation_volume=32;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=A blueprint for synthetic control methodology: A causal inference tool for evaluating natural experiments in population health;,citation_author=Ben Barr;,citation_author=Xingna Zhang;,citation_author=Mark Green;,citation_author=Iain Buchan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_doi=10.1136/bmj.o2712;,citation_isbn=2021054101;,citation_issn=17561833;,citation_pmid=36418028;,citation_volume=379;,citation_journal_title=Bmj;">
<meta name="citation_reference" content="citation_title=Wildfire Smoke and COVID-19;,citation_author=Natural Disasters and Severe Weather;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://www.cdc.gov/disasters/covid-19/wildfire{\_}smoke{\_}covid-19.html;">
<meta name="citation_reference" content="citation_title=Making Sense of Random Forest Probabilities: a Kernel Perspective;,citation_abstract=A random forest is a popular tool for estimating probabilities in machine learning classification tasks. However, the means by which this is accomplished is unprincipled: one simply counts the fraction of trees in a forest that vote for a certain class. In this paper, we forge a connection between random forests and kernel regression. This places random forest probability estimation on more sound statistical footing. As part of our investigation, we develop a model for the proximity kernel and relate it to the geometry and sparsity of the estimation problem. We also provide intuition and recommendations for tuning a random forest to improve its probability estimates.;,citation_author=Matthew A. Olson;,citation_author=Abraham J. Wyner;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=http://arxiv.org/abs/1812.05792;">
<meta name="citation_reference" content="citation_title=Applied Causal Inference Powered by ML and AI;,citation_abstract=An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.;,citation_author=Victor Chernozhukov;,citation_author=Christian Hansen;,citation_author=Nathan Kallus;,citation_author=Martin Spindler;,citation_author=Syrgkanis Vasilis;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;">
<meta name="citation_reference" content="citation_title=A local generalized method of moments estimator;,citation_abstract=A local Generalized Method of Moments Estimator is proposed for nonparametrically estimating unknown functions that are defined by conditional moment restrictions.  2006 Elsevier B.V. All rights reserved.;,citation_author=Arthur Lewbel;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=1;,citation_doi=10.1016/j.econlet.2006.08.011;,citation_issn=01651765;,citation_volume=94;,citation_journal_title=Economics Letters;">
<meta name="citation_reference" content="citation_title=7 Instrumental Variables Causal Inference :;,citation_author=The Mixtape;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Causal Forest Estimation of Heterogeneous Household Response to Time-Of-Use Electricity Pricing Schemes;,citation_author=Eoghan O Neill;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/arXiv:1810.09179v3;">
<meta name="citation_reference" content="citation_title=Large sample properties of matching estimators for average treatment effects;,citation_abstract=Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N 1/2-consistent in general and describe conditions under which matching estimators do attain N 1/2-consistency. Second, we show that even in settings where matching estimators are N 1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.  The Econometric Society 2006.;,citation_author=Alberto Abadie;,citation_author=Guido W. Imbens;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=1;,citation_doi=10.1111/j.1468-0262.2006.00655.x;,citation_issn=00129682;,citation_volume=74;,citation_journal_title=Econometrica;">
<meta name="citation_reference" content="citation_title=Cross-validated:What is the difference between the G-formula , G- estmation , G-computation and G-methods;,citation_author=James Robins;">
<meta name="citation_reference" content="citation_title=The impact of coffee certification on small-scale producers’ livelihoods: A case study from the Jimma Zone, Ethiopia;,citation_abstract=What is the impact of product certification on small-scale farmers’ livelihoods? To what extent does the participation of Ethiopian small-scale coffee farmers in certified local cooperative structures improve their socioeconomic situation? To answer these questions, this article employs household data of 249 coffee farmers from six different cooperatives collected in the Jimma zone of Southwestern Ethiopia in 2009. Findings show that the certification of coffee cooperatives has in total a low impact on small-scale coffee producers’ livelihoods mainly due to (1) low productivity, (2) insignificant price premium, and (3) poor access to credit and information from the cooperative. Differences in production and organizational capacities between the local cooperatives are mirrored in the extent of the certification benefits for the smallholders. &amp;amp;amp;quot;Good&amp;quot; cooperatives have reaped the benefits of certification, whereas &quot;bad&quot; ones did not fare well. In this regard the &quot;cooperative effect&quot; overlies the &quot;certification effect&quot;.  2012 International Association of Agricultural Economists.;,citation_author=Pradyot Ranjan Jena;,citation_author=Bezawit Beyene Chichaibelu;,citation_author=Till Stellmacher;,citation_author=Ulrike Grote;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=4;,citation_doi=10.1111/j.1574-0862.2012.00594.x;,citation_issn=01695150;,citation_volume=43;,citation_journal_title=Agricultural Economics (United Kingdom);">
<meta name="citation_reference" content="citation_title=Variable selection for propensity score models;,citation_abstract=Despite the growing popularity of propensity score (PS) methods in epidemiology, relatively little has been written in the epidemiologic literature about the problem of variable selection for PS models. The authors present the results of two simulation studies designed to help epidemiologists gain insight into the variable selection problem in a PS analysis. The simulation studies illustrate how the choice of variables that are included in a PS model can affect the bias, variance, and mean squared error of an estimated exposure effect. The results suggest that variables that are unrelated to the exposure but related to the outcome should always be included in a PS model. The inclusion of these variables will decrease the variance of an estimated exposure effect without increasing bias. In contrast, including variables that are related to the exposure but not to the outcome will increase the variance of the estimated exposure effect without decreasing bias. In very small studies, the inclusion of variables that are strongly related to the exposure but only weakly related to the outcome can be detrimental to an estimate in a mean squared error sense. The addition of these variables removes only a small amount of bias but can increase the variance of the estimated exposure effect. These simulation studies and other analytical results suggest that standard model-building tools designed to create good predictive models of the exposure will not always lead to optimal PS models, particularly in small studies. Copyright  2006 by the Johns Hopkins Bloomberg School of Public Health All rights reserved.;,citation_author=M. Alan Brookhart;,citation_author=Sebastian Schneeweiss;,citation_author=Kenneth J. Rothman;,citation_author=Robert J. Glynn;,citation_author=Jerry Avorn;,citation_author=Til Stürmer;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=12;,citation_doi=10.1093/aje/kwj149;,citation_issn=00029262;,citation_pmid=16624967;,citation_volume=163;,citation_journal_title=American Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=STATS 361: Causal Inference;,citation_author=Stefan Wager;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=Residuals and Influence in Regression;,citation_author=Sanford Weisberg;,citation_author=R D Cook;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;">
<meta name="citation_reference" content="citation_title=[ On the Application of Probability Theory to Agricultural Experiments . Essay on Principles . Section 9 .] Comment : Neyman ( 1923 ) and Causal Inference in Experiments and Observational Studies Author ( s ): Donald B . Rubin Source : Statistical Science;,citation_author=Donald B. Rubin;,citation_publication_date=1923;,citation_cover_date=1923;,citation_year=1923;">
<meta name="citation_reference" content="citation_title=Springer Texts in Statistics An Introduction to Statistical Learning wth application in R;,citation_abstract=An Introduction to Statistical Learning provides an accessible overview of the fi eld of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fi elds ranging from biology to fi nance to marketing to astrophysics in the past twenty years. Th is book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classifi cation, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in sci-ence, industry, and other fi elds, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical soft ware platform.;,citation_author=Gareth James;,citation_author=undefined DanielaWitten;,citation_author=Trevor Hastie;,citation_author=Robert Tibshirani;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_isbn=9781461471370;,citation_issn=01621459;,citation_pmid=10911016;">
<meta name="citation_reference" content="citation_title=Different worlds Confirmatory versus exploratory research;,citation_abstract=Simon Schwab and Leonhard Held explain the differences between confirmatory and exploratory research and the dangers of confusing the two concepts.;,citation_author=Simon Schwab;,citation_author=Leonhard Held;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=2;,citation_doi=10.1111/1740-9713.01369;,citation_issn=17409713;,citation_volume=17;,citation_journal_title=Significance;">
<meta name="citation_reference" content="citation_title=Does matching overcome LaLonde’s critique of nonexperimental estimators?;,citation_abstract=This paper applies cross-sectional and longitudinal propensity score matching estimators to data from the National Supported Work (NSW) Demonstration that have been previously analyzed by LaLonde (1986) and Dehejia and Wahba (1999, 2002). We find that estimates of the impact of NSW based on propensity score matching are highly sensitive to both the set of variables included in the scores and the particular analysis sample used in the estimation. Among the estimators we study, the difference-in-differences matching estimator performs the best. We attribute its performance to the fact that it eliminates potential sources of temporally invariant bias present in the NSW data, such as geographic mismatch between participants and nonparticipants and the use of a dependent variable measured in different ways for the two groups. Our analysis demonstrates that while propensity score matching is a potentially useful econometric tool, it does not represent a general solution to the evaluation problem.  2004 Elsevier B.V. All rights reserved.;,citation_author=Jeffrey A. Smith;,citation_author=Petra E. Todd;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_doi=10.1016/j.jeconom.2004.04.011;,citation_isbn=1301405353;,citation_issn=03044076;,citation_volume=125;">
<meta name="citation_reference" content="citation_title=American Economic Association Evaluating the Econometric Evaluations of Training Programs with Experimental Data Author ( s ): Robert J . LaLonde Source : The American Economic Review , Vol . 76 , No . 4 ( Sep ., 1986 ), pp . 604-620 Published by : Americ;,citation_abstract=This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric proce- dures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.;,citation_author=Robert J. LaLonde;,citation_publication_date=1986;,citation_cover_date=1986;,citation_year=1986;,citation_issue=4;,citation_volume=76;,citation_journal_title=The American Economic Review;">
<meta name="citation_reference" content="citation_title=Causal Inference in the Presence of Interference in Sponsored Search Advertising;,citation_abstract=In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the likelihood of a user clicking on a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.;,citation_author=Razieh Nabi;,citation_author=Joel Pfeiffer;,citation_author=Denis Charles;,citation_author=Emre Kıcıman;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2010.07458;,citation_doi=10.3389/fdata.2022.888592;,citation_issn=2624909X;,citation_volume=5;,citation_journal_title=Frontiers in Big Data;">
<meta name="citation_reference" content="citation_title=Excess of COVID-19 cases and deaths due to fine particulate matter exposure during the 2020 wildfires in the United States;,citation_abstract=The year 2020 brought unimaginable challenges in public health, with the confluence of the COVID-19 pandemic and wildfires across the western United States. Wildfires produce high levels of fine particulate matter (PM2.5). Recent studies reported that short-term exposure to PM2.5 is associated with increased risk of COVID-19 cases and deaths. We acquired and linked publicly available daily data on PM2.5, the number of COVID-19 cases and deaths, and other confounders for 92 western U.S. counties that were affected by the 2020 wildfires. We estimated the association between short-term exposure to PM2.5 during the wildfires and the epidemiological dynamics of COVID-19 cases and deaths. We adjusted for several time-varying confounding factors (e.g., weather, seasonality, long-term trends, mobility, and population size). We found strong evidence that wildfires amplified the effect of short-term exposure to PM2.5 on COVID-19 cases and deaths, although with substantial heterogeneity across counties.;,citation_author=Xiaodan Zhou;,citation_author=Kevin Josey;,citation_author=Leila Kamareddine;,citation_author=Miah C. Caine;,citation_author=Tianjia Liu;,citation_author=Loretta J. Mickley;,citation_author=Matthew Cooper;,citation_author=Francesca Dominici;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=33;,citation_doi=10.1126/sciadv.abi8789;,citation_issn=23752548;,citation_pmid=34389545;,citation_volume=7;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=Biased Random Forest for Dealing with the Class Imbalance Problem;,citation_abstract=The class imbalance issue has been a persistent problem in machine learning that hinders the accurate predictive analysis of data in many real-world applications. The class imbalance problem exists when the number of instances present in a class (or classes) is significantly fewer than the number of instances belonging to another class (or classes). Sufficiently recognizing the minority class during classification is a problem as most algorithms employed to learn from data input are biased toward the majority class. The underlying issue is made more complex with the presence of data difficult factors embedded in such data input. This paper presents a novel and effective ensemble-based method for dealing with the class imbalance problem. This paper is motivated by the idea of moving the oversampling from the data level to the algorithm level, instead of increasing the minority instances in the data sets, the algorithms in this paper aims to ’oversample the classification ensemble’ by increasing the number of classifiers that represent the minority class in the ensemble, i.e., random forest. The proposed biased random forest algorithm employs the nearest neighbor algorithm to identify the critical areas in a given data set. The standard random forest is then fed with more random trees generated based on the critical areas. The results show that the proposed algorithm is very effective in dealing with the class imbalance problem.;,citation_author=Mohammed Bader-El-Den;,citation_author=Eleman Teitei;,citation_author=Todd Perry;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=7;,citation_doi=10.1109/TNNLS.2018.2878400;,citation_issn=21622388;,citation_pmid=30475733;,citation_volume=30;,citation_journal_title=IEEE Transactions on Neural Networks and Learning Systems;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=Causal inference;,citation_abstract=We have examined only a few of the basic questions about causal inference that result from Reichenbach’s two principles. We have not considered what happens when the probability distribution is a mixture of distributions from different causal structures, or how unmeasured common causes can be detected, or what inferences can reliably be drawn about causal relations among unmeasured variables, or the exact advantages that experimental control offers. A good deal is known about these questions, and there is a good deal more to find out.  1991 Kluwer Academic Publishers.;,citation_author=undefined Mixtape;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_doi=10.1007/BF00388284;,citation_issn=01650106;,citation_volume=35;">
<meta name="citation_reference" content="citation_title=Improving propensity score weighting using machine learning;,citation_abstract=Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART-based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non-linear and non-additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non-linearity or non-additivity alone. However, under conditions of both moderate non-additivity and moderate non-linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright  2009 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Brian K. Lee;,citation_author=Justin Lessler;,citation_author=Elizabeth A. Stuart;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_doi=10.1002/sim.3782;,citation_issn=02776715;,citation_pmid=19960510;,citation_volume=29;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=Causal Inference: What If (1st ed.);,citation_author=A. Miguel Hernan;,citation_author=M. James Robins;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.1201/9781315374932;,citation_volume=[1] A. M.;">
<meta name="citation_reference" content="citation_title=Measuring the effectiveness of protected area networks in reducing deforestation;,citation_abstract=Global efforts to reduce tropical deforestation rely heavily on the establishment of protected areas. Measuring the effectiveness of these areas is difficult because the amount of deforestation that would have occurred in the absence of legal protection cannot be directly observed. Conventional methods of evaluating the effectiveness of protected areas can be biased because protection is not randomly assigned and because protection can induce deforestation spillovers (displacement) to neighboring forests. We demonstrate that estimates of effectiveness can be substantially improved by controlling for biases along dimensions that are observable, measuring spatial spillovers, and testing the sensitivity of estimates to potential hidden biases. We apply matching methods to evaluate the impact on deforestation of Costa Rica’s renowned protected-area system between 1960 and 1997. We find that protection reduced deforestation: approximately 10% of the protected forests would have been deforested had they not been protected. Conventional approaches to evaluating conservation impact, which fail to control for observable covariates correlated with both protection and deforestation, substantially overestimate avoided deforestation (by over 65%, based on our estimates). We also find that deforestation spillovers from protected to unprotected forests are negligible. Our conclusions are robust to potential hidden bias, as well as to changes in modeling assumptions. Our results show that, with appropriate empirical methods, conservation scientists and policy makers can better understand the relationships between human and natural systems and can use this to guide their attempts to protect critical ecosystem services.  2008 by The National Academy of Sciences of the USA.;,citation_author=Kwaw S. Andam;,citation_author=Paul J. Ferraro;,citation_author=Alexander Pfaff;,citation_author=G. Arturo Sanchez-Azofeifa;,citation_author=Juan A. Robalino;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=42;,citation_doi=10.1073/pnas.0800437105;,citation_issn=00278424;,citation_pmid=18854414;,citation_volume=105;,citation_journal_title=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=Classification and Regression Trees;,citation_author=L Breiman;,citation_author=Jerome H Friedman;,citation_author=Richard A Olshen;,citation_author=C J Stone;,citation_publication_date=1984;,citation_cover_date=1984;,citation_year=1984;,citation_fulltext_html_url=https://api.semanticscholar.org/CorpusID:29458883;,citation_volume=40;,citation_journal_title=Biometrics;">
<meta name="citation_reference" content="citation_title=Climate drivers of global wildfire burned area;,citation_abstract=Wildfire is an integral part of the Earth system, but at the same time it can pose serious threats to human society and to certain types of terrestrial ecosystems. Meteorological conditions are a key driver of wildfire activity and extent, which led to the emergence of the use of fire danger indices that depend solely on weather conditions. The Canadian Fire Weather Index (FWI) is a widely used fire danger index of this kind. Here, we evaluate how well the FWI, its components, and the climate variables from which it is derived, correlate with observation-based burned area (BA) for a variety of world regions. We use a novel technique, according to which monthly BA are grouped by size for each Global Fire Emissions Database (GFED) pyrographic region. We find strong correlations of BA anomalies with the FWI anomalies, as well as with the underlying deviations from their climatologies for the four climate variables from which FWI is estimated, namely, temperature, relative humidity, precipitation, and wind. We quantify the relative sensitivity of the observed BA to each of the four climate variables, finding that this relationship strongly depends on the pyrographic region and land type. Our results indicate that the BA anomalies strongly correlate with FWI anomalies at a GFED region scale, compared to the strength of the correlation with individual climate variables. Additionally, among the individual climate variables that comprise the FWI, relative humidity and temperature are the most influential factors that affect the observed BA. Our results support the use of the composite fire danger index FWI, as well as its sub-indices, the Build-Up Index (BUI) and the Initial Spread Index (ISI), comparing to single climate variables, since they are found to correlate better with the observed forest or non-forest BA, for the most regions across the globe.;,citation_author=Manolis Grillakis;,citation_author=Apostolos Voulgarakis;,citation_author=Anastasios Rovithakis;,citation_author=Konstantinos D. Seiradakis;,citation_author=Aristeidis Koutroulis;,citation_author=Robert D. Field;,citation_author=Matthew Kasoar;,citation_author=Athanasios Papadopoulos;,citation_author=Mihalis Lazaridis;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_doi=10.1088/1748-9326/ac5fa1;,citation_issn=17489326;,citation_volume=17;,citation_journal_title=Environmental Research Letters;">
<meta name="citation_reference" content="citation_title=Evaluating uses of data mining techniques in propensity score estimation: a simulation study;,citation_author=Soko Setoguchi;,citation_author=Sebastian Schneeweiss;,citation_author=Alan M. Brookhart;,citation_author=Robert J. Glynn;,citation_author=Francis E. Cook;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=March;,citation_doi=10.1002/pds;,citation_issn=1053-8569;,citation_volume=17;,citation_journal_title=pharmacoepidemiology and drug safety;">
<meta name="citation_reference" content="citation_title=Credibility of propensity score matching estimates. An example from Fair Trade certification of coffee producers;,citation_abstract=Propensity score matching (PSM) is an increasingly popular method for evaluation studies in agricultural and development economics. However, statisticians and econometricians have stressed that results rely on untestable assumptions, and therefore, guidelines for researchers on how to improve credibility have been developed. We follow one of these guidelines with a data set analysed by other authors to evaluate the impact of Fair Trade certification on the income of coffee producers. We provide thereby a best practice example of how to evaluate the credibility of PSM estimates. We find that a thorough assessment of the assumptions made renders the data we use not suitable for a credible PSM estimation of the effects of treatment. We conclude that the debate about the impact of Fair Trade certification would greatly benefit from a detailed reporting of credibility checking.;,citation_author=Nicolas Lampach;,citation_author=Ulrich B. Morawetz;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=44;,citation_doi=10.1080/00036846.2016.1153795;,citation_issn=14664283;,citation_volume=48;,citation_journal_title=Applied Economics;">
<meta name="citation_reference" content="citation_title=Some practical guidance for the implementation of propensity score matching;,citation_abstract=Propensity score matching (PSM) has become a popular approach to estimate causal treatment effects. It is widely applied when evaluating labour market policies, but empirical examples can be found in very diverse fields of study. Once the researcher has decided to use PSM, he is confronted with a lot of questions regarding its implementation. To begin with, a first decision has to be made concerning the estimation of the propensity score. Following that one has to decide which matching algorithm to choose and determine the region of common support. Subsequently, the matching quality has to be assessed and treatment effects and their standard errors have to be estimated. Furthermore, questions like ’what to do if there is choice-based sampling?’ or ’when to measure effects?’ can be important in empirical studies. Finally, one might also want to test the sensitivity of estimated treatment effects with respect to unobserved heterogeneity or failure of the common support condition. Each implementation step involves a lot of decisions and different approaches can be thought of. The aim of this paper is to discuss these implementation issues and give some guidance to researchers who want to use PSM for evaluation purposes.  2008 The Authors Journal compilation  2008 Blackwell Publishing Ltd.;,citation_author=Marco Caliendo;,citation_author=Sabine Kopeinig;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=1;,citation_doi=10.1111/j.1467-6419.2007.00527.x;,citation_issn=09500804;,citation_volume=22;,citation_journal_title=Journal of Economic Surveys;">
<meta name="citation_reference" content="citation_title=Understanding Inverse Probability of Treatment Weighting (IPTW) in Causal Inference An Intuitive Explanation of IPTW and a Comparison to Multivariate Regression;,citation_author=Jonah Breslow;,citation_author=undefined Follow;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;">
<meta name="citation_reference" content="citation_title=Metalearners for estimating heterogeneous treatment effects using machine learning;,citation_abstract=There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.;,citation_author=Sören R. Künzel;,citation_author=Jasjeet S. Sekhon;,citation_author=Peter J. Bickel;,citation_author=Bin Yu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03461;,citation_issue=10;,citation_doi=10.1073/pnas.1804597116;,citation_issn=10916490;,citation_pmid=30770453;,citation_volume=116;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Propensity score model overfitting led to inflated variance of estimated odds ratios;,citation_abstract=Objective Simulation studies suggest that the ratio of the number of events to the number of estimated parameters in a logistic regression model should be not less than 10 or 20 to 1 to achieve reliable effect estimates. Applications of propensity score approaches for confounding control in practice, however, do often not consider these recommendations. Study Design and Setting We conducted extensive Monte Carlo and plasmode simulation studies to investigate the impact of propensity score model overfitting on the performance in estimating conditional and marginal odds ratios using different established propensity score inference approaches. We assessed estimate accuracy and precision as well as associated type I error and type II error rates in testing the null hypothesis of no exposure effect. Results For all inference approaches considered, our simulation study revealed considerably inflated standard errors of effect estimates when using overfitted propensity score models. Overfitting did not considerably affect type I error rates for most inference approaches. However, because of residual confounding, estimation performance and type I error probabilities were unsatisfactory when using propensity score quintile adjustment. Conclusion Overfitting of propensity score models should be avoided to obtain reliable estimates of treatment or exposure effects in individual studies.;,citation_author=Tibor Schuster;,citation_author=Wilfrid Kouokam Lowe;,citation_author=Robert W. Platt;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.jclinepi.2016.05.017;,citation_doi=10.1016/j.jclinepi.2016.05.017;,citation_issn=18785921;,citation_pmid=27498378;,citation_volume=80;,citation_journal_title=Journal of Clinical Epidemiology;,citation_publisher=Elsevier Inc;">
<meta name="citation_reference" content="citation_title=What Makes Forest-Based Heterogeneous Treatment Effect Estimators Work?;,citation_abstract=Estimation of heterogeneous treatment effects (HTE) is of prime importance in many disciplines, from personalized medicine to economics among many others. Random forests have been shown to be a flexible and powerful approach to HTE estimation in both randomized trials and observational studies. In particular “causal forests” introduced by Athey, Tibshirani and Wager (Ann. Statist. 47 (2019) 1148–1178), along with the R implementation in package grf were rapidly adopted. A related approach, called “model-based forests” that is geared toward randomized trials and simultaneously captures effects of both prognostic and predictive variables, was introduced by Seibold, Zeileis and Hothorn (Stat. Methods Med. Res. 27 (2018) 3104–3125) along with a modular implementation in the R package model4you. Neither procedure is directly applicable to the estimation of individualized predictions of excess postpartum blood loss caused by a cesarean section in comparison to vaginal delivery. Clearly, randomization is hardly possible in this setup, and thus model-based forests lack clinical trial data to address this question. On the other hand, the skewed and interval-censored postpartum blood loss observations violate assumptions made by causal forests. Here we present a tailored model-based forest for skewed and interval-censored data to infer possible predictive prepartum characteristics and their impact on excess postpartum blood loss caused by a cesarean section. As a methodological basis, we propose a unifying view on causal and model-based forests that goes beyond the theoretical motivations and investigates which computational elements make causal forests so successful and how these can be blended with the strengths of model-based forests. To do so, we show that both methods can be understood in terms of the same parameters and model assumptions for an additive model under L2 loss. This theoretical insight allows us to implement several flavors of “model-based causal forests” and dissect their different elements in silico. The original causal forests and model-based forests are compared with the new blended versions in a benchmark study exploring both randomized trials and observational settings. In the randomized setting, both approaches performed akin. If confounding was present in the data-generating process, we found local centering of the treatment indicator with the corresponding propensities to be the main driver for good performance. Local centering of the outcome was less important and might be replaced or enhanced by simultaneous split selection with respect to both prognostic and predictive effects. This lays the foundation for future research combining random forests for HTE estimation with other types of models.;,citation_author=Y. Susanne Dandl;,citation_author=Christian Haslinger;,citation_author=Torsten Hothorn;,citation_author=Heidi Seibold;,citation_author=Erik Sverdrup;,citation_author=Stefan Wager;,citation_author=Achim Zeileis;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://arxiv.org/abs/2206.10323;,citation_issue=1;,citation_doi=10.1214/23-AOAS1799;,citation_issn=19417330;,citation_volume=18;,citation_journal_title=Annals of Applied Statistics;">
<meta name="citation_reference" content="citation_title=Root-N-Consistent Semiparametric Regression;,citation_author=P.m. Robinson;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;,citation_fulltext_html_url=https://www.jstor.org/stable/1912705;,citation_issue=4;,citation_volume=56;,citation_journal_title=The Econometric Socieity;">
<meta name="citation_reference" content="citation_title=Estimating Treatment Effects with Causal Forests: An Application;,citation_abstract=We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and discusses resulting practical and conceptual challenges. This note will appear in an upcoming issue of Observational Studies, Empirical Investigation of Methods for Heterogeneity, that compiles several analyses of the same dataset.;,citation_author=Susan Athey;,citation_author=Stefan Wager;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1902.07409;,citation_issue=2;,citation_doi=10.1353/obs.2019.0001;,citation_issn=27673324;,citation_volume=5;,citation_journal_title=Observational Studies;">
<meta name="citation_reference" content="citation_title=Machine Learning Methods for Estimating Heterogeneous Causal Effect;,citation_abstract=In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit’s attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit’s outcome. The challenge is that the “ground truth” for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine. Keywords:;,citation_author=Susan Athey;,citation_author=Guido Imbens;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://arxiv.org/abs/1504.01132;">
<meta name="citation_reference" content="citation_title=Causal inference for climate change events from satellite image time series using computer vision and deep learning;,citation_abstract=We propose a method for causal inference using satellite image time series, in order to determine the treatment effects of interventions which impact climate change, such as deforestation. Simply put, the aim is to quantify the ’before versus after’ effect of climate related human driven interventions, such as urbanization; as well as natural disasters, such as hurricanes and forest fires. As a concrete example, we focus on quantifying forest tree cover change/ deforestation due to human led causes. The proposed method involves the following steps. First, we uae computer vision and machine learning/deep learning techniques to detect and quantify forest tree coverage levels over time, at every time epoch. We then look at this time series to identify changepoints. Next, we estimate the expected (forest tree cover) values using a Bayesian structural causal model and projecting/forecasting the counterfactual. This is compared to the values actually observed post intervention, and the difference in the two values gives us the effect of the intervention (as compared to the non intervention scenario, i.e. what would have possibly happened without the intervention). As a specific use case, we analyze deforestation levels before and after the hyperinflation event (intervention) in Brazil (which ended in 1993-94), for the Amazon rainforest region, around Rondonia, Brazil. For this deforestation use case, using our causal inference framework can help causally attribute change/reduction in forest tree cover and increasing deforestation rates due to human activities at various points in time.;,citation_author=Vikas Ramachandra;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1910.11492;">
<meta name="citation_reference" content="citation_title=A Primer for Applying Propensity-Score Matching;,citation_abstract=The use of microeconometric techniques to estimate the effects of development policies has become a common approach not only for scholars, but also for policy-makers engaged in designing, implementing and evaluating projects in different fields. Among these techniques, Propensity-Score Matching (PSM) is increasingly applied in the policy evaluation community. This technical note provides a guide to the key aspects of implementing PSM methodology for an audience of practitioners interested in understanding its applicability to specific evaluation problems. The note summarizes the basic conditions under which PSM can be used to estimate the impact of a program and the data required. It explains how the Conditional Independence Assumption, combined with the Overlap Condition, reduces selection bias when participation in a program is determined by observable characteristics. It also describes different matching algorithms and some tests to assess the quality of the matching. Case studies are used throughout to illustrate important concepts in impact evaluation and PSM. In the annexes, the note provides an outline of the main technical aspects and a list of statistical and econometric software for implementing PSM.;,citation_author=Carolyn Heinrich;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_fulltext_html_url=http://www.iadb.org/document.cfm?id=35320229;,citation_issue=August;,citation_issn={\textless}null{\textgreater};,citation_journal_title=Development;">
<meta name="citation_reference" content="citation_title=Quasi-oracle estimation of heterogeneous treatment effects;,citation_abstract=Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.;,citation_author=X. Nie;,citation_author=S. Wager;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/1712.04912;,citation_issue=2;,citation_doi=10.1093/biomet/asaa076;,citation_issn=14643510;,citation_volume=108;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Increasing trends in high-severity fire in the southwestern USA from 1984 to 2015;,citation_abstract=In the last three decades, over 4.1 million hectares have burned in Arizona and New Mexico and the largest fires in documented history have occurred in the past two decades. Changes in burn severity over time, however, have not been well documented in forest and woodland ecosystems in the southwestern US. Using remotely sensed burn severity data from 1621 fires (&amp;amp;amp;gt;404 ha), we assessed trends from 1984 to 2015 in Arizona and New Mexico in (1) number of fires and total area burned in all vegetation types; (2) area burned, area of high-severity, and percent of high-severity fire in all forest and woodland areas; and (3) area burned, area of high-severity, and percent of high-severity in seven different grouped forest and woodland vegetation types (Ecological Response Unit [ERU] Fire Regime Types). Number of fires and area burned increased across the Southwest regardless of vegetation type. The significant increasing trends held for area burned, area of high-severity, and percent of high-severity fire in all forest and woodland ecosystems. Area burned and area burned severely increased in all seven ERU Fire Regime Types while percent of high-severity fire increased in two ERUs: Mixed Conifer Frequent Fire and Mixed Conifer with Aspen/Spruce Fir. Managers must face the implications of increasing, uncharacteristic high-severity fire in many ecosystems as climate change and human pressures continue to affect fire regimes.;,citation_author=Megan P. Singleton;,citation_author=Andrea E. Thode;,citation_author=Andrew J. Sánchez Meador;,citation_author=Jose M. Iniguez;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1016/j.foreco.2018.11.039;,citation_issue=November 2018;,citation_doi=10.1016/j.foreco.2018.11.039;,citation_issn=03781127;,citation_volume=433;,citation_journal_title=Forest Ecology and Management;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Causal Rule Ensemble: Interpretable Discovery and Inference of Heterogeneous Treatment Effects;,citation_abstract=In health and social sciences, it is critically important to identify subgroups of the study population where a treatment has notable heterogeneity in the causal effects with respect to the average treatment effect. Data-driven discovery of heterogeneous treatment effects (HTE) via decision tree methods has been proposed for this task. Despite its high interpretability, the single-tree discovery of HTE tends to be highly unstable and to find an oversimplified representation of treatment heterogeneity. To accommodate these shortcomings, we propose Causal Rule Ensemble (CRE), a new method to discover heterogeneous subgroups through an ensemble-of-trees approach. CRE has the following features: 1) provides an interpretable representation of the HTE; 2) allows extensive exploration of complex heterogeneity patterns; and 3) guarantees high stability in the discovery. The discovered subgroups are defined in terms of interpretable decision rules, and we develop a general two-stage approach for subgroup-specific conditional causal effects estimation, providing theoretical guarantees. Via simulations, we show that the CRE method has a strong discovery ability and a competitive estimation performance when compared to state-of-the-art techniques. Finally, we apply CRE to discover subgroups most vulnerable to the effects of exposure to air pollution on mortality for 35.3 million Medicare beneficiaries across the contiguous U.S.;,citation_author=Falco J. Bargagli-Stoffi;,citation_author=Riccardo Cadei;,citation_author=Kwonsang Lee;,citation_author=Francesca Dominici;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=http://arxiv.org/abs/2009.09036;">
<meta name="citation_reference" content="citation_title=An introduction to g methods;,citation_abstract=Robins’ generalized methods (g methods) provide consistent estimates of contrasts (e.g. differences, ratios) of potential outcomes under a less restrictive set of identification conditions than do standard regression methods (e.g. linear, logistic, Cox regression). Uptake of g methods by epidemiologists has been hampered by limitations in understanding both conceptual and technical details. We present a simple worked example that illustrates basic concepts, while minimizing technical complications.;,citation_author=Ashley I. Naimi;,citation_author=Stephen R. Cole;,citation_author=Edward H. Kennedy;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=2;,citation_doi=10.1093/ije/dyw323;,citation_issn=14643685;,citation_pmid=28039382;,citation_volume=46;,citation_journal_title=International Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=Overfitting in propensity score model: a commentary on “propensity score model overfitting led to inflated variance of estimated odds ratios” by Schuster et&nbsp;al.;,citation_author=David Hajage;,citation_author=Florence Tubach;,citation_author=Yann De Rycke;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.jclinepi.2017.05.011;,citation_doi=10.1016/j.jclinepi.2017.05.011;,citation_issn=18785921;,citation_pmid=28549930;,citation_volume=88;,citation_journal_title=Journal of Clinical Epidemiology;,citation_publisher=Elsevier Inc.;">
<meta name="citation_reference" content="citation_title=Regression Shrinkage and Selection via the Lasso;,citation_abstract=We propose a new method for estimation in linear models. The ’lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.;,citation_author=Robert Tibshirani;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://www.jstor.org/stable/pdf/2346178.pdf?refreqid=fastly-default{\%}3Aff57285d6b8854126d21a135984fc4ca{\&amp;amp;amp;}ab{\_}segments={\&amp;}origin={\&amp;}initiator={\&amp;}acceptTC=1;,citation_issue=1;,citation_volume=58;,citation_journal_title=Journal of the Royal Statistical Society;">
<meta name="citation_reference" content="citation_title=Policy Learning With Observational Data;,citation_abstract=In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application‐specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.;,citation_author=Susan Athey;,citation_author=Stefan Wager;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/1702.02896;,citation_issue=1;,citation_doi=10.3982/ecta15732;,citation_issn=0012-9682;,citation_volume=89;,citation_journal_title=Econometrica;">
<meta name="citation_reference" content="citation_title=Propensity score adjustment using machine learning classification algorithms to control selection bias in online surveys;,citation_abstract=Modern survey methods may be subject to non-observable bias, from various sources. Among online surveys, for example, selection bias is prevalent, due to the sampling mechanism commonly used, whereby participants self-select from a subgroup whose characteristics differ from those of the target population. Several techniques have been proposed to tackle this issue. One such is Propensity Score Adjustment (PSA), which is widely used and has been analysed in various studies. The usual method of estimating the propensity score is logistic regression, which requires a reference probability sample in addition to the online nonprobability sample. The predicted propensities can be used for reweighting using various estimators. However, in the online survey context, there are alternatives that might outperform logistic regression regarding propensity estimation. The aim of the present study is to determine the efficiency of some of these alternatives, involving Machine Learning (ML) classification algorithms. PSA is applied in two simulation scenarios, representing situations commonly found in online surveys, using logistic regression and ML models for propensity estimation. The results obtained show that ML algorithms remove selection bias more effectively than logistic regression when used for PSA, but that their efficacy depends largely on the selection mechanism employed and the dimensionality of the data.;,citation_author=Ramón Ferri-García;,citation_author=María Del Mar Rueda;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=4;,citation_doi=10.1371/journal.pone.0231500;,citation_isbn=1111111111;,citation_issn=19326203;,citation_pmid=32320429;,citation_volume=15;,citation_journal_title=PLoS ONE;">
<meta name="citation_reference" content="citation_title=The changing risk and burden of wildfire in the United States;,citation_abstract=Recent dramatic and deadly increases in global wildfire activity have increased attention on the causes of wildfires, their consequences, and how risk from wildfire might be mitigated. Here we bring together data on the changing risk and societal burden of wildfire in the United States. We estimate that nearly 50 million homes are currently in the wildland-urban interface in the United States, a number increasing by 1 million houses every 3 y. To illustrate how changes in wildfire activity might affect air pollution and related health outcomes, and how these linkages might guide future science and policy, we develop a statistical model that relates satellite-based fire and smoke data to information from pollution monitoring stations. Using the model, we estimate that wildfires have accounted for up to 25% of PM2.5 (particulate matter with diameter &amp;amp;amp;lt;2.5 $\mu$m) in recent years across the United States, and up to half in some Western regions, with spatial patterns in ambient smoke exposure that do not follow traditional socioeconomic pollution exposure gradients. We combine the model with stylized scenarios to show that fuel management interventions could have large health benefits and that future health impacts from climate-change-induced wildfire smoke could approach projected overall increases in temperature-related mortality from climate change-but that both estimates remain uncertain. We use model results to highlight important areas for future research and to draw lessons for policy.;,citation_author=Marshall Burke;,citation_author=Anne Driscoll;,citation_author=Sam Heft-Neal;,citation_author=Jiani Xue;,citation_author=Jennifer Burney;,citation_author=Michael Wara;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1073/PNAS.2011048118;,citation_issn=10916490;,citation_pmid=33431571;,citation_volume=118;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Comparison of various machine learning algorithms for estimating generalized propensity score;,citation_abstract=In this paper, we conducted a simulation study to evaluate the performance of four algorithms: multinomial logistic regression (MLR), bagging (BAG), random forest (RF), and gradient boosting (GB), for estimating generalized propensity score (GPS). Similar to the propensity score (PS), the ultimate goal of using GPS is to estimate unbiased average treatment effects (ATEs) in observational studies. We used the GPS estimates computed from these four algorithms with the generalized doubly robust (GDR) estimator to estimate ATEs in observational studies. We evaluated these ATE estimates in terms of bias and mean squared error (MSE). Simulation results show that overall, the GB algorithm produced the best ATE estimates based on these evaluation criteria. Thus, we recommend using the GB algorithm for estimating GPS in practice.;,citation_author=Chunhao Tu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1080/00949655.2019.1571059;,citation_issue=4;,citation_doi=10.1080/00949655.2019.1571059;,citation_issn=15635163;,citation_volume=89;,citation_journal_title=Journal of Statistical Computation and Simulation;">
<meta name="citation_reference" content="citation_title=Springer Series in Statistics The Elements of Statistical Learning;,citation_abstract=During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.;,citation_author=Trevor Hastie;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_fulltext_html_url=http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf;,citation_issue=2;,citation_isbn=9780387848570;,citation_issn=03436993;,citation_pmid=15512507;,citation_volume=27;,citation_journal_title=The Mathematical Intelligencer;">
<meta name="citation_reference" content="citation_title=Causal Inference : An Introduction;,citation_abstract=Lately, the concept of causality has been gaining popularity in the domain of machine learning and artificial intelligence due to its…;,citation_author=Siddhant Haldar;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://medium.com/analytics-vidhya/causal-inference-an-introduction-f424df7c76ef{\%}0Ahttp://files/135/causal-inference-an-introduction-f424df7c76ef.html;,citation_journal_title=Medium;">
<meta name="citation_reference" content="citation_title=Member-only story Understanding Causal Trees How to use regression trees to estimate heterogeneous treatment effects;,citation_abstract=Listen Share More Cover, image by Author causal inference, we are usually interested in estimating the causal effect of a treatment (a drug, ad, product, …) on an outcome of interest (a disease, firm revenue, customer satisfaction, …). However, knowing that a This member-only story is on us. Upgrade to access all of Medium.;,citation_author=Matteo Courthoud;,citation_author=undefined Follow;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;">
<meta name="citation_reference" content="citation_title=Directed Acyclic Graphs;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2307/j.ctv1c29t27.6;,citation_inbook_title=Causal inference: The mixtape;">
<meta name="citation_reference" content="citation_title=An Introduction to Recursive Partitioning for Heterogeneous Causal Effects Estimation Using causalTree package;,citation_author=Susan Athey;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;">
<meta name="citation_reference" content="citation_title=On the application of probability theory to agricultural experiments. Essay on principles. Section 9;,citation_abstract=In the portion of the paper translated here, Neyman introduces a model for the analysis of field experiments conducted for the purpose of comparing a number of crop varieties, which makes use of a double-indexed array of unknown potential yields, one index corresponding to varieties and the other to plots. The yield corresponding to only one variety will be observed on any given plot, but through an urn model embodying sampling without replacement from this doubly indexed array, Neyman obtains a formula for the variance of the difference between the averages of the observed yields of two varieties. This variance involves the variance over all plots of the potential yields and the correlation coefficient r between the potential yields of the two varieties on the same plot. Since it is impossible to estimate r directly, Neyman advises taking r = 1, observing that in practice this may lead to using too large an estimated standard deviation, when comparing two variety means.  1990, Institute of Mathematical Statistics. All Rights Reserved.;,citation_author=Jerzy Splawa-Neyman;,citation_publication_date=1923;,citation_cover_date=1923;,citation_year=1923;,citation_fulltext_html_url=https://www.mimuw.edu.pl/{~}noble/courses/BayesianNetworks/90NeymanTranslation.pdf;,citation_doi=10.1214/ss/1177012031;,citation_issn=08834237;">
<meta name="citation_reference" content="citation_title=Matching and Subclassification;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=10.2307/j.ctv1c29t27.8;,citation_inbook_title=Causal inference: The mixtape;">
<meta name="citation_reference" content="citation_title=Generalized random forests;,citation_abstract=We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.;,citation_author=Susan Athey;,citation_author=Julie Tibshirani;,citation_author=Stefan Wager;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1610.01271;,citation_issue=2;,citation_doi=10.1214/18-AOS1709;,citation_issn=00905364;,citation_volume=47;,citation_journal_title=Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Machine Learning in Policy Evaluation: New Tools for Causal Inference;,citation_abstract=While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, pre-occupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the &amp;amp;amp;quot;fundamental problem of causal inference&amp;quot;) prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the $\backslash$textit{unconfoundedness} and positivity assumptions (also known as exchangeability and overlap assumptions). This article reviews popular supervised machine learning algorithms, including the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g. the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g. targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates.;,citation_author=Noémi Kreif;,citation_author=Karla DiazOrdaz;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1903.00402;,citation_doi=10.1093/acrefore/9780190625979.013.256;,citation_journal_title=Oxford Research Encyclopedia of Economics and Finance;">
<meta name="citation_reference" content="citation_title=The central role of the propensity score in observational studies for causal effects;,citation_abstract=The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.;,citation_author=Paul R. Rosenbaum;,citation_author=Donald B. Rubin;,citation_publication_date=1983;,citation_cover_date=1983;,citation_year=1983;,citation_issue=1;,citation_doi=10.1017/CBO9780511810725.016;,citation_isbn=9780511810725;,citation_volume=70;,citation_journal_title=Biometrika;">
<meta name="citation_reference" content="citation_title=Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package;,citation_abstract=The Toolkit for Weighting and Analysis of Nonequivalent Groups , twang, contains a set of functions and procedures to support causal modeling of observational data through the estimation and evaluation of propensity scores and associated weights. This package was ... $\backslash$n;,citation_author=Greg Ridgeway;,citation_author=Dan Mccaffrey;,citation_author=Andrew Morral;,citation_author=Matthew Cefalu;,citation_author=Lane Burgette;,citation_author=Beth Ann Griffin;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_doi=10.7249/tl136.1;">
<meta name="citation_reference" content="citation_title=Estimating individual treatment effect: Generalization bounds and algorithms;,citation_abstract=There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms leam a &amp;amp;amp;quot;balanced&amp;quot; representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.;,citation_author=Uri Shalit;,citation_author=Fredrik D. Johansson;,citation_author=David Sontag;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1606.03976;">
<meta name="citation_reference" content="citation_title=Causal inference in statistics: An overview;,citation_abstract=This review presents empirical researcherswith recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called &amp;amp;amp;quot;causal effects&amp;quot; or &quot;policy evaluation&quot;) (2) queries about probabilities of counterfactuals, (including assessment of &quot;regret,&quot; &quot;attribution&quot; or &quot;causes of effects&quot;) and (3) queries about direct and indirect effects (also known as &quot;mediation&quot;). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.;,citation_author=Judea Pearl;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_doi=10.1214/09-SS057;,citation_issn=19357516;,citation_volume=3;">
<meta name="citation_reference" content="citation_title=Excess of COVID-19 cases and deaths due to fine particulate matter exposure during the 2020 wildfires in the United States;,citation_abstract=The year 2020 brought unimaginable challenges in public health, with the confluence of the COVID-19 pandemic and wildfires across the western United States. Wildfires produce high levels of fine particulate matter (PM2.5). Recent studies reported that short-term exposure to PM2.5 is associated with increased risk of COVID-19 cases and deaths. We acquired and linked publicly available daily data on PM2.5, the number of COVID-19 cases and deaths, and other confounders for 92 western U.S. counties that were affected by the 2020 wildfires. We estimated the association between short-term exposure to PM2.5 during the wildfires and the epidemiological dynamics of COVID-19 cases and deaths. We adjusted for several time-varying confounding factors (e.g., weather, seasonality, long-term trends, mobility, and population size). We found strong evidence that wildfires amplified the effect of short-term exposure to PM2.5 on COVID-19 cases and deaths, although with substantial heterogeneity across counties.;,citation_author=Xiaodan Zhou;,citation_author=Kevin Josey;,citation_author=Leila Kamareddine;,citation_author=Miah C. Caine;,citation_author=Tianjia Liu;,citation_author=Loretta J. Mickley;,citation_author=Matthew Cooper;,citation_author=Francesca Dominici;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=33;,citation_doi=10.1126/sciadv.abi8789;,citation_issn=23752548;,citation_pmid=34389545;,citation_volume=7;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003;,citation_abstract=Propensity-score methods are increasingly being used to reduce the impact of treatment-selection bias in the estimation of treatment effects using observational data. Commonly used propensity-score methods include covariate adjustment using the propensity score, stratification on the propensity score, and propensityscore matching. Empirical and theoretical research has demonstrated that matching on the propensity score eliminates a greater proportion of baseline differences between treated and untreated subjects than does stratification on the propensity score. However, the analysis of propensity-score-matched samples requires statistical methods appropriate for matched-pairs data. We critically evaluated 47 articles that were published between 1996 and 2003 in the medical literature and that employed propensity-score matching. We found that only two of the articles reported the balance of baseline characteristics between treated and untreated subjects in the matched sample and used correct statistical methods to assess the degree of imbalance. Thirteen (28 per cent) of the articles explicitly used statistical methods appropriate for the analysis of matched data when estimating the treatment effect and its statistical significance. Common errors included using the log-rank test to compare Kaplan–Meier survival curves in the matched sample, using Cox regression, logistic regression, chi-squared tests, t-tests, and Wilcoxon rank sum tests in the matched sample, thereby failing to account for the matched nature of the data. We provide guidelines for the analysis and reporting of studies that employ propensity-score matching.;,citation_author=Peter C. Austin;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=April;,citation_doi=10.1002/sim.3150;,citation_volume=27;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=The contribution of wildfire to PM2.5 trends in the USA;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.unep.org/resources/report/spreading-wildfire-rising-threat-extraordinary-landscape-fires?gad{\_}source=1{\&amp;amp;amp;}gclid=CjwKCAjwwr6wBhBcEiwAfMEQs4akEBSfMrNFii0aVgYlS-DOo-zlKl{\_}yb8q0JC5zm5LzDeC{\_}eM6YQxoCPSMQAvD{\_}BwE;">
<meta name="citation_reference" content="citation_title=A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting;,citation_abstract=Propensity score matching (PSM) and propensity score weighting (PSW) are popular tools to estimate causal effects in observational studies. We address two open issues: how to estimate propensity scores and assess covariate balance. Using simulations, we compare the performance of PSM and PSW based on logistic regression and machine learning algorithms (CART; Bagging; Boosting; Random Forest; Neural Networks; naive Bayes). Additionally, we consider several measures of covariate balance (Absolute Standardized Average Mean (ASAM) with and without interactions; measures based on the quantile-quantile plots; ratio between variances of propensity scores; area under the curve (AUC)) and assess their ability in predicting the bias of PSM and PSW estimators. We also investigate the importance of tuning of machine learning parameters in the context of propensity score methods. Two simulation designs are employed. In the first, the generating processes are inspired to birth register data used to assess the effect of labor induction on the occurrence of caesarean section. The second exploits more general generating mechanisms. Overall, among the different techniques, random forests performed the best, especially in PSW. Logistic regression and neural networks also showed an excellent performance similar to that of random forests. As for covariate balance, the simplest and commonly used metric, the ASAM, showed a strong correlation with the bias of causal effects estimators. Our findings suggest that researchers should aim at obtaining an ASAM lower than 10% for as many variables as possible. In the empirical study we found that labor induction had a small and not statistically significant impact on caesarean section.;,citation_author=Massimo Cannas;,citation_author=Bruno Arpino;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=4;,citation_doi=10.1002/bimj.201800132;,citation_issn=15214036;,citation_pmid=31090108;,citation_volume=61;,citation_journal_title=Biometrical Journal;">
<meta name="citation_reference" content="citation_title=Causal Inference : 4 Potential Outcomes Causal Model;,citation_author=Scott Cunningham;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_doi=https://doi.org/10.2307/j.ctv1c29t27;,citation_isbn=9780300255881;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Methamphetamine use and HIV risk behavior among men who inject drugs: Causal inference using coarsened exact matching;,citation_abstract=Background: Understanding the association between methamphetamine (MA) use and HIV risk behavior among people who inject drugs (PWID) will assist policy-makers and program managers to sharpen the focus of HIV prevention interventions. This study examines the relationship between MA use and HIV risk behavior among men who inject drugs (MWID) in Tehran, Iran, using coarsened exact matching (CEM). Methods: Data for these analyses were derived from a cross-sectional study conducted between June and July 2016. We assessed three outcomes of interest - all treated as binary variables, including distributive and receptive needle and syringe (NS) sharing and condomless sex during the month before interview. Our primary exposure of interest was whether study participants reported any MA use in the month prior to the interview. Firstly, we report the descriptive statistics for the pooled samples and matched sub-samples using CEM. The pooled and matched estimates of the associations and their 95% CI were estimated using a logistic regression model. Results: Overall, 500 MWID aged between 18 and 63 years (mean = 28.44, SD = 7.22) were recruited. Imbalances in the measured demographic characteristics and risk behaviors between MA users and non-users were attenuated using matching. In the matched samples, the regression models showed participants who reported MA use were 1.82 times more likely to report condomless sex (OR = 1.82 95% CI 1.51, 4.10; P = 0.031), and 1.35 times more likely to report distributive NS sharing in the past 30 days, as compared to MA non-users (OR = 1.35 95% CI 1.15-1.81). Finally, there was a statistically significant relationship between MA use and receptive NS sharing in the past month. People who use MA in the last month had higher odds of receptive NS sharing when compared to MA non-users (OR = 4.2 95% CI 2.7, 7.5; P = 0.013). Conclusions: Our results show a significant relationship between MA use and HIV risk behavior among MWID in Tehran, Iran. MA use was related with increased NS sharing, which is associated with higher risk for HIV exposure and transmission.;,citation_author=Mehdi Noroozi;,citation_author=Peter Higgs;,citation_author=Alireza Noroozi;,citation_author=Bahram Armoon;,citation_author=Bentolhoda Mousavi;,citation_author=Rosa Alikhani;,citation_author=Mohammad Rafi Bazrafshan;,citation_author=Ali Nazeri Astaneh;,citation_author=Azadeh Bayani;,citation_author=Ladan Fattah Moghaddam;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=66;,citation_doi=10.1186/s12954-020-00411-1;,citation_issn=14777517;,citation_pmid=32957982;,citation_volume=17;,citation_journal_title=Harm Reduction Journal;,citation_publisher=Harm Reduction Journal;">
<meta name="citation_reference" content="citation_title=Model-Based Direct Ad justment;,citation_abstract=Direct adjustment or standardization applies population weights to sub- class means in an effort to estimate population quantities from a sample that is not representative of the population. Direct adjustment has several attractive features, but when there are many subclasses it can attach large weights to small quantities of data, often in a fairly erratic manner. In the extreme, direct adjustment can attach infinite weight to nonexistent data, a noticeable inconvenience in practice. This article proposes a method of model-based direct adjustment that preserves the attractive features of conventional direct adjustment while stabilizing the weights attached to small subclasses. The sample mean and conventional direct adjustment are both special cases of model-based direct adjustment under two different extreme models for the subclass-specific selection proba- bilities. The discussion of this method provides some insights into the behavior of true and estimated propensity scores: the estimated scores are better than the true ones for almost the same reason that direct adjustment can outperform the sample mean in a simple random sample. The method is applied to a nonrandom sample in an effort to estimate a discrete distribution of essay scores in the College Board’s Advanced Placement Examination in Biology.;,citation_author=Paul R Rosenbaum;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_issue=398;,citation_volume=82;">
<meta name="citation_reference" content="citation_title=Causal Inference: What If;,citation_author=A. Miguel Hernan;,citation_author=M. James Robins;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1201/9781315374932;,citation_isbn=9781315374932;">
<meta name="citation_reference" content="citation_title=Causal inference using regression on the treatment variable;,citation_abstract=9.1 Causal inference and predictive comparisons So far, we have been interpreting regressions predictively: given the values of several inputs, the fitted model allows us to predict y, considering the n data points as a simple random sample from a hypothetical infinite &amp;amp;amp;quot; superpopulation &amp;quot; or probability distribution. Then we can make comparisons across different combinations of values for these inputs. This chapter and the next consider causal inference, which concerns what would happen to an outcome y as a result of a hypothesized &quot; treatment &quot; or intervention. In a regression framework, the treatment can be written as a variable T : 1 T i = 1 if unit i receives the &quot; treatment &quot; 0 if unit i receives the &quot; control, &quot; or, for a continuous treatment, T i = level of the &quot; treatment &quot; assigned to unit i. In the usual regression context, predictive inference relates to comparisons between units, whereas causal inference addresses comparisons of different treatments if applied to the same units. More generally, causal inference can be viewed as a special case of prediction in which the goal is to predict what would have happened under different treatment options. We shall discuss this theoretical framework more thoroughly in Section 9.2. Causal interpretations of regression coefficients can only be justified by relying on much stricter assumptions than are needed for predictive inference. To motivate the detailed study of regression models for causal effects, we present two simple examples in which predictive comparisons do not yield appropriate causal inferences. Hypothetical example of zero causal effect but positive predictive comparison;,citation_author=Andrew Gelman;,citation_author=Jennifer Hill;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_doi=10.1017/cbo9780511790942.012;,citation_journal_title=Data Analysis Using Regression and Multilevel/Hierarchical Models;">
<meta name="citation_reference" content="citation_title=Random Forests;,citation_abstract=Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same The distribution for all trees in the forest. generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.;,citation_author=Leo Brieman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_doi=https://doi.org/10.1023/A:1010933404324;,citation_issn=2158107X;,citation_volume=45;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=The health effects of ambient PM2.5 and potential mechanisms;,citation_abstract=The impacts of ambient PM2.5 on public health have become great concerns worldwide, especially in the developing countries. Epidemiological and toxicological studies have shown that PM2.5 does not only induce cardiopulmonary disorders and/or impairments, but also contributes to a variety of other adverse health effects, such as driving the initiation and progression of diabetes mellitus and eliciting adverse birth outcomes. Of note, recent findings have demonstrated that PM2.5 may still pose a hazard to public health even at very low levels (far below national standards) of exposure. The proposed underlying mechanisms whereby PM2.5 causes adverse effects to public health include inducing intracellular oxidative stress, mutagenicity/genotoxicity and inflammatory responses. The present review aims to provide an brief overview of new insights into the molecular mechanisms linking ambient PM2.5 exposure and health effects, which were explored with new technologies in recent years.;,citation_author=Shaolong Feng;,citation_author=Dan Gao;,citation_author=Fen Liao;,citation_author=Furong Zhou;,citation_author=Xinming Wang;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1016/j.ecoenv.2016.01.030;,citation_doi=10.1016/j.ecoenv.2016.01.030;,citation_issn=10902414;,citation_pmid=26896893;,citation_volume=128;,citation_journal_title=Ecotoxicology and Environmental Safety;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Do production contracts raise farm productivity? an instrumental variables approach;,citation_abstract=Estimating how the use of production contracts affects farm productivity is difficult when unobservable factors are correlated with both the decision to contract and productivity. To account for potential selection bias, this study uses the local availability of production contracts as an instrument for whether a farm uses a contract in order to estimate the impact of contract use on total factor productivity. Results indicate that use of a production contract is associated with a large increase in productivity for feeder-to-finish hog farms in the United States. The instrumental variable method makes it credible to assert that the observed association is a causal relationship rather than simply a correlation. Copyright 2008 Northeastern Agricultural and Resource Economics Association.;,citation_author=Nigel Key;,citation_author=William D. McBride;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=2;,citation_doi=10.1017/S1068280500002987;,citation_issn=10682805;,citation_volume=37;,citation_journal_title=Agricultural and Resource Economics Review;">
<meta name="citation_reference" content="citation_title=Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania;,citation_abstract=On April 1, 1992, New Jersey’s minimum wage rose from $4.25 to $5.05 per hour. To evaluate the impact of the law we surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage. We also compare employment changes at stores in New Jersey that were initially paying high wages (above $5) to the changes at lower-wage stores. We find no indication that the rise in the minimum wage reduced employment.;,citation_author=David Card;,citation_author=Alan B Krueger;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_issue=4;,citation_doi=10.3386/w4509;,citation_volume=84;,citation_journal_title=American Economic Review;">
<meta name="citation_reference" content="citation_title=Greedy Function Approximation: A Gradient Boosting Machine;,citation_author=Jerome H. Friedman;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_fulltext_html_url=https://www.jstor.org/stable/2699986;,citation_issue=5;,citation_volume=29;,citation_journal_title=The Annals of Statistics;">
<meta name="citation_reference" content="citation_title=Confidence intervals for random forests: The jackknife and the infinitesimal jackknife;,citation_abstract=We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = ⊖(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = ⊖(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.  2014 Stefan Wager, Trevor Hastie and Bradley Efron.;,citation_author=Stefan Wager;,citation_author=Trevor Hastie;,citation_author=Bradley Efron;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://arxiv.org/abs/1311.4555;,citation_issn=15337928;,citation_pmid=25580094;,citation_volume=15;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=Metalearners for estimating heterogeneous treatment effects using machine learning;,citation_abstract=There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.;,citation_author=Sören R. Künzel;,citation_author=Jasjeet S. Sekhon;,citation_author=Peter J. Bickel;,citation_author=Bin Yu;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03461;,citation_issue=10;,citation_doi=10.1073/pnas.1804597116;,citation_issn=10916490;,citation_pmid=30770453;,citation_volume=116;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=Causal Inference of Human Resources Key Performance Indicators;,citation_abstract=The purpose of this study is to examine the relationship between attrition rates and key per- formance indicators in a corporate workforce by using the propensity score (PS) matching. The study shows the possibilities of using logistic regression and propensity score matching methods in human capital strategic decisions. The data used here was from a fictional data set created by IBM data scientists based on active and separated employees to uncover the factors that lead to employee attrition. For each of the 1,470 employee records, information was generated about de- mographic characteristics such as age, gender, marital status, education level, employment status and culture, compensation, and performance factors. Two logistic equations are defined for two key performance objectives, culture and work life balance. A logistic regression analysis on each equation, with support from contrast estimation, reveals a comparison between the most and least favorable responses to key performance indicators is most significant. After successfully balancing a treatment and control group using the nearest neighbor matching technique on propensity score estimates from the logistic regression, a paired t-test reveals a statistically significant difference for the work life balance key performance indi- cator. This result is interpreted as having the highest probability of successfully reducing attrition when the focus is on increasing employee responses to satisfaction levels of work life balance in comparison to other key performance indicators.;,citation_author=Matthew Kovach;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/;,citation_dissertation_institution=Bowling Green State University;">
<meta name="citation_reference" content="citation_title=Machine Learning for Causal Inference;,citation_author=Sheng Li;,citation_author=Zhixuan Chu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1007/978-3-031-35051-1;,citation_isbn=9783031350504;">
<meta name="citation_reference" content="citation_title=Joint culpability: The effects of medical marijuana laws on crime;,citation_abstract=Most U.S. states have passed medical marijuana laws. In this paper, we study the effects of these laws on violent and property crime. We first estimate models that control for city fixed effects and flexible city-specific time trends. To supplement this regression analysis, we use the synthetic control method which can relax the parallel trend assumption and better account for heterogeneous policy effects. Both the regression analysis and the synthetic control method suggest no causal effects of medical marijuana laws on violent or property crime at the national level. We also find no strong effects within individual states, except for in California where the medical marijuana law reduced both violent and property crime by 20%.;,citation_author=Yu-Wei Luke Chu;,citation_author=Wilbur Townsend;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://doi.org/10.1016/j.jebo.2018.07.003;,citation_doi=10.1016/j.jebo.2018.07.003;,citation_issn=01672681;,citation_volume=159;,citation_journal_title=Journal of Economic Behavior and Organization;,citation_publisher=Elsevier B.V.;">
<meta name="citation_reference" content="citation_title=Air pollution and COVID-19 mortality in the United States: Strengths and limitations of an ecological regression analysis;,citation_abstract=Assessing whether long-term exposure to air pollution increases the severity of COVID-19 health outcomes, including death, is an important public health objective. Limitations in COVID-19 data availability and quality remain obstacles to conducting conclusive studies on this topic. At present, publicly available COVID-19 outcome data for representative populations are available only as area-level counts. Therefore, studies of long-term exposure to air pollution and COVID-19 outcomes using these data must use an ecological regression analysis, which precludes controlling for individual-level COVID-19 risk factors. We describe these challenges in the context of one of the first preliminary investigations of this question in the United States, where we found that higher historical PM2.5 exposures are positively associated with higher county-level COVID-19 mortality rates after accounting for many area-level confounders. Motivated by this study, we lay the groundwork for future research on this important topic, describe the challenges, and outline promising directions and opportunities.;,citation_author=X. Wu;,citation_author=R. C. Nethery;,citation_author=M. B. Sabath;,citation_author=D. Braun;,citation_author=F. Dominici;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=45;,citation_doi=10.1126/SCIADV.ABD4049;,citation_issn=23752548;,citation_pmid=33148655;,citation_volume=6;,citation_journal_title=Science Advances;">
<meta name="citation_reference" content="citation_title=The state of applied econometrics: Causality and policy evaluation;,citation_author=Susan Athey;,citation_author=Guido W. Imbens;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1607.00699;,citation_issue=2;,citation_doi=10.1257/jep.31.2.3;,citation_issn=08953309;,citation_volume=31;,citation_journal_title=Journal of Economic Perspectives;">
<meta name="citation_reference" content="citation_title=Reflection on modern methods: When worlds collide - Prediction, machine learning and causal inference;,citation_abstract=Causal inference requires theory and prior knowledge to structure analyses, and is not usuallythought of as an arena for the application of prediction modelling. However, contemporary causal inference methods, premised on counterfactual or potential outcomes approaches, often include processing steps before the final estimation step. The purposes of this paper are: (i) to overview the recent emergence of prediction underpinning steps in contemporary causal inference methods as a usefulperspective on contemporary causal inference methods, and (ii) explore the role of machine learning(as one approach to ’best prediction’) in causal inference. Causal inference methods covered include propensity scores, inverse probability of treatment weights (IPTWs), G computation and targeted maximum likelihood estimation (TMLE). Machine learning has been used more for propensity scores and TMLE, and there is potential for increased use in G computation and estimation of IPTWs.;,citation_author=Tony Blakely;,citation_author=John Lynch;,citation_author=Koen Simons;,citation_author=Rebecca Bentley;,citation_author=Sherri Rose;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=6;,citation_doi=10.1093/ije/dyz132;,citation_issn=14643685;,citation_pmid=31298274;,citation_volume=49;,citation_journal_title=International Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=A tutorial on propensity score estimation for multiple treatments using generalized boosted models;,citation_abstract=The use of propensity scores to control for pretreatment imbalances on observed variables in non-randomized or observational studies examining the causal effects of treatments or interventions has become widespread over the past decade. For settings with two conditions of interest such as a treatment and a control, inverse probability of treatment weighted estimation with propensity scores estimated via boosted models has been shown in simulation studies to yield causal effect estimates with desirable properties. There are tools (e.g., the twang package in R) and guidance for implementing this method with two treatments. However, there is not such guidance for analyses of three or more treatments. The goals of this paper are twofold: (1) to provide step-by-step guidance for researchers who want to implement propensity score weighting for multiple treatments and (2) to propose the use of generalized boosted models (GBM) for estimation of the necessary propensity score weights. We define the causal quantities that may be of interest to studies of multiple treatments and derive weighted estimators of those quantities. We present a detailed plan for using GBM to estimate propensity scores and using those scores to estimate weights and causal effects. We also provide tools for assessing balance and overlap of pretreatment variables among treatment groups in the context of multiple treatments. A case study examining the effects of three treatment programs for adolescent substance abuse demonstrates the methods.  2013 John Wiley &amp;amp;amp; Sons, Ltd.;,citation_author=Daniel F. Mccaffrey;,citation_author=Beth Ann Griffin;,citation_author=Daniel Almirall;,citation_author=Mary Ellen Slaughter;,citation_author=Rajeev Ramchand;,citation_author=Lane F. Burgette;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_issue=19;,citation_doi=10.1002/sim.5753;,citation_issn=02776715;,citation_pmid=23508673;,citation_volume=32;,citation_journal_title=Statistics in Medicine;">
<meta name="citation_reference" content="citation_title=Group Average Treatment Effects for Observational Studies;,citation_abstract=The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. The groups can be understood as a broader aggregation of the conditional average treatment effect (CATE) where the number of groups is set in advance. In economics, this approach is similar to pre-analysis plans. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias, we implement a doubly-robust estimator in the first stage. We use machine learning methods to learn the conditional mean functions as well as the propensity score. The group average treatment effect is then estimated via a linear projection model. The linear model is easy to interpret, provides p-values and confidence intervals, and limits the danger of finding spurious heterogeneity due to small subgroups in the CATE. To control for confounding in the linear model, we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We find that our proposed method has lower absolute errors as well as smaller bias than the benchmark doubly-robust estimator. We further introduce a bagging type averaging for the CATE function for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.;,citation_author=Daniel Jacob;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1911.02688;">
<meta name="citation_reference" content="citation_title=Recursive partitioning for heterogeneous causal effects;,citation_abstract=In this paper we propose methods for estimating heterogeneity in causal effects in experimental and observational studies and for conducting hypothesis tests about the magnitude of differences in treatment effects across subsets of the population. We provide a data-driven approach to partition the data into subpopulations that differ in the magnitude of their treatment effects. The approach enables the construction of valid confidence intervals for treatment effects, even with many covariates relative to the sample size, and without &amp;amp;amp;quot;sparsity&amp;quot; assumptions. We propose an &quot;honest&quot; approach to estimation, whereby one sample is used to construct the partition and another to estimate treatment effects for each subpopulation. Our approach builds on regression tree methods, modified to optimize for goodness of fit in treatment effects and to account for honest estimation. Our model selection criterion anticipates that bias will be eliminated by honest estimation and also accounts for the effect of making additional splits on the variance of treatment effect estimates within each subpopulation. We address the challenge that the &quot;ground truth&quot; for a causal effect is not observed for any individual unit, so that standard approaches to cross-validation must be modified. Through a simulation study, we show that for our preferred method honest estimation results in nominal coverage for 90% confidence intervals, whereas coverage ranges between 74% and 84% for nonhonest approaches. Honest estimation requires estimating the model with a smaller sample size; the cost in terms of mean squared error of treatment effects for our preferred method ranges between 7-22%.;,citation_author=Susan Athey;,citation_author=Guido Imbens;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=https://arxiv.org/abs/1504.01132;,citation_issue=27;,citation_doi=10.1073/pnas.1510489113;,citation_issn=10916490;,citation_pmid=27382149;,citation_volume=113;,citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America;">
<meta name="citation_reference" content="citation_title=RFRSF: Employee Turnover Prediction Based on Random Forests and Survival Analysis;,citation_abstract=In human resource management, employee turnover problem is heavily concerned by managers since the leave of key employees can bring great loss to the company. However, most existing researches are employee-centered, which ignored the historical events of turnover behaviors or the longitudinal data of job records. In this paper, from an event-centered perspective, we design a hybrid model based on survival analysis and machine learning, and propose a turnover prediction algorithm named RFRSF, which combines survival analysis for censored data processing and ensemble learning for turnover behavior prediction. In addition, we take strategies to handle employees with multiple turnover records so as to construct survival data with censored records. We compare RFRSF with several baseline methods on a real dataset crawled from one of the biggest online professional social platforms of China. The results show that the survival analysis model can significantly benefit the employee turnover prediction performance.;,citation_author=Ziwei Jin;,citation_author=Jiaxing Shang;,citation_author=Qianwen Zhu;,citation_author=Chen Ling;,citation_author=Wu Xie;,citation_author=Baohua Qiang;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1007/978-3-030-62008-0_35;,citation_isbn=9783030620073;,citation_issn=16113349;,citation_volume=12343 LNCS;,citation_journal_title=Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);">
<meta name="citation_reference" content="citation_title=Propensity score estimation with boosted regression for evaluating causal effects in observational studies;,citation_abstract=Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pre-treatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This article demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. The authors illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences and substantially alter the apparent relative effects of adolescent substance abuse treatment.;,citation_author=Daniel F. McCaffrey;,citation_author=Greg Ridgeway;,citation_author=Andrew R. Morral;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_issue=4;,citation_doi=10.1037/1082-989X.9.4.403;,citation_issn=1082989X;,citation_pmid=15598095;,citation_volume=9;,citation_journal_title=Psychological Methods;">
<meta name="citation_reference" content="citation_title=Bagging predictors;,citation_author=Leo Breiman;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_doi=10.3390/risks8030083;,citation_issn=22279091;,citation_volume=24;,citation_journal_title=Machine Learning;">
<meta name="citation_reference" content="citation_title=Estimation and Inference of Heterogeneous Treatment Effects using Random Forests;,citation_abstract=Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman’s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.;,citation_author=Stefan Wager;,citation_author=Susan Athey;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1080/01621459.2017.1319839;,citation_issue=523;,citation_doi=10.1080/01621459.2017.1319839;,citation_issn=1537274X;,citation_volume=113;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=Taylor &amp;amp;amp; Francis;">
<meta name="citation_reference" content="citation_title=The contribution of wildfire to PM2.5 trends in the USA;,citation_abstract=Steady improvements in ambient air quality in the USA over the past several decades, in part a result of public policy1,2, have led to public health benefits1–4. However, recent trends in ambient&nbsp;concentrations of&nbsp;particulate matter with diameters less than 2.5 $\mu$m (PM2.5), a pollutant regulated under the Clean Air Act1, have stagnated or begun to reverse throughout much of the USA5. Here we use a combination of ground- and satellite-based air pollution data from 2000 to 2022 to quantify the contribution of wildfire smoke to these PM2.5 trends. We find that since at least 2016, wildfire smoke has influenced trends in average annual PM2.5 concentrations in nearly three-quarters of states in the contiguous USA, eroding about 25% of previous multi-decadal progress in reducing PM2.5 concentrations on average in those states, equivalent to 4 years of air quality progress, and more than 50% in many western states. Smoke influence on trends in the number of days with extreme PM2.5 concentrations is detectable by 2011, but the influence can be detected primarily in western and mid-western states. Wildfire-driven increases in ambient PM2.5 concentrations are unregulated under current air pollution law6 and, in the absence of further interventions, we show that the contribution&nbsp;of wildfire to regional and national air quality trends is likely to grow as the climate continues to warm.;,citation_author=Marshall Burke;,citation_author=Marissa L. Childs;,citation_author=Brandon Cuesta;,citation_author=Minghao Qiu;,citation_author=Jessica Li;,citation_author=Carlos F. Gould;,citation_author=Sam Heft-Neal;,citation_author=Michael Wara;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=7984;,citation_doi=10.1038/s41586-023-06522-6;,citation_issn=14764687;,citation_pmid=37730996;,citation_volume=622;,citation_journal_title=Nature;,citation_publisher=Springer US;">
<meta name="citation_reference" content="citation_title=Marginal structural models in clinical research: When and how to use them?;,citation_abstract=Marginal structural models are a multi-step estimation procedure designed to control for the effect of confounding variables that change over time, and are affected by previous treatment. When a time-varying confounder is affected by prior treatment standard methods for confounding control are inappropriate, because over time the covariate plays both the role of confounder and mediator of the effect of treatment on outcome. Marginal structural models first calculate a weight to assign to each observation. These weights reflect the extent to which observations with certain characteristics (covariate values) are under-represented or over-represented in the sample with the respect to a target population in which these characteristics are balanced across treatment groups. Then, marginal structural models estimate the outcome of interest taking into account these weights. Marginal structural models are a powerful method for confounding control in longitudinal study designs that collect time-varying information on exposure, outcome and other covariates.;,citation_author=Tyler Williamson;,citation_author=Pietro Ravani;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=February;,citation_doi=10.1093/ndt/gfw341;,citation_issn=14602385;,citation_pmid=28201767;,citation_volume=32;,citation_journal_title=Nephrology Dialysis Transplantation;">
<meta name="citation_reference" content="citation_title=Optimizing variance-bias trade-off in the TWANG package for estimation of propensity scores;,citation_abstract=While propensity score weighting has been shown to reduce bias in treatment effect estimation when selection bias is present, it has also been shown that such weighting can perform poorly if the estimated propensity score weights are highly variable. Various approaches have been proposed which can reduce the variability of the weights and the risk of poor performance, particularly those based on machine learning methods. In this study, we closely examine approaches to fine-tune one machine learning technique [generalized boosted models (GBM)] to select propensity scores that seek to optimize the variance-bias trade-off that is inherent in most propensity score analyses. Specifically, we propose and evaluate three approaches for selecting the optimal number of trees for the GBM in the twang package in R. Normally, the twang package in R iteratively selects the optimal number of trees as that which maximizes balance between the treatment groups being considered. Because the selected number of trees may lead to highly variable propensity score weights, we examine alternative ways to tune the number of trees used in the estimation of propensity score weights such that we sacrifice some balance on the pre-treatment covariates in exchange for less variable weights. We use simulation studies to illustrate these methods and to describe the potential advantages and disadvantages of each method. We apply these methods to two case studies: one examining the effect of dog ownership on the owner’s general health using data from a large, population-based survey in California, and a second investigating the relationship between abstinence and a long-term economic outcome among a sample of high-risk youth.;,citation_author=Layla Parast;,citation_author=Daniel F. McCaffrey;,citation_author=Lane F. Burgette;,citation_author=Fernando Hoces Guardia;,citation_author=Daniela Golinelli;,citation_author=Jeremy N. V. Miles;,citation_author=Beth Ann Griffin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=3-4;,citation_doi=10.1007/s10742-016-0168-2;,citation_issn=15729400;,citation_volume=17;,citation_journal_title=Health Services and Outcomes Research Methodology;,citation_publisher=Springer US;">
<meta name="citation_reference" content="citation_title=We are all social scientists now: How big data, machine learning, and causal inference work together;,citation_author=Justin Grimmer;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=1;,citation_doi=10.1017/S1049096514001784;,citation_issn=15375935;,citation_volume=48;,citation_journal_title=American Political Science Association;">
<meta name="citation_reference" content="citation_title=Recent developments in the econometrics of program evaluation;,citation_abstract=Many empirical questions in economics and other social sciences depend on causal effects of programs or policies. In the last two decades, much research has been done on the econometric and statistical analysis of such causal effects. This recent theoretical literature has built on, and combined features of earlier work in both the statistics and econometrics literatures. It has by now reached a level of maturity that makes it an important tool in many areas of empirical research in economics, including labor economics, public finance, development economics, industrial organization, and other areas of empirical microeconomics. In this review, we discuss some of the recent developments. We focus primarily on practical issues for empirical researchers, as well as provide a historical overview of the area and give references to more technical research.;,citation_author=Guido W. Imbens;,citation_author=Jeffrey M. Wooldridge;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=1;,citation_doi=10.1257/jel.47.1.5;,citation_issn=00220515;,citation_volume=47;,citation_journal_title=Journal of Economic Literature;">
<meta name="citation_reference" content="citation_title=Agricultural extension and technology adoption for food security: Evidence from Uganda;,citation_abstract=We evaluate causal impacts of a large-scale agricultural extension program for smallholder women farmers on technology adoption and food security in Uganda through a regression discontinuity design that exploits an arbitrary distance-to-branch threshold for village program eligibility. We find eligible farmers used better basic cultivation methods, achieved improved food security. Given minimal changes in adoption of relatively expensive inputs, we attribute these gains to improved cultivation methods that require low upfront monetary investment. Farmers also modified their shockcoping methods. These results highlight the role of information and training in boosting agricultural productivity among poor farmers and, indirectly, improving food security.;,citation_author=Yao Pan;,citation_author=Stephen C. Smith;,citation_author=Munshi Sulaiman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=4;,citation_doi=10.1093/ajae/aay012;,citation_issn=14678276;,citation_volume=100;,citation_journal_title=American Journal of Agricultural Economics;">
<meta name="citation_reference" content="citation_title=Practical Guide to Honest Causal Forests for Identifying Heterogeneous Treatment Effects;,citation_abstract=&amp;amp;amp;quot;Heterogeneous treatment effects&amp;quot;is a term which refers to conditional average treatment effects (i.e., CATEs) that vary across population subgroups. Epidemiologists are often interested in estimating such effects because they can help detect populations that may particularly benefit from or be harmed by a treatment. However, standard regression approaches for estimating heterogeneous effects are limited by preexisting hypotheses, test a single effect modifier at a time, and are subject to the multiple-comparisons problem. In this article, we aim to offer a practical guide to honest causal forests, an ensemble tree-based learning method which can discover as well as estimate heterogeneous treatment effects using a data-driven approach. We discuss the fundamentals of tree-based methods, describe how honest causal forests can identify and estimate heterogeneous effects, and demonstrate an implementation of this method using simulated data. Our implementation highlights the steps required to simulate data sets, build honest causal forests, and assess model performance across a variety of simulation scenarios. Overall, this paper is intended for epidemiologists and other population health researchers who lack an extensive background in machine learning yet are interested in utilizing an emerging method for identifying and estimating heterogeneous treatment effects.;,citation_author=Neal Jawadekar;,citation_author=Katrina Kezios;,citation_author=Michelle C. Odden;,citation_author=Jeanette A. Stingone;,citation_author=Sebastian Calonico;,citation_author=Kara Rudolph;,citation_author=Adina Zeki Al Hazzouri;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=7;,citation_doi=10.1093/aje/kwad043;,citation_issn=14766256;,citation_pmid=36843042;,citation_volume=192;,citation_journal_title=American Journal of Epidemiology;">
<meta name="citation_reference" content="citation_title=Does the estimation of the propensity score by machine learning improve matching estimation? The case of Germany’s programmes for long term unemployed;,citation_abstract=Matching-type estimators using the propensity score are the major workhorse in active labour market policy evaluation. This work investigates if machine learning algorithms for estimating the propensity score lead to more credible estimation of average treatment effects on the treated using a radius matching framework. Considering two popular methods, the results are ambiguous: We find that using LASSO based logit models to estimate the propensity score delivers more credible results than conventional methods in small and medium sized high dimensional datasets. However, the usage of Random Forests to estimate the propensity score may lead to a deterioration of the performance in situations with a low treatment share. The application reveals a positive effect of the training programme on days in employment for long-term unemployed. While the choice of the “first stage” is highly relevant for settings with low number of observations and few treated, machine learning and conventional estimation becomes more similar in larger samples and higher treatment shares.;,citation_author=Daniel Goller;,citation_author=Michael Lechner;,citation_author=Andreas Moczall;,citation_author=Joachim Wolff;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=March;,citation_doi=10.1016/j.labeco.2020.101855;,citation_issn=09275371;,citation_volume=65;,citation_journal_title=Labour Economics;,citation_publisher=Elsevier B.V.;">
<meta name="citation_reference" content="citation_title=Machine Learning Methods Economists Should Know Athey and Imbens 2019;,citation_abstract=We discuss the relevance of the recent Machine Learning (ML) literature for eco- nomics and econometrics. First we discuss the differences in goals, methods and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the machine learning literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics, methods that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, problems that include causal inference for average treat- ment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.;,citation_author=Susan Athey;,citation_author=Guido W Imbens;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://www.annualreviews.org/content/journals/10.1146/annurev-economics-080217-053433;,citation_doi=10.1146/annurev-economics-080217-053433;,citation_volume=11;,citation_journal_title=Annual Review of Economics;">
<meta name="citation_reference" content="citation_title=Estimating Causal Effects of Treatments in Experimental and Observational Studies;,citation_abstract=A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible, but the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases.;,citation_author=Donald B. Rubin;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_issue=5;,citation_doi=10.1002/j.2333-8504.1972.tb00631.x;,citation_issn=0424-6144;,citation_volume=66;,citation_journal_title=Journal of Educational Psychology;">
<meta name="citation_reference" content="citation_title=TDS: Heterogeneous Treatment Effect and Meta Learners;,citation_author=Zain Ahmed;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;">
<meta name="citation_reference" content="citation_title=An experiment in cotwin control: Adaptation to space travel;,citation_abstract=Monozygotic (MZ) twins who experience different environmental exposures offer an informative, naturally occurring cotwin control design. Given their genetic identity, differences in their physical, medical, and behavioral outcomes can be linked to their experiential differences. In space research, a broad range of ethological studies has been conducted on the effects of microgravity on sensorimotor activity, as well as on social group behavior during orbital flights, in a large array of isolated and confined environments, both for short-term, mid-term, and long-term missions. The study of MZ twin astronaut Scott Kelly, who spent almost 1-year at the International Space Station, while his cotwin Mark Kelly stayed on earth, is a unique opportunity to identify factors affecting astronauts’ health. This experiment of nature can also reveal the extent to which a space mission modifies different adaptive systems at the genetic and epigenetic levels.;,citation_author=Carole Tafforin;,citation_author=Nancy L. Segal;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.sciencedirect.com/book/9780128215142/twin-research-for-everyone?via=ihub=;,citation_doi=https://doi.org/10.1016/C2019-0-02208-X;,citation_isbn=9780128215159;,citation_inbook_title=Twin research for everyone;">
<meta name="citation_reference" content="citation_title=Why Propensity Scores Should Not Be Used for Matching;,citation_abstract=We show that propensity score matching (PSM), an enormously popular method of preprocessing data for causal inference, often accomplishes the opposite of its intended goal - thus increasing imbalance, inefficiency, model dependence, and bias. The weakness of PSM comes from its attempts to approximate a completely randomized experiment, rather than, as with other matching methods, a more efficient fully blocked randomized experiment. PSM is thus uniquely blind to the often large portion of imbalance that can be eliminated by approximating full blocking with other matching methods. Moreover, in data balanced enough to approximate complete randomization, either to begin with or after pruning some observations, PSM approximates random matching which, we show, increases imbalance even relative to the original data. Although these results suggest researchers replace PSM with one of the other available matching methods, propensity scores have other productive uses.;,citation_author=Gary King;,citation_author=Richard Nielsen;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=4;,citation_doi=10.1017/pan.2019.11;,citation_issn=14764989;,citation_volume=27;,citation_journal_title=Political Analysis;">
<meta name="citation_reference" content="citation_title=Using synthetic controls: Feasibility, data requirements, and methodological aspects;,citation_abstract=Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.;,citation_author=Alberto Abadie;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issue=2;,citation_doi=10.1257/jel.20191450;,citation_issn=00220515;,citation_volume=59;,citation_journal_title=Journal of Economic Literature;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=causaldata: Example data sets for causal inference textbooks;,citation_author=Nick Huntington-Klein;,citation_author=Malcolm Barrett;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://CRAN.R-project.org/package=causaldata;">
<meta name="citation_reference" content="citation_title=cobalt: Covariate balance tables and plots;,citation_author=Noah Greifer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=cobalt;">
<meta name="citation_reference" content="citation_title=data.table: Extension of “data.frame”;,citation_author=Tyson Barrett;,citation_author=Matt Dowle;,citation_author=Arun Srinivasan;,citation_author=Jan Gorecki;,citation_author=Michael Chirico;,citation_author=Toby Hocking;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=data.table;">
<meta name="citation_reference" content="citation_title=gbm: Generalized boosted regression models;,citation_author=Greg Ridgeway;,citation_author=GBM Developers;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=gbm;">
<meta name="citation_reference" content="citation_title=kableExtra: Construct complex table with “kable” and pipe syntax;,citation_author=Hao Zhu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=kableExtra;">
<meta name="citation_reference" content="citation_title=knitr: A general-purpose package for dynamic report generation in r;,citation_author=Yihui Xie;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://yihui.org/knitr/;">
<meta name="citation_reference" content="citation_title=Dynamic documents with R and knitr;,citation_author=Yihui Xie;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://yihui.org/knitr/;">
<meta name="citation_reference" content="citation_title=knitr: A comprehensive tool for reproducible research in R;,citation_author=Yihui Xie;,citation_editor=Victoria Stodden;,citation_editor=Friedrich Leisch;,citation_editor=Roger D. Peng;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_inbook_title=Implementing reproducible computational research;">
<meta name="citation_reference" content="citation_title=How to interpret statistical models using marginaleffects in R and Python;,citation_author=Vincent Arel-Bundock;,citation_author=Noah Greifer;,citation_author=Andrew Heiss;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=MatchIt: Nonparametric preprocessing for parametric causal inference;,citation_author=Daniel E. Ho;,citation_author=Kosuke Imai;,citation_author=Gary King;,citation_author=Elizabeth A. Stuart;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=8;,citation_doi=10.18637/jss.v042.i08;,citation_volume=42;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=patchwork: The composer of plots;,citation_author=Thomas Lin Pedersen;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=patchwork;">
<meta name="citation_reference" content="citation_title=Classification and regression by randomForest;,citation_author=Andy Liaw;,citation_author=Matthew Wiener;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_fulltext_html_url=https://CRAN.R-project.org/doc/Rnews/;,citation_issue=3;,citation_volume=2;,citation_journal_title=R News;">
<meta name="citation_reference" content="citation_title=renv: Project environments;,citation_author=Kevin Ushey;,citation_author=Hadley Wickham;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=renv;">
<meta name="citation_reference" content="citation_title=rmarkdown: Dynamic documents for r;,citation_author=JJ Allaire;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Jonathan McPherson;,citation_author=Javier Luraschi;,citation_author=Kevin Ushey;,citation_author=Aron Atkins;,citation_author=Hadley Wickham;,citation_author=Joe Cheng;,citation_author=Winston Chang;,citation_author=Richard Iannone;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://github.com/rstudio/rmarkdown;">
<meta name="citation_reference" content="citation_title=R markdown: The definitive guide;,citation_author=Yihui Xie;,citation_author=J. J. Allaire;,citation_author=Garrett Grolemund;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://bookdown.org/yihui/rmarkdown;,citation_isbn=9781138359338;">
<meta name="citation_reference" content="citation_title=R markdown cookbook;,citation_author=Yihui Xie;,citation_author=Christophe Dervieux;,citation_author=Emily Riederer;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://bookdown.org/yihui/rmarkdown-cookbook;,citation_isbn=9780367563837;">
<meta name="citation_reference" content="citation_title=showtext: Using fonts more easily in r graphs;,citation_author=Yixuan Qiu;,citation_author=undefined See file AUTHORS for details.;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=showtext;">
<meta name="citation_reference" content="citation_title=WeightIt: Weighting for covariate balance in observational studies;,citation_author=Noah Greifer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=WeightIt;">
<meta name="citation_reference" content="citation_title=Welcome to the tidyverse;,citation_author=Hadley Wickham;,citation_author=Mara Averick;,citation_author=Jennifer Bryan;,citation_author=Winston Chang;,citation_author=Lucy D’Agostino McGowan;,citation_author=Romain François;,citation_author=Garrett Grolemund;,citation_author=Alex Hayes;,citation_author=Lionel Henry;,citation_author=Jim Hester;,citation_author=Max Kuhn;,citation_author=Thomas Lin Pedersen;,citation_author=Evan Miller;,citation_author=Stephan Milton Bache;,citation_author=Kirill Müller;,citation_author=Jeroen Ooms;,citation_author=David Robinson;,citation_author=Dana Paige Seidel;,citation_author=Vitalie Spinu;,citation_author=Kohske Takahashi;,citation_author=Davis Vaughan;,citation_author=Claus Wilke;,citation_author=Kara Woo;,citation_author=Hiroaki Yutani;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=43;,citation_doi=10.21105/joss.01686;,citation_volume=4;,citation_journal_title=Journal of Open Source Software;">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/implimentation_workflow.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tutorial: Implimentation, Workflow, and Example with <code>WeightIt</code> and<code>gbm</code> in R</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning and the Propensity Score</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mitchellcameron123/ML-PS.git" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-and-the-Propensity-Score.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=|url|" title="Facebook" class="quarto-navigation-tool px-1" aria-label="Facebook"><i class="bi bi-facebook"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/intro_background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/pscore_theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Propensity Scores and Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/implimentation_workflow.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tutorial: Implimentation, Workflow, and Example with <code>WeightIt</code> and<code>gbm</code> in R</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/coffee_replication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Replication Case Study</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusion and Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-gbm-tune-workflow" id="toc-sec-gbm-tune-workflow" class="nav-link active" data-scroll-target="#sec-gbm-tune-workflow"><span class="header-section-number">3.1</span> Hyperparameter Tuning and Workflow</a></li>
  <li><a href="#example-nsw-jobs-dataset-using-r" id="toc-example-nsw-jobs-dataset-using-r" class="nav-link" data-scroll-target="#example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.2</span> Example: NSW Jobs Dataset Using R</a>
  <ul class="collapse">
  <li><a href="#step-1-6-model-fitting-and-tuning" id="toc-step-1-6-model-fitting-and-tuning" class="nav-link" data-scroll-target="#step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.2.1</span> Step 1-6: Model Fitting and Tuning</a></li>
  <li><a href="#sec-nsw-balance" id="toc-sec-nsw-balance" class="nav-link" data-scroll-target="#sec-nsw-balance"><span class="header-section-number">3.2.2</span> Step 7 and 8: Assessing Balance</a></li>
  <li><a href="#sec-nsw-results" id="toc-sec-nsw-results" class="nav-link" data-scroll-target="#sec-nsw-results"><span class="header-section-number">3.2.3</span> Step 9: Results</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mitchellcameron123/ML-PS.git/edit/main/chapters/implimentation_workflow.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-demo" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tutorial: Implimentation, Workflow, and Example with <code>WeightIt</code> and<code>gbm</code> in R</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Based on <span class="citation" data-cites="Friedman2001">Friedman (<a href="references.html#ref-Friedman2001" role="doc-biblioref">2001</a>)</span>, the <code>gbm</code> package implements a <em>Generalized Boosting Machine</em>. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. Although this implementation very closely follows <span class="citation" data-cites="Friedman2001">Friedman (<a href="references.html#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> who introduced the gradient boosting machine. <code>gbm</code> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <code>bag.fraction</code> parameter.</p>
<p>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <code>gbm</code> package does not support this type of tuning, a wrapper package like <code>WeightIt</code> is necessary. <code>WeightIt</code> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <code>WeightIt</code> supports hyperparameter turning of <code>shrinkage</code>, <code>interaction.depth</code>, and <code>n.trees</code>. Once the best model is identified, propensity scores are predicted inside <code>WeightIt</code>. These can be used inside <code>WeightIt</code> to perform IPW or extracted for other implementations. <code>WeightIt</code> also supports an offset meaning that logistic regression predictions are supplied to the <code>GBM</code> package.</p>
<p>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="citation" data-cites="McCaffrey2004 Ridgeway2024">(see <a href="references.html#ref-McCaffrey2004" role="doc-biblioref">McCaffrey, Ridgeway, and Morral 2004</a>; <a href="references.html#ref-Ridgeway2024" role="doc-biblioref">Ridgeway et al. 2024</a>)</span>. A very low learning rate, such as <span class="math inline">\(0.01\)</span> or <span class="math inline">\(0.0005\)</span>, allows a smooth descent of the loss function. The model should include a high number of trees, with <span class="math inline">\(10,000\)</span> or <span class="math inline">\(20,000\)</span> being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for <span class="math inline">\(3\)</span> splits. Based on anecdotal experience, <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span> splits per tree is optimal, consistent with recommendations by <span class="citation" data-cites="McCaffrey2004">McCaffrey, Ridgeway, and Morral (<a href="references.html#ref-McCaffrey2004" role="doc-biblioref">2004</a>)</span>.</p>
<p>Another package, <code>twang</code>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <code>caret</code> must be used. Although <code>twang</code> has other useful functionalities which users may wish to implement.</p>
<section id="sec-gbm-tune-workflow" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-gbm-tune-workflow"><span class="header-section-number">3.1</span> Hyperparameter Tuning and Workflow</h2>
<!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  -->
<p>The <code>WeigthtIt</code> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <code>cobalt</code>. The best information for this package can be found on this <a href="https://ngreifer.github.io/WeightIt/index.html">website</a> or accessed with <code>vignette("WeightIt")</code> inside R after installation using <code>install.packages("WeightIt")</code>.</p>
<p>A workflow for hyperparameter tuning in <code>WeightIt</code> may be completed as follows:</p>
<ol type="1">
<li><p>Specify the <code>criterion</code> option, which specifies the measure of the <em>best model</em>. The available options are the options that the <code>cobalt</code> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <code>sdm.mean</code> or the smallest maximum SDM across covariates called <code>sdm.max</code>.</p></li>
<li><p>Set the number of trees high. The package default is <code>n.trees = 10000</code> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <code>n.tree</code> if optimal.</p></li>
<li><p>Specify the grid search for the depth of the tree called <code>interaction.depth</code> and the learning rate called <code>shrinkage</code>. These values can be specified using <code>c()</code> such as <code>shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)</code> or as integers such as <code>interaction.depth = 1:5</code>. These particular values are heuristically selected <em>suggestions</em> of good starting values. Additionally, an offset can be considered by performing a grid search across <code>offset = c(TRUE,FALSE)</code>.</p></li>
<li><p>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <code>my_weightit_object$info$best.tune</code>.</p></li>
<li><p>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <code>interaction.depth = 1:5</code> and the best fit is <span class="math inline">\(5\)</span>, then a new search can consider <code>interaction.depth = 3:7</code> so that the local area around <span class="math inline">\(5\)</span> can be searched.</p></li>
<li><p>Experiment with <code>bag.fraction</code>, which means each tree will consider a drawn proportion of observations equal to <code>bag.fraction</code>. Iteratively changing <code>bag.fraction</code> and assessing balance at each value should be practical. Consider <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.67\)</span>, and <span class="math inline">\(1\)</span>.</p></li>
<li><p>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <code>love.plot()</code> such as <a href="coffee_replication.html#fig-coffee-replication-lplot" class="quarto-xref">Figure&nbsp;<span>4.1</span></a>.</p></li>
<li><p>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</p></li>
<li><p>Estimation and reporting of treatment effect.</p></li>
</ol>
</section>
<section id="example-nsw-jobs-dataset-using-r" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="example-nsw-jobs-dataset-using-r"><span class="header-section-number">3.2</span> Example: NSW Jobs Dataset Using R</h2>
<p>For demonstration, propensity scores are estimated following the workflow discussed in <a href="#sec-gbm-tune-workflow" class="quarto-xref"><span>Section 3.1</span></a> to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in <a href="appendix.html#sec-data-nsw-jobs" class="quarto-xref"><span>Section 6.1.1</span></a>. Randomisation should eliminate structural differences between groups, but <span class="citation" data-cites="Rosenbaum1983">Rosenbaum and Rubin (<a href="references.html#ref-Rosenbaum1983" role="doc-biblioref">1983</a>)</span> notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <code>WeightIt</code> and a logistic regression model using <code>glm()</code>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</p>
<div id="nte-ipw" class="callout callout-style-default callout-note callout-titled" title="Inverse Probability of Treatment Weighting">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;3.1: Inverse Probability of Treatment Weighting
</div>
</div>
<div class="callout-body-container callout-body">
<p>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a <em>pseudo-population</em> where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</p>
</div>
</div>
<section id="step-1-6-model-fitting-and-tuning" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="step-1-6-model-fitting-and-tuning"><span class="header-section-number">3.2.1</span> Step 1-6: Model Fitting and Tuning</h3>
<p>The <code>glm()</code> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <code>binomial()</code>. Recall the <code>nsw_formula</code> is specified in <a href="pscore_theory.html#sec-bagg-rf-probmachines" class="quarto-xref"><span>Section 2.2.2</span></a>.</p>
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-2" class="code-annotation-target"><a href="#annotated-cell-1-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>())</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-4" class="code-annotation-target"><a href="#annotated-cell-1-4" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="2" data-code-annotation="1">Fits a logistic regression model using the <code>glm()</code> function specified to be a logistic model with <code>family=binomial()</code> using the previously created <code>nsw_formula</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="4" data-code-annotation="2">Extracts the fitted values (propensity scores) from the model.</span>
</dd>
</dl>
<p>Using the propensity score column of <code>nsw_data</code>, the <code>WeightIt</code> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <code>nsw_logit_weight</code>.</p>
<div class="sourceCode" id="annotated-cell-2"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-2-2" class="code-annotation-target"><a href="#annotated-cell-2-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-2-3" class="code-annotation-target"><a href="#annotated-cell-2-3" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-2-4" class="code-annotation-target"><a href="#annotated-cell-2-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="3" data-code-annotation="2">Provides <code>weightit()</code> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <code>weightit()</code> function with <code>method = "glm"</code>. The separate estimation of the propensity scores is for illustrative purposes.</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="4" data-code-annotation="3">Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
</dd>
</dl>
<p>A GBM model for propensity scores can be specified using <code>method = "gbm"</code> inside the <code>weightit()</code> function. To ensure consistent results, running <code>set.seed(88)</code> will ensure each tree uses the same <code>seed</code> if <code>bag.fraction</code> less than <span class="math inline">\(1\)</span>. The model is fit using the heuristically suggested starting values. Note that this model may take approximately <span class="math inline">\(30\)</span> second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</p>
<div class="sourceCode" id="annotated-cell-3"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-3-3" class="code-annotation-target"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="annotated-cell-3-4"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>,</span>
<span id="annotated-cell-3-5"><a href="#annotated-cell-3-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, </span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-3-6" class="code-annotation-target"><a href="#annotated-cell-3-6" aria-hidden="true" tabindex="-1"></a>                                             <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</span>
<span id="annotated-cell-3-7"><a href="#annotated-cell-3-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-3-8" class="code-annotation-target"><a href="#annotated-cell-3-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="annotated-cell-3-9"><a href="#annotated-cell-3-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-3-10" class="code-annotation-target"><a href="#annotated-cell-3-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>,</span>
<span id="annotated-cell-3-11"><a href="#annotated-cell-3-11" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="6" onclick="event.preventDefault();">6</a><span id="annotated-cell-3-12" class="code-annotation-target"><a href="#annotated-cell-3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="1">Specifies the formula and data.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="3,4" data-code-annotation="2">Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="6,7" data-code-annotation="3">Performs a grid search over these values of the learning rate and depth of tree.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="8,9" data-code-annotation="4">Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="10,11" data-code-annotation="5">Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be <span class="math inline">\(10000\)</span> which is the package default.</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="12" data-code-annotation="6">Prints the tune details of the model with the best covariate balance.</span>
</dd>
</dl>
<!-- clarify the meaning of learning rate/shrinkage -->
<!-- change all the instructions to active speech not passive.  -->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
6       0.3                 1    bernoulli      FALSE    0.02253485      2392</code></pre>
</div>
</div>
<p>The best balance across all tuning combinations yields an average SMD of <span class="math inline">\(0.023\)</span> showing strong balance compared to the <span class="math inline">\(0.1\)</span> threshold. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in <a href="#sec-nsw-balance" class="quarto-xref"><span>Section 3.2.2</span></a> accompanying a discussion of balance.</p>
<p>The best machine has a learning rate of <span class="math inline">\(0.3\)</span> and contains <span class="math inline">\(2392\)</span> decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to <span class="math inline">\(0.3\)</span>. A reduction in the depth of tree and number of trees will reduce computation time.</p>
<p>The new tune grid will consider <code>shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)</code> as this allows the GBM to consider values between <span class="math inline">\(0.2\)</span> and <span class="math inline">\(0.3\)</span> and above <span class="math inline">\(0.3\)</span> which were missing in the previous grid.</p>
<div class="cell">
<details class="code-fold">
<summary>Code: Fit <code>weightit()</code> with Updated Tune Grid</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight2 <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method=</span><span class="st">"gbm"</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage=</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.45</span>, <span class="fl">0.5</span>),</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>best.tune)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   shrinkage interaction.depth distribution use.offset best.smd.mean best.tree
11      0.45                 2    bernoulli      FALSE    0.01965492        95</code></pre>
</div>
</div>
<p>Comparing the two iterations, there is a reduction from <span class="math inline">\(0.022\)</span> to <span class="math inline">\(0.02\)</span>. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of <span class="math inline">\(0.45\)</span>, a tree depth of <span class="math inline">\(2\)</span>, and <span class="math inline">\(95\)</span> trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</p>
<p>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, <a href="#fig-balance-iterations" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows the optimal number of trees is highly variable. If the learning rate is set to <code>shrinkage = 0.05</code>, then the best balance is not achieved until near to <span class="math inline">\(20,000\)</span> trees.</p>
<div class="cell">
<details class="code-fold">
<summary>Code: Create <a href="fig-balance-iterations">Figure 3.1</a></summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                                  <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>) <span class="sc">+</span> custom_ggplot_theme</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-balance-iterations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="implimentation_workflow_files/figure-html/fig-balance-iterations-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-balance-iterations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Relatoinship between standardised mean difference, number of interations, and learning rate in a GBM model. Please note the difference in horozontal scale between the two learning rates. The model is fit using <code>weightit</code> from the <code>WeightIt</code> package.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</p>
</section>
<section id="sec-nsw-balance" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-nsw-balance"><span class="header-section-number">3.2.2</span> Step 7 and 8: Assessing Balance</h3>
<div class="callout callout-style-default callout-warning callout-titled" title="The Importance of Discussing Balance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Importance of Discussing Balance
</div>
</div>
<div class="callout-body-container callout-body">
<p>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study’s conclusions. <span class="citation" data-cites="King2019">King and Nielsen (<a href="references.html#ref-King2019" role="doc-biblioref">2019</a>)</span> notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</p>
<p>A good resource of information for assessing balance is documentation from the <code>cobalt</code> package, which can be viewed by running <code>vignette(“cobalt”, package = “cobalt”)</code> in R.</p>
</div>
</div>
<p><code>cobalt</code> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <code>WeightIt</code> for IPW and <code>MatchIt</code> for propensity score matching. Balance tables are created using <code>bal.tab()</code>.</p>
<!-- make sure this comment about integration is not repeditive  -->
<div class="sourceCode" id="annotated-cell-6"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-6-1" class="code-annotation-target"><a href="#annotated-cell-6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt)</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-6-2" class="code-annotation-target"><a href="#annotated-cell-6-2" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight,</span>
<span id="annotated-cell-6-3"><a href="#annotated-cell-6-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-6-4" class="code-annotation-target"><a href="#annotated-cell-6-4" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="annotated-cell-6-5"><a href="#annotated-cell-6-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="4" onclick="event.preventDefault();">4</a><span id="annotated-cell-6-6" class="code-annotation-target"><a href="#annotated-cell-6-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="annotated-cell-6-7"><a href="#annotated-cell-6-7" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-6" data-target-annotation="5" onclick="event.preventDefault();">5</a><span id="annotated-cell-6-8" class="code-annotation-target"><a href="#annotated-cell-6-8" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-6" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="1" data-code-annotation="1">Loads the <code>cobalt</code> package. This assumes the package is already installed with <code>install.packages("cobalt")</code></span>
</dd>
<dt data-target-cell="annotated-cell-6" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="2,3" data-code-annotation="2">Uses the <code>bal.tab()</code> fucntion to create balance statistics for the previously created <code>nsw_logit_weight</code> model.</span>
</dd>
<dt data-target-cell="annotated-cell-6" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="4,5" data-code-annotation="3">Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
</dd>
<dt data-target-cell="annotated-cell-6" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="6" data-code-annotation="4">Sets a threshold of balance to be <span class="math inline">\(0.1\)</span> to determine if a covariate is balanced.</span>
</dd>
<dt data-target-cell="annotated-cell-6" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-6" data-code-lines="8" data-code-annotation="5">Extracts the balance table of the <code>nsw_logit_btab</code> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
</dd>
</dl>
<p>Additionally, <code>bal.tab()</code> will create balance tables for the GBM method’s IPWs and the raw data. For presentation, <code>dplyr</code> combines each of the individual balance tables for presentation using <code>kable</code> and <code>kableExtra</code>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code: Create Balance Tables</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_boosted_weight, </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                            <span class="at">data =</span> nsw_data,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_formula, </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                        <span class="at">data =</span> nsw_data, </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>                        <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>                        <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                        <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>                        <span class="at">s.d.denom =</span> <span class="st">"treated"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab <span class="ot">&lt;-</span> nsw_boosted_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab <span class="ot">&lt;-</span> nsw_raw_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code: Create <a href="tbl-combined-btab">Table 3.1</a></summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits =</span> <span class="dv">4</span>, <span class="at">booktabs =</span> <span class="cn">TRUE</span>, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width =</span> <span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co">#perhaps use the method from the coffe table as this is not as neat as the other method. for mergin that is.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-combined-btab" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Standardised mean difference (a measure of balance) across different covariates in the National Supported Word dataset. The values are categorised for different propenensity score methods allowing a comparison. Balance tables are computed using <code>bal.tab()</code> from the <code>cobalt</code> package.
</figcaption>
<div aria-describedby="tbl-combined-btab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variable</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Type</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">SMD</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Balanced</th>
<th data-quarto-table-cell-role="th" style="text-align: center; font-weight: bold;">Variance Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>Raw Data</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.1066</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.0278</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.1281</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">1.5513</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.0824</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.0763</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0449</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.2040</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.2783</td>
<td style="text-align: center; width: 3cm;">No</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0902</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Logistic Regression</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.0001</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.9809</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.0012</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.2725</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.0081</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.7971</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0006</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.0031</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0009</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0045</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd" data-grouplength="7">
<td colspan="5" style="border-bottom: 1px solid"><strong>IPTW: Boosting</strong></td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Age</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.0065</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">0.9086</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Education</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">0.0220</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.1391</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Income 1975</td>
<td style="text-align: center; width: 3cm;">Contin.</td>
<td style="text-align: center; width: 3cm;">-0.0152</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">1.0134</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Black</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0028</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Hispanic</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">-0.0547</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Degree</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0481</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
<tr class="even">
<td style="text-align: center; font-weight: bold; padding-left: 2em;" data-indentlevel="1">Married</td>
<td style="text-align: center; width: 3cm;">Binary</td>
<td style="text-align: center; width: 3cm;">0.0085</td>
<td style="text-align: center; width: 3cm;">Yes</td>
<td style="text-align: center; width: 3cm;">NA</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<!-- double check that the variables are in the righ tpalces in the table  -->
<p><a href="#tbl-combined-btab" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</p>
<!-- perhaps find the threshold for variance ratios -->
</section>
<section id="sec-nsw-results" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-nsw-results"><span class="header-section-number">3.2.3</span> Step 9: Results</h3>
<p>Finally, the treatment effect can be estimated using <code>lm_weightit()</code> from the <code>WeightIt</code> package and <code>avg_comparisons()</code> from the <code>marginaleffects</code> package. <code>lm_weightit()</code> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <code>avg_comparisons()</code> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</p>
<p>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="citation" data-cites="Naimi2017">(<a href="references.html#ref-Naimi2017" role="doc-biblioref">Naimi, Cole, and Kennedy 2017</a>)</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <code>re78</code> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</p>
<div class="sourceCode" id="annotated-cell-9"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-9-1" class="code-annotation-target"><a href="#annotated-cell-9-1" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span></span>
<span id="annotated-cell-9-2"><a href="#annotated-cell-9-2" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data,</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-9-3" class="code-annotation-target"><a href="#annotated-cell-9-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights)</span>
<span id="annotated-cell-9-4"><a href="#annotated-cell-9-4" aria-hidden="true" tabindex="-1"></a></span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="3" onclick="event.preventDefault();">3</a><span id="annotated-cell-9-5" class="code-annotation-target"><a href="#annotated-cell-9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects)</span>
<span id="annotated-cell-9-6"><a href="#annotated-cell-9-6" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="1,2" data-code-annotation="1">Uses <code>lm_weightit()</code> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <code>*</code> indicates multiplication in R.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="3" data-code-annotation="2">Specifies the <code>weights</code> from the <code>nsw_boosted_weight</code> object created earlier by the <code>weightit()</code> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <code>nsw_boosted_weight$weights</code>.</span>
</dd>
<dt data-target-cell="annotated-cell-9" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="5,6" data-code-annotation="3">Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
</dd>
</dl>
<p>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</p>
<div class="cell">
<details class="code-fold">
<summary>Code: Create <a href="tbl-nsw-comparisons">Table 3.2</a></summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>nsw_logit_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78<span class="sc">~</span>treat<span class="sc">*</span>(age <span class="sc">+</span> educ <span class="sc">+</span> </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                             re75 <span class="sc">+</span> black <span class="sc">+</span> hisp <span class="sc">+</span> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                             degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                             <span class="at">weights =</span> nsw_logit_weight<span class="sc">$</span>weights)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>nsw_logit_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_logit_lm, <span class="at">variables =</span> <span class="st">"treat"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>nsw_comparisons_tab <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">extract_comparison_results</span>(nsw_logit_result),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">extract_comparison_results</span>(nsw_boosted_result))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(nsw_comparisons_tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Logistic Regression"</span>, <span class="st">"GBM"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(nsw_comparisons_tab, <span class="at">digits =</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-nsw-comparisons" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Comparison of estimates of the average treatment for the National Supported Work data.
</figcaption>
<div aria-describedby="tbl-nsw-comparisons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Estimate</th>
<th style="text-align: right;">SE</th>
<th style="text-align: right;">P.Value</th>
<th style="text-align: right;">Lower.CI</th>
<th style="text-align: right;">Upper.CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: right;">1610.786</td>
<td style="text-align: right;">668.4870</td>
<td style="text-align: right;">0.0160</td>
<td style="text-align: right;">300.5756</td>
<td style="text-align: right;">2920.997</td>
</tr>
<tr class="even">
<td style="text-align: left;">GBM</td>
<td style="text-align: right;">1609.947</td>
<td style="text-align: right;">669.4201</td>
<td style="text-align: right;">0.0162</td>
<td style="text-align: right;">297.9081</td>
<td style="text-align: right;">2921.987</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#tbl-nsw-comparisons" class="quarto-xref">Table&nbsp;<span>3.2</span></a> shows that both estimates of the treatment effect are nearly identical at <span class="math inline">\(\$1610\)</span> with logistic regression inferring a <span class="math inline">\(\$0.86\)</span> larger treatment effect. Additionally, these results are statistically significant at the <span class="math inline">\(5\%\)</span> level with nearly identical standard errors.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="https://www.jstor.org/stable/2699986">https://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-King2019" class="csl-entry" role="listitem">
King, Gary, and Richard Nielsen. 2019. <span>“<span class="nocase">Why Propensity Scores Should Not Be Used for Matching</span>.”</span> <em>Political Analysis</em> 27 (4): 435–54. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-McCaffrey2004" class="csl-entry" role="listitem">
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. <span>“<span class="nocase">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</span>.”</span> <em>Psychological Methods</em> 9 (4): 403–25. <a href="https://doi.org/10.1037/1082-989X.9.4.403">https://doi.org/10.1037/1082-989X.9.4.403</a>.
</div>
<div id="ref-Naimi2017" class="csl-entry" role="listitem">
Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. 2017. <span>“<span class="nocase">An introduction to g methods</span>.”</span> <em>International Journal of Epidemiology</em> 46 (2): 756–62. <a href="https://doi.org/10.1093/ije/dyw323">https://doi.org/10.1093/ije/dyw323</a>.
</div>
<div id="ref-Ridgeway2024" class="csl-entry" role="listitem">
Ridgeway, Greg, Dan Mccaffrey, Andrew Morral, Matthew Cefalu, Lane Burgette, and Beth Ann Griffin. 2024. <span>“<span class="nocase">Toolkit for Weighting and Analysis of Nonequivalent Groups: A Tutorial for the R TWANG Package</span>.”</span> <a href="https://doi.org/10.7249/tl136.1">https://doi.org/10.7249/tl136.1</a>.
</div>
<div id="ref-Rosenbaum1983" class="csl-entry" role="listitem">
Rosenbaum, Paul R., and Donald B. Rubin. 1983. <span>“<span class="nocase">The central role of the propensity score in observational studies for causal effects</span>.”</span> <em>Biometrika</em> 70 (1): 41–55. <a href="https://doi.org/10.1017/CBO9780511810725.016">https://doi.org/10.1017/CBO9780511810725.016</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/pscore_theory.html" class="pagination-link" aria-label="Propensity Scores and Machine Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Propensity Scores and Machine Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/coffee_replication.html" class="pagination-link" aria-label="Replication Case Study">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Replication Case Study</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="in">#| include: false</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="in">load(file = "globals.RData")</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Tutorial: Implimentation, Workflow, and Example with `WeightIt` and`gbm` in R {#sec-demo}</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>Based on @Friedman2001, the <span class="in">`gbm`</span> package implements a *Generalized Boosting Machine*. Here, the “generalized” is because the package provides generalisations of the boosting framework to other distributions such as Bernoulli, Poisson, and Cox-proportional hazards partial likelihood of class probability predictions. Although this implementation very closely follows @Friedman2001 who introduced the gradient boosting machine. <span class="in">`gbm`</span> also supports stochastic gradient boosting, which performs random bootstrap sampling for each tree using the <span class="in">`bag.fraction`</span> parameter.</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>To fit and tune a GBM for propensity scores, wrapper packages facilitate optimal hyperparameter tuning for covariate balance. An effective approach involves fitting the model and computing balance statistics at each hyperparameter combination. Since the <span class="in">`gbm`</span> package does not support this type of tuning, a wrapper package like <span class="in">`WeightIt`</span> is necessary. <span class="in">`WeightIt`</span> allows for hyperparameter tuning based on covariate balance and inverse propensity weighting (IPW). <span class="in">`WeightIt`</span> supports hyperparameter turning of <span class="in">`shrinkage`</span>, <span class="in">`interaction.depth`</span>, and <span class="in">`n.trees`</span>. Once the best model is identified, propensity scores are predicted inside <span class="in">`WeightIt`</span>. These can be used inside <span class="in">`WeightIt`</span> to perform IPW or extracted for other implementations. <span class="in">`WeightIt`</span> also supports an offset meaning that logistic regression predictions are supplied to the <span class="in">`GBM`</span> package.</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>Multiple sources, including package documentation and other research, suggest values for hyperparameters <span class="co">[</span><span class="ot">see @McCaffrey2004; @Ridgeway2024</span><span class="co">]</span>. A very low learning rate, such as $0.01$ or $0.0005$, allows a smooth descent of the loss function. The model should include a high number of trees, with $10,000$ or $20,000$ being a typical default value. While this may seem excessive, it is required when a low learning rate is used. A grid search process should consider many options including a very high number of trees and even though the optimal model may contain fewer trees. While GBMs often use shallow trees like stumps, allowing a few splits per tree can better model non-linearity and additivity. The package default allows for $3$ splits. Based on anecdotal experience, $1$ to $5$ splits per tree is optimal, consistent with recommendations by @McCaffrey2004.</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>Another package, <span class="in">`twang`</span>, proves functionality to tune the number of trees, but there are no inbuilt options for tuning of other hyperparameters and so accessory packages such as <span class="in">`caret`</span> must be used. Although <span class="in">`twang`</span> has other useful functionalities which users may wish to implement.</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter Tuning and Workflow {#sec-gbm-tune-workflow}</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- might be useful: @McCaffrey2004 suggest that a learning rate as low as $0.0005$ is optimal with $20,000$ trees. In conventional machine learning contexts, such significant number of trees is likely to causa overiftting, however this may not be a concern in the context of propensity scores.  --&gt;</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>The <span class="in">`WeigthtIt`</span> package seems to have the best options for hyperparameter tuning and integration with a package for assessing balance called <span class="in">`cobalt`</span>. The best information for this package can be found on this <span class="co">[</span><span class="ot">website</span><span class="co">](https://ngreifer.github.io/WeightIt/index.html)</span> or accessed with <span class="in">`vignette("WeightIt")`</span> inside R after installation using <span class="in">`install.packages("WeightIt")`</span>.</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>A workflow for hyperparameter tuning in <span class="in">`WeightIt`</span> may be completed as follows:</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specify the <span class="in">`criterion`</span> option, which specifies the measure of the *best model*. The available options are the options that the <span class="in">`cobalt`</span> can compute. A simple option to choose may be the average standardised mean difference (SMD) across all covariates called <span class="in">`sdm.mean`</span> or the smallest maximum SDM across covariates called <span class="in">`sdm.max`</span>.</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Set the number of trees high. The package default is <span class="in">`n.trees = 10000`</span> for binary treatments, but this may be too small depending on the learning rate. Typically, it is best to increase the number of trees to allow slow learners to reach their minimum criterion. There is no modelling downside to a larger number of trees other than computation time as the model will predict propensity scores with a smaller <span class="in">`n.tree`</span> if optimal.</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specify the grid search for the depth of the tree called <span class="in">`interaction.depth`</span> and the learning rate called <span class="in">`shrinkage`</span>. These values can be specified using <span class="in">`c()`</span> such as <span class="in">`shrinkage = c(0.0005, 0.001, 0.05, 0.1, 0.2, 0.3)`</span> or as integers such as <span class="in">`interaction.depth = 1:5`</span>. These particular values are heuristically selected *suggestions* of good starting values. Additionally, an offset can be considered by performing a grid search across <span class="in">`offset = c(TRUE,FALSE)`</span>.</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>The model is fit and a grid search is performed. The tune grid and balance statistics can be retrieved with <span class="in">`my_weightit_object$info$best.tune`</span>.</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>The best model should be inspected and to determine if the initial grid is appropriate. If the selection of the best model is at the boundary of a grid search, then a new grid should be created and step 3 and 4 are repeated. For example, if the initial fit is completed with <span class="in">`interaction.depth = 1:5`</span> and the best fit is $5$, then a new search can consider <span class="in">`interaction.depth = 3:7`</span> so that the local area around $5$ can be searched.</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Experiment with <span class="in">`bag.fraction`</span>, which means each tree will consider a drawn proportion of observations equal to <span class="in">`bag.fraction`</span>. Iteratively changing <span class="in">`bag.fraction`</span> and assessing balance at each value should be practical. Consider $0.5$, $0.67$, and $1$.</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>Assess balance of covariates and model fit. Covariate balance can be assessed with a balance table or visualisation of the variables using <span class="in">`love.plot()`</span> such as @fig-coffee-replication-lplot.</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>The tuning process is stated and reported. Balance tables are presented and discussed. Comparison to other methods of estimation if relevant.</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Estimation and reporting of treatment effect.</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: NSW Jobs Dataset Using R</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>For demonstration, propensity scores are estimated following the workflow discussed in @sec-gbm-tune-workflow to estimate inverse propensity weights (IPW). The NSW jobs dataset arises from a randomised setting as described in @sec-data-nsw-jobs. Randomisation should eliminate structural differences between groups, but @Rosenbaum1983 notes that randomisation only addresses structural balance and does not account for chance imbalance. To address this, propensity scores can mitigate any remaining chance imbalance, providing a more accurate estimate of the treatment effect. This example will include the fitting process of a GBM using <span class="in">`WeightIt`</span> and a logistic regression model using <span class="in">`glm()`</span>. Additionally, balance statistics will be computed leading to a robust estimate of the treatment effect. All code to replicate this process and results is provided.</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>::: {#nte-ipw .callout-note title="Inverse Probability of Treatment Weighting"}</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>Inverse probability of treatment weighting or inverse propensity weighting (IPW) adjusts for confounding in observational data by weighting individuals based on the inverse of their probability of receiving the treatment they actually got. This method creates a *pseudo-population* where treatment assignment is independent of observed covariates, similar to a randomized controlled trial. In this re-weighted population, the treatment and control groups should be have covariate balance, allowing for unbiased estimation of treatment effects. Essentially, IPW simulates random treatment assignment by rebalancing the sample, thereby eliminating confounding and enabling more accurate causal inferences.</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 1-6: Model Fitting and Tuning</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>The <span class="in">`glm()`</span> function will fit a conventional propensity score model with logistic regression in R. Logistic regression is performed by specifying the family to be the <span class="in">`binomial()`</span>. Recall the <span class="in">`nsw_formula`</span> is specified in @sec-bagg-rf-probmachines.</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_logit_pmodel</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false </span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pmodel &lt;- glm(nsw_formula, data = nsw_data,</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="in">                        family=binomial()) #&lt;1&gt;</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_pscores &lt;- nsw_logit_pmodel$fitted.values#&lt;2&gt;</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>nsw_logit_pmodel <span class="ot">&lt;-</span> <span class="fu">glm</span>(nsw_formula, <span class="at">data =</span> nsw_data,</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>                        <span class="at">family=</span><span class="fu">binomial</span>()) <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>nsw_logit_pscores <span class="ot">&lt;-</span> nsw_logit_pmodel<span class="sc">$</span>fitted.values<span class="co">#&lt;2&gt;</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Fits a logistic regression model using the <span class="in">`glm()`</span> function specified to be a logistic model with <span class="in">`family=binomial()`</span> using the previously created <span class="in">`nsw_formula`</span>.</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Extracts the fitted values (propensity scores) from the model.</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>Using the propensity score column of <span class="in">`nsw_data`</span>, the <span class="in">`WeightIt`</span> package will perform IPW and assign a weight to each observation such that the pseudo-population should exhibit covariate balance. The model object will be called <span class="in">`nsw_logit_weight`</span>.</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nsw_logit_weight</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(WeightIt)</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>nsw_logit_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>                             <span class="at">ps =</span> nsw_logit_pscores,<span class="co">#&lt;2&gt;</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>                             <span class="at">estimand =</span> <span class="st">"ATE"</span>)<span class="co">#&lt;3&gt;</span></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Provides <span class="in">`weightit()`</span> with the propensity scores from the logistic regression function. Note that in practice this can be completed within the <span class="in">`weightit()`</span> function with <span class="in">`method = "glm"`</span>. The separate estimation of the propensity scores is for illustrative purposes.</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the estimand as the average treatment effect or ATE. For the purposes of demonstration, this is an arbitrary choice.</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>A GBM model for propensity scores can be specified using <span class="in">`method = "gbm"`</span> inside the <span class="in">`weightit()`</span> function. To ensure consistent results, running <span class="in">`set.seed(88)`</span> will ensure each tree uses the same <span class="in">`seed`</span> if <span class="in">`bag.fraction`</span> less than $1$. The model is fit using the heuristically suggested starting values. Note that this model may take approximately $30$ second to fit as a grid search procedure is computationally intensive. Additionally, the best tuning specification is printed to assess if the initial tuning grid is appropriate.</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">88</span>)</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>nsw_boosted_weight <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.0005</span>, <span class="fl">0.001</span>, <span class="fl">0.05</span>, </span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>                                             <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>                               <span class="at">bag.fraction =</span> <span class="dv">1</span>, <span class="co">#&lt;4&gt;</span></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="co">#&lt;4&gt;</span></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, <span class="co">#&lt;5&gt;</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">10000</span>) <span class="co">#&lt;5&gt;</span></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(nsw_boosted_weight<span class="sc">$</span>info<span class="sc">$</span>best.tune) <span class="co">#&lt;6&gt;</span></span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specifies the formula and data.</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the propensity score prediction method to be a GBM and the estimand to the ATE.</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Performs a grid search over these values of the learning rate and depth of tree.</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Requires the model to use every observation in every tree, meaning the model will not perform stochastic gradient boosting. The function will will fit an offset and level GBM and select the specification with the best balance.</span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Defines the optimisation criteria to be the tune with the lowest average standardised mean difference (SMD). Additionally, the number of trees will be $10000$ which is the package default.</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Prints the tune details of the model with the best covariate balance.</span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- clarify the meaning of learning rate/shrinkage --&gt;</span></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- change all the instructions to active speech not passive.  --&gt;</span></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight</span></span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.0005, 0.001, 0.05, </span></span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a><span class="in">                                            0.1, 0.2, 0.3),</span></span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:5,</span></span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 10000)</span></span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight$info$best.tune)</span></span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a>The best balance across all tuning combinations yields an average SMD of $0.023$ showing strong balance compared to the $0.1$ threshold. Note averages can conceal extremes and a low average SMD does not mean all variables are balanced. A full balance table is presented in @sec-nsw-balance accompanying a discussion of balance.</span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a>The best machine has a learning rate of $0.3$ and contains $2392$ decision stumps (trees with a depth of 1). The learning rate is on the boundary of the initial tuning grid showing that the tuning grid should be re-specified to include values near to $0.3$. A reduction in the depth of tree and number of trees will reduce computation time.</span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a>The new tune grid will consider <span class="in">`shrinkage = c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5)`</span> as this allows the GBM to consider values between $0.2$ and $0.3$ and above $0.3$ which were missing in the previous grid.</span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_boosted_weight2</span></span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Code: Fit `weightit()` with Updated Tune Grid"</span></span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a><span class="in">set.seed(88)</span></span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_weight2 &lt;- weightit(nsw_formula, data = nsw_data, </span></span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a><span class="in">                               method="gbm",</span></span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a><span class="in">                               estimand = "ATE", </span></span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a><span class="in">                               shrinkage= c(0.25, 0.3, 0.35, 0.4, 0.45, 0.5),</span></span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a><span class="in">                               interaction.depth = 1:3,</span></span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a><span class="in">                               bag.fraction = 1,</span></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a><span class="in">                               offset = c(TRUE, FALSE),</span></span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a><span class="in">                               criterion = "smd.mean", </span></span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a><span class="in">                               n.trees = 5000)</span></span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a><span class="in">print(nsw_boosted_weight2$info$best.tune)</span></span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a>Comparing the two iterations, there is a reduction from $0.022$ to $0.02$. The optimal tuning values are towards the centre of the tuning grid, implying that an adequate search of the local area has been completed. The best machine has a learning rate of $0.45$, a tree depth of $2$, and $95$ trees. The learning rate is higher than expected, but this also explains why fewer trees are optimal.</span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a>Plotting the relationship between the number of trees and the average SMD is informative for the behaviour of the machine. Additionally, @fig-balance-iterations shows the optimal number of trees is highly variable. If the learning rate is set to <span class="in">`shrinkage = 0.05`</span>, then the best balance is not achieved until near to $20,000$ trees.</span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-balance-iterations</span></span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Relatoinship between standardised mean difference, number of interations, and learning rate in a GBM model. Please note the difference in horozontal scale between the two learning rates. The model is fit using `weightit` from the `WeightIt` package."</span></span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-showtext: true</span></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Code: Create [Figure 3.1](fig-balance-iterations)"</span></span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a>low_shrinkage <span class="ot">&lt;-</span> <span class="fu">weightit</span>(nsw_formula, <span class="at">data =</span> nsw_data, </span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>                               <span class="at">method =</span> <span class="st">"gbm"</span>,</span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>                               <span class="at">estimand =</span> <span class="st">"ATE"</span>, </span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a>                               <span class="at">shrinkage =</span> <span class="fl">0.05</span>,</span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>                               <span class="at">interaction.depth =</span> <span class="dv">1</span>,</span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>                               <span class="at">offset =</span> <span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a>                               <span class="at">criterion =</span> <span class="st">"smd.mean"</span>, </span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n.trees =</span> <span class="dv">40000</span>)</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(nsw_boosted_weight2<span class="sc">$</span>info<span class="sc">$</span>tree.val, </span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Optimal Tune"</span>,</span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>,</span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Average Standardised Mean Difference"</span>) <span class="sc">+</span></span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">500</span>)</span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a>lowshrinkage_boost_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>tree.val, </span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>                                  <span class="fu">aes</span>(<span class="at">x =</span> tree, <span class="at">y =</span> smd.mean)) <span class="sc">+</span></span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">"#2780e3"</span>) <span class="sc">+</span> </span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">"Low Learning Rate (shrinkage = 0.05)"</span>,</span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Number of Iterations"</span>, </span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"curve"</span>, <span class="at">x =</span> <span class="dv">30000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, </span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a>           <span class="at">xend =</span> low_shrinkage<span class="sc">$</span>info<span class="sc">$</span>best.tree, <span class="at">yend =</span> <span class="fl">0.0231</span>,</span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a>           <span class="at">curvature =</span> <span class="fl">0.3</span>, <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="dv">2</span>, <span class="st">"mm"</span>))) <span class="sc">+</span></span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">"text"</span>, <span class="at">x =</span> <span class="dv">31000</span>, <span class="at">y =</span> <span class="fl">0.05</span>, <span class="at">label =</span> <span class="st">"Minimum"</span>, </span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a>           <span class="at">hjust =</span> <span class="st">"left"</span>, <span class="at">color =</span> <span class="st">"#333333"</span>, <span class="at">size =</span> <span class="dv">3</span>, </span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">"Source Sans Pro"</span>)  </span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>optimal_boost_plot <span class="sc">+</span> lowshrinkage_boost_plot <span class="sc">+</span> <span class="fu">plot_annotation</span>(</span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a>  <span class="at">title =</span> <span class="st">'Number of Tree Iterations and Balance'</span>) <span class="sc">+</span> custom_ggplot_theme</span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>For the optimal machine fit, finding that balance worsens as the number of trees increases is just as informative as knowing the correct number of trees. Provided sufficient computational performance, a wide grid search is beneficial in the long run to ensure that each model specification reaches the best balance possible.</span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 7 and 8: Assessing Balance {#sec-nsw-balance}</span></span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title="The Importance of Discussing Balance"}</span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>Assessing balance is crucial because it ensures that the treated and control groups are comparable on observed covariates. This comparability is essential for reducing confounding and making valid causal inferences. Without proper balance, differences in outcomes between the groups could be due to pre-existing differences rather than the treatment itself. Balance assessment helps to verify that the propensity score model has effectively adjusted for covariates, creating a pseudo-randomized scenario. This step is vital for the reliability and validity of the study's conclusions. @King2019 notes that many papers that implement propensity score methods do not assess or report a balance in their studies, which can undermine the credibility of the research process and make it hard for readers to understand why results are robust.</span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>A good resource of information for assessing balance is documentation from the <span class="in">`cobalt`</span> package, which can be viewed by running <span class="in">`vignette(“cobalt”, package = “cobalt”)`</span> in R.</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a><span class="in">`cobalt`</span> is a powerful package to create tables and visualisations of to assess balance. The package also provides very good integration with other related packages such as <span class="in">`WeightIt`</span> for IPW and <span class="in">`MatchIt`</span> for propensity score matching. Balance tables are created using <span class="in">`bal.tab()`</span>.</span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- make sure this comment about integration is not repeditive  --&gt;</span></span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: nsw-btab-logit</span></span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>),<span class="co">#&lt;3&gt;</span></span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>,<span class="co">#&lt;3&gt;</span></span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>))<span class="co">#&lt;4&gt;</span></span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cobalt) <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> <span class="fu">bal.tab</span>(nsw_logit_weight, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a>                          <span class="at">data =</span> nsw_data, <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a>                          <span class="at">stats =</span> <span class="fu">c</span>(<span class="st">"mean.diffs"</span>,<span class="st">"variance.ratios"</span>), <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a>                          <span class="at">binary =</span> <span class="st">"std"</span>, <span class="at">continuous =</span> <span class="st">"std"</span>, <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a>                          <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="at">mean.diffs =</span> <span class="fl">0.1</span>)) <span class="co">#&lt;4&gt;</span></span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab <span class="ot">&lt;-</span> nsw_logit_btab<span class="sc">$</span>Balance[<span class="sc">-</span><span class="dv">1</span>,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] <span class="co">#&lt;5&gt;</span></span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Loads the <span class="in">`cobalt`</span> package. This assumes the package is already installed with <span class="in">`install.packages("cobalt")`</span></span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Uses the <span class="in">`bal.tab()`</span> fucntion to create balance statistics for the previously created <span class="in">`nsw_logit_weight`</span> model.</span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Specifies the calculation of standardised mean differences and variance ratios for each covariate. The mean differences will be standardised for binary and continuous variables.</span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Sets a threshold of balance to be $0.1$ to determine if a covariate is balanced.</span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Extracts the balance table of the <span class="in">`nsw_logit_btab`</span> object and removes excessive columns. This is only completed for ease of visualisation and is not typically required.</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-287"><a href="#cb8-287" aria-hidden="true" tabindex="-1"></a>Additionally, <span class="in">`bal.tab()`</span> will create balance tables for the GBM method's IPWs and the raw data. For presentation, <span class="in">`dplyr`</span> combines each of the individual balance tables for presentation using <span class="in">`kable`</span> and <span class="in">`kableExtra`</span>.</span>
<span id="cb8-288"><a href="#cb8-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-289"><a href="#cb8-289" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-290"><a href="#cb8-290" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: nsw_btabs</span></span>
<span id="cb8-291"><a href="#cb8-291" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb8-292"><a href="#cb8-292" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-293"><a href="#cb8-293" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb8-294"><a href="#cb8-294" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb8-295"><a href="#cb8-295" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Code: Create Balance Tables"</span></span>
<span id="cb8-296"><a href="#cb8-296" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- bal.tab(nsw_boosted_weight, </span></span>
<span id="cb8-297"><a href="#cb8-297" aria-hidden="true" tabindex="-1"></a><span class="in">                            data = nsw_data,</span></span>
<span id="cb8-298"><a href="#cb8-298" aria-hidden="true" tabindex="-1"></a><span class="in">                            stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb8-299"><a href="#cb8-299" aria-hidden="true" tabindex="-1"></a><span class="in">                            binary = "std", continuous = "std",</span></span>
<span id="cb8-300"><a href="#cb8-300" aria-hidden="true" tabindex="-1"></a><span class="in">                            thresholds = c(mean.diffs = 0.1))</span></span>
<span id="cb8-301"><a href="#cb8-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-302"><a href="#cb8-302" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- bal.tab(nsw_formula, </span></span>
<span id="cb8-303"><a href="#cb8-303" aria-hidden="true" tabindex="-1"></a><span class="in">                        data = nsw_data, </span></span>
<span id="cb8-304"><a href="#cb8-304" aria-hidden="true" tabindex="-1"></a><span class="in">                        stats = c("mean.diffs","variance.ratios"),</span></span>
<span id="cb8-305"><a href="#cb8-305" aria-hidden="true" tabindex="-1"></a><span class="in">                        binary = "std", continuous = "std",</span></span>
<span id="cb8-306"><a href="#cb8-306" aria-hidden="true" tabindex="-1"></a><span class="in">                        thresholds = c(mean.diffs = 0.1),</span></span>
<span id="cb8-307"><a href="#cb8-307" aria-hidden="true" tabindex="-1"></a><span class="in">                        s.d.denom = "treated")</span></span>
<span id="cb8-308"><a href="#cb8-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-309"><a href="#cb8-309" aria-hidden="true" tabindex="-1"></a><span class="in"># Extracts the balance table and removes unwanted columns. </span></span>
<span id="cb8-310"><a href="#cb8-310" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_btab &lt;- nsw_boosted_btab$Balance[-1,-c(2,3)]</span></span>
<span id="cb8-311"><a href="#cb8-311" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_raw_btab &lt;- nsw_raw_btab$Balance[-c(5,6)]</span></span>
<span id="cb8-312"><a href="#cb8-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-313"><a href="#cb8-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-314"><a href="#cb8-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-317"><a href="#cb8-317" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-318"><a href="#cb8-318" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-combined-btab</span></span>
<span id="cb8-319"><a href="#cb8-319" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb8-320"><a href="#cb8-320" aria-hidden="true" tabindex="-1"></a><span class="co">#| cache: true</span></span>
<span id="cb8-321"><a href="#cb8-321" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb8-322"><a href="#cb8-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb8-323"><a href="#cb8-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Code: Create [Table 3.1](tbl-combined-btab)"</span></span>
<span id="cb8-324"><a href="#cb8-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Standardised mean difference (a measure of balance) across different covariates in the National Supported Word dataset. The values are categorised for different propenensity score methods allowing a comparison. Balance tables are computed using `bal.tab()` from the `cobalt` package."</span></span>
<span id="cb8-325"><a href="#cb8-325" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb8-326"><a href="#cb8-326" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span>
<span id="cb8-327"><a href="#cb8-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-328"><a href="#cb8-328" aria-hidden="true" tabindex="-1"></a>collabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Type"</span>, <span class="st">"SMD"</span>, <span class="st">"Balanced"</span>, <span class="st">"Variance Ratio"</span>,<span class="st">"Method"</span>)</span>
<span id="cb8-329"><a href="#cb8-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-330"><a href="#cb8-330" aria-hidden="true" tabindex="-1"></a>rowlabels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Age"</span>, <span class="st">"Education"</span>, <span class="st">"Income 1975"</span>,<span class="st">"Black"</span>, </span>
<span id="cb8-331"><a href="#cb8-331" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Hispanic"</span>, <span class="st">"Degree"</span>, <span class="st">"Married"</span>)</span>
<span id="cb8-332"><a href="#cb8-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-333"><a href="#cb8-333" aria-hidden="true" tabindex="-1"></a>nsw_raw_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"Raw Data"</span></span>
<span id="cb8-334"><a href="#cb8-334" aria-hidden="true" tabindex="-1"></a>nsw_logit_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Logistic Regression"</span></span>
<span id="cb8-335"><a href="#cb8-335" aria-hidden="true" tabindex="-1"></a>nsw_boosted_btab<span class="sc">$</span>method <span class="ot">&lt;-</span> <span class="st">"IPTW: Boosting"</span></span>
<span id="cb8-336"><a href="#cb8-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-337"><a href="#cb8-337" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(<span class="fu">setNames</span>(nsw_raw_btab,collabels),</span>
<span id="cb8-338"><a href="#cb8-338" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_logit_btab,collabels),</span>
<span id="cb8-339"><a href="#cb8-339" aria-hidden="true" tabindex="-1"></a>                           <span class="fu">setNames</span>(nsw_boosted_btab,collabels))</span>
<span id="cb8-340"><a href="#cb8-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-341"><a href="#cb8-341" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(rowlabels,<span class="dv">3</span>)</span>
<span id="cb8-342"><a href="#cb8-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-343"><a href="#cb8-343" aria-hidden="true" tabindex="-1"></a>combined_btab <span class="ot">&lt;-</span> combined_btab[<span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)]</span>
<span id="cb8-344"><a href="#cb8-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-345"><a href="#cb8-345" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(combined_btab) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb8-346"><a href="#cb8-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-347"><a href="#cb8-347" aria-hidden="true" tabindex="-1"></a>combined_btab<span class="sc">$</span>Balanced <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb8-348"><a href="#cb8-348" aria-hidden="true" tabindex="-1"></a>          combined_btab<span class="sc">$</span>Balanced <span class="sc">==</span> <span class="st">"Not Balanced, &gt;0.1"</span>, <span class="st">"No"</span>, <span class="st">"Yes"</span>)</span>
<span id="cb8-349"><a href="#cb8-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-350"><a href="#cb8-350" aria-hidden="true" tabindex="-1"></a><span class="fu">kbl</span>(combined_btab[<span class="sc">-</span><span class="dv">6</span>], <span class="at">digits =</span> <span class="dv">4</span>, <span class="at">booktabs =</span> <span class="cn">TRUE</span>, <span class="at">align =</span> <span class="st">"c"</span>, </span>
<span id="cb8-351"><a href="#cb8-351" aria-hidden="true" tabindex="-1"></a>      <span class="at">font_size =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-352"><a href="#cb8-352" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb8-353"><a href="#cb8-353" aria-hidden="true" tabindex="-1"></a>  <span class="fu">row_spec</span>(<span class="dv">0</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-354"><a href="#cb8-354" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">1</span>, <span class="at">bold =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-355"><a href="#cb8-355" aria-hidden="true" tabindex="-1"></a>  <span class="fu">column_spec</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">bold =</span> F, <span class="at">width =</span> <span class="st">"3cm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb8-356"><a href="#cb8-356" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pack_rows</span>(<span class="at">index =</span> <span class="fu">rev</span>(<span class="fu">table</span>(combined_btab<span class="sc">$</span>Method)))</span>
<span id="cb8-357"><a href="#cb8-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-358"><a href="#cb8-358" aria-hidden="true" tabindex="-1"></a><span class="co">#perhaps use the method from the coffe table as this is not as neat as the other method. for mergin that is.</span></span>
<span id="cb8-359"><a href="#cb8-359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-360"><a href="#cb8-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-361"><a href="#cb8-361" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- double check that the variables are in the righ tpalces in the table  --&gt;</span></span>
<span id="cb8-362"><a href="#cb8-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-363"><a href="#cb8-363" aria-hidden="true" tabindex="-1"></a>@tbl-combined-btab shows that both logistic regression and the GBM have reduced imbalance. The raw data exhibits imbalance across age, years of education, if someone is gispanic, and if someone has a bachelors degree. Imbalanced datasets leads to biased treatment effect estimation so the estimate of the treatment effect in the raw data may be biased. In this example, logistic regression appears to achieve the best covariate balance although GBM achieves slightly better variance ratios.</span>
<span id="cb8-364"><a href="#cb8-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-365"><a href="#cb8-365" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- perhaps find the threshold for variance ratios --&gt;</span></span>
<span id="cb8-366"><a href="#cb8-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-367"><a href="#cb8-367" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 9: Results {#sec-nsw-results}</span></span>
<span id="cb8-368"><a href="#cb8-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-369"><a href="#cb8-369" aria-hidden="true" tabindex="-1"></a>Finally, the treatment effect can be estimated using <span class="in">`lm_weightit()`</span> from the <span class="in">`WeightIt`</span> package and <span class="in">`avg_comparisons()`</span> from the <span class="in">`marginaleffects`</span> package. <span class="in">`lm_weightit()`</span> fits a linear model with a covariance matrix that accounts for the estimation of weights using IPW. Additionally, <span class="in">`avg_comparisons()`</span> computes the contrast between the treatment and control group to obtain an estimate of the treatment effect.</span>
<span id="cb8-370"><a href="#cb8-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-371"><a href="#cb8-371" aria-hidden="true" tabindex="-1"></a>These steps perform G-computation, meaning that potential outcomes are estimated under treatment and control for each observation <span class="co">[</span><span class="ot">@Naimi2017</span><span class="co">]</span>. The contrast of the mean of each of the two potential outcomes is the estimate of the treatment effect. Note that the outcome variable is <span class="in">`re78`</span> which is real income in 1978 meaning that the income is adjusted for inflation. Previously, the treatment indicator was the outcome variable because the propensity scores are a prediction of the treatment indicator.</span>
<span id="cb8-372"><a href="#cb8-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-373"><a href="#cb8-373" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-374"><a href="#cb8-374" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: compute-ate-nsw-boosted</span></span>
<span id="cb8-375"><a href="#cb8-375" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb8-376"><a href="#cb8-376" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb8-377"><a href="#cb8-377" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-378"><a href="#cb8-378" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_lm &lt;- lm_weightit(re78 ~ treat * (age + educ + re75 + black + </span></span>
<span id="cb8-379"><a href="#cb8-379" aria-hidden="true" tabindex="-1"></a><span class="in">                              hisp + degree + marr), data = nsw_data, </span></span>
<span id="cb8-380"><a href="#cb8-380" aria-hidden="true" tabindex="-1"></a><span class="in">                              weights = nsw_boosted_weight$weights)</span></span>
<span id="cb8-381"><a href="#cb8-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-382"><a href="#cb8-382" aria-hidden="true" tabindex="-1"></a><span class="in">library(marginaleffects)</span></span>
<span id="cb8-383"><a href="#cb8-383" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_boosted_result &lt;- avg_comparisons(nsw_boosted_lm, variables = "treat")</span></span>
<span id="cb8-384"><a href="#cb8-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-385"><a href="#cb8-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-386"><a href="#cb8-386" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-387"><a href="#cb8-387" aria-hidden="true" tabindex="-1"></a>nsw_boosted_lm <span class="ot">&lt;-</span> <span class="fu">lm_weightit</span>(re78 <span class="sc">~</span> treat <span class="sc">*</span> (age <span class="sc">+</span> educ <span class="sc">+</span> re75 <span class="sc">+</span> black <span class="sc">+</span> <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-388"><a href="#cb8-388" aria-hidden="true" tabindex="-1"></a>                              hisp <span class="sc">+</span> degree <span class="sc">+</span> marr), <span class="at">data =</span> nsw_data, <span class="co">#&lt;1&gt;</span></span>
<span id="cb8-389"><a href="#cb8-389" aria-hidden="true" tabindex="-1"></a>                              <span class="at">weights =</span> nsw_boosted_weight<span class="sc">$</span>weights) <span class="co">#&lt;2&gt;</span></span>
<span id="cb8-390"><a href="#cb8-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-391"><a href="#cb8-391" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(marginaleffects) <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-392"><a href="#cb8-392" aria-hidden="true" tabindex="-1"></a>nsw_boosted_result <span class="ot">&lt;-</span> <span class="fu">avg_comparisons</span>(nsw_boosted_lm, <span class="at">variables =</span> <span class="st">"treat"</span>) <span class="co">#&lt;3&gt;</span></span>
<span id="cb8-393"><a href="#cb8-393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-394"><a href="#cb8-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-395"><a href="#cb8-395" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Uses <span class="in">`lm_weightit()`</span> to compute pseudo-outcomes. The formula here specifies an interaction between the treatment and all other variables. Note that <span class="in">`*`</span> indicates multiplication in R.</span>
<span id="cb8-396"><a href="#cb8-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-397"><a href="#cb8-397" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Specifies the <span class="in">`weights`</span> from the <span class="in">`nsw_boosted_weight`</span> object created earlier by the <span class="in">`weightit()`</span> function. Intuitively, this is performing linear regression using the pseudo-population, where the pseudo-population is created weighting the data by <span class="in">`nsw_boosted_weight$weights`</span>.</span>
<span id="cb8-398"><a href="#cb8-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-399"><a href="#cb8-399" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Computes a comparison between the potential outcomes as well as standard errors for inference.</span>
<span id="cb8-400"><a href="#cb8-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-401"><a href="#cb8-401" aria-hidden="true" tabindex="-1"></a>Additionally, this process is followed for the logistic regression propensity scores and the results are combined in to a table for comparison.</span>
<span id="cb8-402"><a href="#cb8-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-403"><a href="#cb8-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r }</span></span>
<span id="cb8-404"><a href="#cb8-404" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: tbl-nsw-comparisons</span></span>
<span id="cb8-405"><a href="#cb8-405" aria-hidden="true" tabindex="-1"></a><span class="in">#| warning: false</span></span>
<span id="cb8-406"><a href="#cb8-406" aria-hidden="true" tabindex="-1"></a><span class="in">#| eval: true</span></span>
<span id="cb8-407"><a href="#cb8-407" aria-hidden="true" tabindex="-1"></a><span class="in">#| cache: true</span></span>
<span id="cb8-408"><a href="#cb8-408" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-fold: true</span></span>
<span id="cb8-409"><a href="#cb8-409" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: !expr "knitr::is_html_output()"</span></span>
<span id="cb8-410"><a href="#cb8-410" aria-hidden="true" tabindex="-1"></a><span class="in">#| code-summary: "Code: Create [Table 3.2](tbl-nsw-comparisons)"</span></span>
<span id="cb8-411"><a href="#cb8-411" aria-hidden="true" tabindex="-1"></a><span class="in">#| tbl-cap: "Comparison of estimates of the average treatment for the National Supported Work data."</span></span>
<span id="cb8-412"><a href="#cb8-412" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_lm &lt;- lm_weightit(re78~treat*(age + educ + </span></span>
<span id="cb8-413"><a href="#cb8-413" aria-hidden="true" tabindex="-1"></a><span class="in">                             re75 + black + hisp + </span></span>
<span id="cb8-414"><a href="#cb8-414" aria-hidden="true" tabindex="-1"></a><span class="in">                             degree + marr), data = nsw_data, </span></span>
<span id="cb8-415"><a href="#cb8-415" aria-hidden="true" tabindex="-1"></a><span class="in">                             weights = nsw_logit_weight$weights)</span></span>
<span id="cb8-416"><a href="#cb8-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-417"><a href="#cb8-417" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_logit_result &lt;- avg_comparisons(nsw_logit_lm, variables = "treat")</span></span>
<span id="cb8-418"><a href="#cb8-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-419"><a href="#cb8-419" aria-hidden="true" tabindex="-1"></a><span class="in">nsw_comparisons_tab &lt;- rbind(extract_comparison_results(nsw_logit_result),</span></span>
<span id="cb8-420"><a href="#cb8-420" aria-hidden="true" tabindex="-1"></a><span class="in">                             extract_comparison_results(nsw_boosted_result))</span></span>
<span id="cb8-421"><a href="#cb8-421" aria-hidden="true" tabindex="-1"></a><span class="in">rownames(nsw_comparisons_tab) &lt;- c("Logistic Regression", "GBM")</span></span>
<span id="cb8-422"><a href="#cb8-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-423"><a href="#cb8-423" aria-hidden="true" tabindex="-1"></a><span class="in">knitr::kable(nsw_comparisons_tab, digits = 4)</span></span>
<span id="cb8-424"><a href="#cb8-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-425"><a href="#cb8-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-426"><a href="#cb8-426" aria-hidden="true" tabindex="-1"></a>@tbl-nsw-comparisons shows that both estimates of the treatment effect are nearly identical at $\$1610$ with logistic regression inferring a $\$0.86$ larger treatment effect. Additionally, these results are statistically significant at the $5\%$ level with nearly identical standard errors.</span>
<span id="cb8-427"><a href="#cb8-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-428"><a href="#cb8-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-431"><a href="#cb8-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb8-432"><a href="#cb8-432" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb8-433"><a href="#cb8-433" aria-hidden="true" tabindex="-1"></a><span class="fu">save.image</span>(<span class="at">file =</span> <span class="st">"globals.RData"</span>)</span>
<span id="cb8-434"><a href="#cb8-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mitchellcameron123/ML-PS.git/edit/main/chapters/implimentation_workflow.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>